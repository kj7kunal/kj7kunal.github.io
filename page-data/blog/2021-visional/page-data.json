{"componentChunkName":"component---src-templates-blog-post-jsx","path":"/blog/2021-visional/","result":{"data":{"site":{"siteMetadata":{"name":"Kunal Jain","title":"Kunal Jain | Machine Learning Engineer","description":"Machine Learning Engineer currently in Tokyo, Japan","about":"I'm a Machine Learning Engineer passionate about working in challenging and high-impact environments, and building automation pipelines that help save human time and effort. I love investigating new cutting-edge technologies and spending caffeinated lofi evenings implementing them in my personal projects. In my free time, I like to go long-distance running, visit coffee shops or learn the piano!","author":"kj7kunal","github":"https://github.com/kj7kunal","linkedin":"https://www.linkedin.com/in/kj7kunal","resume":"https://tinyurl.com/kunaljainresume","mail":"kjkunal7996@gmail.com"}},"markdownRemark":{"id":"19317e87-fe55-54a6-ad2e-07294abf0130","excerpt":"I have been working with Visional Inc., based in\nTokyo, Japan, since October 2019. This was my first job after graduation, and I have been\nworking as an ML/AI…","html":"<p>I have been working with <a href=\"https://www.visional.inc/ja/index.html\">Visional Inc.</a>, based in\nTokyo, Japan, since October 2019. This was my first job after graduation, and I have been\nworking as an ML/AI Engineer in the AI Team. As it is my <strong>second work anniversary</strong>, I\nthink it would be worthwhile to look back and reflect on my journey so far.</p>\n<p>My work mainly involves the development of end-to-end Machine Learning workflows as well as\ntheir deployment in the various services and products offered by the company.</p>\n<p>I also contribute to Machine Learning research undertaken by the AI team, and had initiated\nand led the research and development of an OCR system for Japanese handwritten addresses.</p>\n<p>I have limited work proficiency in Japanese, so I prefer to use English as much as possible.\nI am able to use English for work within the AI team, but often need to switch to Japanese\nfor cross-team communication.</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\"></code></pre></div>\n<h2>Contents</h2>\n<ul>\n<li><a href=\"#company\">About Visional</a></li>\n<li><a href=\"#projects\">My Projects</a>\n<ul>\n<li><a href=\"#p1\">Financial Document OCR System</a></li>\n<li><a href=\"#p2\">Research: Japanese handwritten OCR</a></li>\n<li><a href=\"#p3\">Employee Slack Analytics</a></li>\n<li><a href=\"#p4\">HeadHunter Recommendation</a></li>\n<li><a href=\"#p5\">Security Bug Report Classification</a></li>\n</ul>\n</li>\n<li><a href=\"#experience\">My Experience</a></li>\n<li><a href=\"#selected\">How I got selected</a>\n<ul>\n<li><a href=\"#preparation\">Interview Preparation</a></li>\n<li><a href=\"#placements\">Placement Interviews</a></li>\n</ul>\n</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\"></code></pre></div>\n<h2 id=\"company\">About Visional</h2>\n<p align=\"center\">\n  <span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; margin: 0 0 30px;\">\n      <a class=\"gatsby-resp-image-link\" href=\"/static/e750cc63f74625ca0094f46ebeacaf20/8a8a2/visional_way.jpg\" style=\"display: block\" target=\"_blank\" rel=\"noopener\">\n    <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 56.08108108108109%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAALABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAAUDBP/EABYBAQEBAAAAAAAAAAAAAAAAAAACA//aAAwDAQACEAMQAAABpo2mau4Ev//EABkQAAIDAQAAAAAAAAAAAAAAAAABAxITMv/aAAgBAQABBQLVM0LMm6jLM//EABURAQEAAAAAAAAAAAAAAAAAAAAS/9oACAEDAQE/AVP/xAAVEQEBAAAAAAAAAAAAAAAAAAAAEv/aAAgBAgEBPwGlv//EABgQAAMBAQAAAAAAAAAAAAAAAAAQMQER/9oACAEBAAY/ArwqxU//xAAaEAEAAwEBAQAAAAAAAAAAAAABABEhMUFx/9oACAEBAAE/IVCGTVXMNb2uRKvm4xNvrMHfIVx//9oADAMBAAIAAwAAABA87//EABcRAAMBAAAAAAAAAAAAAAAAAAABEVH/2gAIAQMBAT8QqSpGH//EABYRAQEBAAAAAAAAAAAAAAAAABEAAf/aAAgBAgEBPxDdDK//xAAZEAEAAwEBAAAAAAAAAAAAAAABABEhMaH/2gAIAQEAAT8QJitAD6QHU1Acc6kOy2hogMZcaNTrWRtgrbreuwIBT//Z'); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"Visional\" title=\"\" src=\"/static/e750cc63f74625ca0094f46ebeacaf20/1c72d/visional_way.jpg\" srcset=\"/static/e750cc63f74625ca0094f46ebeacaf20/a80bd/visional_way.jpg 148w,\n/static/e750cc63f74625ca0094f46ebeacaf20/1c91a/visional_way.jpg 295w,\n/static/e750cc63f74625ca0094f46ebeacaf20/1c72d/visional_way.jpg 590w,\n/static/e750cc63f74625ca0094f46ebeacaf20/a8a14/visional_way.jpg 885w,\n/static/e750cc63f74625ca0094f46ebeacaf20/8a8a2/visional_way.jpg 1008w\" sizes=\"(max-width: 590px) 100vw, 590px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n  </a>\n    </span>\n</p>\n<p>Established in April 2009, BizReach has been operating a variety of Internet services\nthat support the future of work in Japan with a mission that roughly translates into\n“creating a society where everyone can believe in their own potential”. The company,\nheadquartered in (Shibuya) Tokyo, has regional offices in Osaka, Nagoya, Fukuoka, Shizuoka,\nand Hiroshima.</p>\n<p>Visional was born in February 2020, when Bizreach Inc. shifted to a group management\nstructure. The company’s main focus is primarily in the HR Tech and SaaS business that promote\ndigital transformation (DX) of industry and support the improvement of productivity in Japan.\nThe group broadly has a heirarchical structure with the following products and services:</p>\n<blockquote>\n<p><a href=\"https://www.visional.inc/ja/index.html\">Visional Corporation</a>, the holding company, supports the group management</p>\n</blockquote>\n<blockquote>\n<p><a href=\"https://www.bizreach.co.jp/\">Bizreach Corporation</a>, responsible for the management of the original HR Tech and SaaS businesses</p>\n</blockquote>\n<ul>\n<li><a href=\"https://www.bizreach.jp/\">Bizreach</a>, a job-change website that connects companies with human resources with immediate availability</li>\n<li><a href=\"https://www.careertrek.com/\">CareerTrek</a>, a job search site for young graduates</li>\n<li><a href=\"https://hrmos.co/\">HRMOS</a>, a human resources management cloud platform</li>\n<li><a href=\"https://br-campus.jp/\">Campus</a>, an alumni network service for career consultation</li>\n</ul>\n<blockquote>\n<p><a href=\"https://www.visional.inc/ja/visional-incubation.html\">Visional Incubation Corporation</a>, responsible for new business development and acquisitions</p>\n</blockquote>\n<ul>\n<li><a href=\"https://br-succeed.jp/\">Succeed</a>, an M&#x26;A platform for business succession</li>\n<li><a href=\"https://yamory.io/\">Yamory</a>, security vulnerability management cloud for IT systems</li>\n<li><a href=\"https://bizhint.jp/\">BizHint</a>, a website providing “business tips” in the form of keywords, case studies and solutions</li>\n<li><a href=\"https://binar.jp/\">BINAR</a>, a career change platform for highly specialized IT Engineers</li>\n<li><a href=\"https://jp.stanby.com/\">StanBy</a>, a job-search engine jointly owned with Yahoo Japan</li>\n</ul>\n<p>Each of the products have their own specific engineering teams that follow custom Agile\nsoftware development life cycles (SDLCs) they are comfortable with.</p>\n<p>The <strong>AI Team</strong> is a team of around 20 ML/AI engineers that collaborate with one or more of the\nabove mentioned teams to understand their business requirements, and leverage data to deploy\nAI features in production environments to drive business solutions.</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\"></code></pre></div>\n<h2 id=\"projects\">My Projects</h2>\n<p>As an engineer in the AI team, I have to actively communicate requirements and decisions on\nvarious ML pipelines with multiple service teams simultaneously. I usually have 1-2\nservice-related projects and an independent research project in a single half-year term.</p>\n<p>From my experience, I believe that the engineers working in the AI Team at Visional switch\nfrequently between the roles and responsibilities of a <strong>Data Scientist</strong>, an <strong>ML Engineer</strong>,\nas well as a <strong>Research Scientist</strong> for different projects undertaken at a time.</p>\n<p>An average ML project includes the following:</p>\n<blockquote>\n<ul>\n<li>understanding the requirements of the service teams</li>\n<li>analyzing data to optimize and improve services</li>\n<li>developing custom data models, algorithms and full-stack systems</li>\n<li>collaborating with service teams to deploy the AI features in production</li>\n<li>considering evaluation metrics/systems to quantify/monitor outcomes</li>\n</ul>\n</blockquote>\n<p>The projects require a good understanding of ML techniques, algorithms and statistics. Most\nof the work also involves Natural Language Processing techniques. I have also been able to\napply my Computer Vision skills to a few projects.</p>\n<p>Following are few of the projects I have worked on so far.</p>\n<h3 id=\"p1\">Financial Document OCR System</h3>\n<p><a href=\"https://br-succeed.jp/service\">BizReach Succeed</a> provides an M&#x26;A platform to match companies\nlooking for succession with potential buyers. As part of their services, they have to analyze\nfinancial documents, which are most often personally handed over as hard-copies by the clients.\nThese documents are then scanned and the data is manually entered into spreadsheets for\nfinancial analysis typically resulting in business valuations.</p>\n<p>Since Succeed is a relatively small team, the manual data-entry efforts can take upto\n<strong>100hrs/month</strong>. Data-entry basically involves entering tabular data into spreadsheet. This\ntabular data is mostly present within PDF scans. Current OCR systems are well equipped to\ndetect text in an image, however, they fail to preserve spatial information such as table cells.</p>\n<p>I developed an <strong>OCR system</strong> capable of <strong>preserving tabular structure</strong> within the image.\nThe system was built using <a href=\"https://pypi.org/project/opencv-python/\">OpenCV</a> and\n<a href=\"https://cloud.google.com/vision/docs/ocr\">Google Vision API</a> in <strong>Python</strong>, and deployed\nusing <strong>Docker</strong>, <strong>Terraform</strong> and <strong>AWS</strong>. Some text post-processing was implemented using\na dictionary of Japanese financial terms.</p>\n<p>The pipeline roughly consists of the following steps:</p>\n<blockquote>\n<ul>\n<li>Parsing OCR request from AWS SQS and AWS S3</li>\n<li>Getting page images from PDF and fixing text orientation</li>\n<li>Table detection and extraction from page image</li>\n<li>OCR Text Detection request to Google Vision API</li>\n<li>Alignment of Google Vision API annotations</li>\n<li>Proofreading result text using a dictionary</li>\n<li>Save output XLS workbook to AWS S3</li>\n</ul>\n</blockquote>\n<p align=\"center\">\n  <span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; margin: 0 0 30px;\">\n      <a class=\"gatsby-resp-image-link\" href=\"/static/a1bcd07a05bdf262fc28224f19e40278/37114/focr.png\" style=\"display: block\" target=\"_blank\" rel=\"noopener\">\n    <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 45.27027027027027%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAABYlAAAWJQFJUiTwAAABYklEQVR42k1Sy3KDMAzk/z8uJc0AoecEUoa3bTDYoGrlMVMu2JJ2tVo5cc7ReZx0HAft+06444z/tm3XHTncvfcc2yWW3u90u91oGEeq6orarqUEifM8Bbgsi4DiHSTeg8zRuloyZrnuB4u4M2FRFIRvnCauWSkBeOQObdtKou976R6UOLLbTq/XS3Jaa1JKCynyWZbT8/kUAUopbmgCodaGSQchmrhTGN3LH2rQDAQAzPMsMZCmaUplWQoO0wkhOoOkaRqy1sr55Ng6T6S7jjYGV++35ACwPBbI1W9D3zxynufSGOpRIwqHYRBC+NcxiSwFS+ACazeq68+lDkD4h3z6lfLYGZ8dGVaIZkn0CsXwIfikro1j9EiGsZCHRWgK/zByjBujg0IEAEARwGgSnwf86vvgr+VNo9a5sJSy/JEtx9ehWcil8P/7i38UYqvTNMuTwTluE0t5PB5C6LkeViH3B1lHtdyi/PVPAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"Financial Document OCR\" title=\"\" src=\"/static/a1bcd07a05bdf262fc28224f19e40278/fcda8/focr.png\" srcset=\"/static/a1bcd07a05bdf262fc28224f19e40278/12f09/focr.png 148w,\n/static/a1bcd07a05bdf262fc28224f19e40278/e4a3f/focr.png 295w,\n/static/a1bcd07a05bdf262fc28224f19e40278/fcda8/focr.png 590w,\n/static/a1bcd07a05bdf262fc28224f19e40278/efc66/focr.png 885w,\n/static/a1bcd07a05bdf262fc28224f19e40278/c83ae/focr.png 1180w,\n/static/a1bcd07a05bdf262fc28224f19e40278/37114/focr.png 2300w\" sizes=\"(max-width: 590px) 100vw, 590px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n  </a>\n    </span>\n</p>\n<h3 id=\"p2\">Research: Japanese handwritten OCR</h3>\n<p>In many ways, hand-filled forms are still a part of Japan’s official processes. For instance,\nour company requires candidates to fill out their details for registration. Since it is\nimperative that we make no mistakes while entering the details into our systems, this information\nis then manually filled by the operator. However, to me, it looked like an opportunity to explore and further research on Japanese\nhandwritten OCR systems for the AI team.</p>\n<p>The research focusses on the address field in the BizReach registration forms, and aims to\nharness the structure present in addresses to explore domain augmentation methods. A\n<a href=\"http://jusyo.jp/index.html\">list of Japanese addresses</a> distributed by Japan Post was used\nto synthetically generate the train dataset. The textual addresses were transformed into\naddress images using character images available in the <a href=\"http://etlcdb.db.aist.go.jp/\">ETL</a>\nand <a href=\"https://www.nist.gov/itl/products-and-services/emnist-dataset\">EMNIST</a> datasets.\nData augmentation was introduced to account for variances in handwriting:</p>\n<blockquote>\n<ul>\n<li>shape of characters</li>\n<li>spacing between characters (narrow or wide)</li>\n<li>rotation of characters (straight or slant)</li>\n<li>size of characters (big or small)</li>\n<li>stroke strength of characters (thick or thin)</li>\n</ul>\n</blockquote>\n<p align=\"center\">\n  <span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; margin: 0 0 30px;\">\n      <a class=\"gatsby-resp-image-link\" href=\"/static/c43fe12e2dd8fa820d1e6f547436e982/cb93d/hocr_sent.png\" style=\"display: block\" target=\"_blank\" rel=\"noopener\">\n    <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 10.81081081081081%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAACCAYAAABYBvyLAAAACXBIWXMAABYlAAAWJQFJUiTwAAAAfklEQVR42h2MsQqFIAAA+/+fag60jCi1SJEoCmtoaAiDezynu+Wu2Pedvu+Z5xlrLVprtm1jWRamaUIIgVIK7z12HJF1zTAMuYkxZoYQ+L6PlBLF8zyc58l93xzHkYd/rutK13WUZYmUkrZtMcZQVRVN02S/riu7c473ffPwB3Q1kAXeRNNMAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"Sentence Image\" title=\"\" src=\"/static/c43fe12e2dd8fa820d1e6f547436e982/fcda8/hocr_sent.png\" srcset=\"/static/c43fe12e2dd8fa820d1e6f547436e982/12f09/hocr_sent.png 148w,\n/static/c43fe12e2dd8fa820d1e6f547436e982/e4a3f/hocr_sent.png 295w,\n/static/c43fe12e2dd8fa820d1e6f547436e982/fcda8/hocr_sent.png 590w,\n/static/c43fe12e2dd8fa820d1e6f547436e982/efc66/hocr_sent.png 885w,\n/static/c43fe12e2dd8fa820d1e6f547436e982/c83ae/hocr_sent.png 1180w,\n/static/c43fe12e2dd8fa820d1e6f547436e982/cb93d/hocr_sent.png 2304w\" sizes=\"(max-width: 590px) 100vw, 590px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n  </a>\n    </span>\n</p>\n<p>The <a href=\"https://arxiv.org/abs/1507.05717\"><strong>Convolutional Recurrent Neural Network</strong> (CRNN)</a>\narchitecture was chosen. The end-to-end model, that combines CNN (image feature extraction),\nRNN (sequence recogntion) and CTC loss, was implemented in <strong>PyTorch</strong>.</p>\n<p>The lexicon-free model was able to achieve a <strong>44% lower CER (character error rate) than the\n<a href=\"https://cloud.google.com/vision/docs/handwriting\">Google Vision API</a></strong> on a test set of actual\nhandwritten Japanese addresses. Currently, I am trying to improve the CER further using\nlexicon-based algorithms, and simultaneously documenting the methodology and results aimed at a\nfuture publication.</p>\n<p align=\"center\">\n  <span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; margin: 0 0 30px;\">\n      <a class=\"gatsby-resp-image-link\" href=\"/static/7b7d457b3595fa4f7980ade8951acec4/f5aa5/hocr_cer.png\" style=\"display: block\" target=\"_blank\" rel=\"noopener\">\n    <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 14.18918918918919%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAADCAYAAACTWi8uAAAACXBIWXMAABYlAAAWJQFJUiTwAAAAiklEQVR42oWOzQrEIAyE+/7vpwfpQVS0liq2VfojMksCu9cNDHGS8dMphICUEtZ15S6EwDzPWJYF1lporVmlFOScsW0baq3oveN5np/I024iUIyRgQSRUsIYg73sPKMQPUq71hqu62KNMfC+L8O+Z5pP3nv+AV1wzkEpxf48T9z3zSECHcfB/l99AEJX5EX8ubeuAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"CRNN Result CER\" title=\"\" src=\"/static/7b7d457b3595fa4f7980ade8951acec4/fcda8/hocr_cer.png\" srcset=\"/static/7b7d457b3595fa4f7980ade8951acec4/12f09/hocr_cer.png 148w,\n/static/7b7d457b3595fa4f7980ade8951acec4/e4a3f/hocr_cer.png 295w,\n/static/7b7d457b3595fa4f7980ade8951acec4/fcda8/hocr_cer.png 590w,\n/static/7b7d457b3595fa4f7980ade8951acec4/efc66/hocr_cer.png 885w,\n/static/7b7d457b3595fa4f7980ade8951acec4/c83ae/hocr_cer.png 1180w,\n/static/7b7d457b3595fa4f7980ade8951acec4/f5aa5/hocr_cer.png 2312w\" sizes=\"(max-width: 590px) 100vw, 590px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n  </a>\n    </span>\n</p>\n<h3 id=\"p3\">Employee Slack Analytics</h3>\n<p>The <a href=\"https://hrmos.co/\">HRMOS</a> team develops a variety of HR-Tech services, such as employee\ndatabases, attendance logging and performance tracking systems. Their focus is the improvement\nof productivity within client business teams.</p>\n<p>Each company has data on their employees which can be leveraged using AI to measure, in an\nabstract sense, the productivity of the employees. This data is usually available within the\nemployee database of the company and has a well-defined structure to it. In recent years,\nSlack, Workplace, Teams, etc have gained preference over emails, especially in the tech\nindustry.</p>\n<p><strong>Public conversations</strong> can be viewed as unconventional sources of data, which can\nbe processed using NLP techniques to quantify productivity. For instance, messages and\nreplies can be considered entities that signal a connection link between two employees.\nFrequency of interactions on public channels can signal the degree of involvement of a\ncertain employee in a certain topic.</p>\n<p>I built a simple GUI in <strong>Vue.js</strong> and <strong>Python</strong>, and deployed it using <strong>Docker</strong> for\ninternal HR to visualize activity and messaging trends on the Visional Slack workspace.\n<strong>Japanese text processing</strong> using <a href=\"https://taku910.github.io/mecab/\">MeCab</a> tagger and\n<a href=\"https://github.com/neologd/mecab-ipadic-neologd#overview\">NEologd</a> dictionary were used\nto clean the unstructured conversational data. The following are some of the features have\nbeen implemented in the GUI:</p>\n<blockquote>\n<ul>\n<li>Conversation topic transitions, to visualize the change over time in underlying topics of conversation, discovered using <strong>LDA topic modeling</strong></li>\n<li>Interactive User/Channel wordclouds, to visualize a summarized history of conversation keywords</li>\n<li>Matching similar users, based on high topic probabilities in conversations</li>\n</ul>\n</blockquote>\n<h3 id=\"p4\">HeadHunter Recommendation</h3>\n<p><a href=\"https://www.bizreach.jp/\">BizReach</a>, Visional’s primary service, is a job-change website that\nconnects individuals with recruiters and companies through job listings. Other than job search\nresults, the website’s homepage also recommends job listings to the candidates on the basis of\ntheir preferences, past activity and searches. From point of view of the recruiters, BizReach\naims to recommend candidates that are most likely to be scouted by them.</p>\n<p>The HeadHunter recommendation feature was proposed by the AI team, with the aim to increase\nscouting rate for headhunters. The feature was developed to assist existing recommendation\nsystems by using <strong>implicit feedback</strong> datasets like candidate-job access log data for newly\nregistered candidates, for whom there is insufficient scout data. The same data could be\nused to recommend candidates to recruiters who had posted the jobs.</p>\n<p>I was involved in the assessment of the effectiveness of using access log data for the problem\nand comparing the performance of various recommendation models in offline testing.\nA 25% hit rate was obtained using <a href=\"https://implicit.readthedocs.io/en/latest/quickstart.html\">Implicit</a>’s\n<a href=\"https://implicit.readthedocs.io/en/latest/als.html\">Alternating Least Squares</a> <strong>collaborative\nfiltering</strong> model for demonstration of the proposed project.</p>\n<h3 id=\"p5\">Security Bug Report Classification</h3>\n<p>Publicly disclosed cybersecurity vulnerabilities are assigned a\n<a href=\"https://www.cve.org/About/Overview\">Common Vulnerabilities and Exposures (CVE)</a> ID, and\nthe <a href=\"https://nvd.nist.gov/\">National Vulnerability Database</a> performs analysis on CVEs by\naggregating data points from the description, references supplied and any supplemental\ndata that can be found publicly at that time.</p>\n<p><a href=\"https://yamory.io/\">Yamory</a> develops a security vulnerability management cloud for IT systems.\nThe vulnerability database used by yamory consists of information analyzed and evaluated manually\nby a dedicated security analyst. To build this database, analysts have to scour multiple sources\nlike tweets, bug report websites, security articles, etc.</p>\n<p>We developed a system to scrape data from such sources and classify them as security vulnerability\nreports. Tweets mentioning a CVE ID were scraped and stored and analyzed using <strong>Elasticsearch</strong>\nwhereas <strong>Scrapy</strong> was used to scrape bug reports from active bug reporting websites.\n<strong>Named Entity Recognition</strong> was used to identify software names within the descriptions.\nA 0.81 F-score was achieved in classifying bug reports using a <strong>bigram language model</strong>\ntrained over NVD/CVE Descriptions as ground truth.</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\"></code></pre></div>\n<h2 id=\"experience\">My Experience</h2>\n<p align=\"center\">\n  <span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; margin: 0 0 30px;\">\n      <a class=\"gatsby-resp-image-link\" href=\"/static/16675411408cd4a3f027fde157aa25e6/c655d/first_day.png\" style=\"display: block\" target=\"_blank\" rel=\"noopener\">\n    <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 66.89189189189189%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAYAAACpUE5eAAAACXBIWXMAABYlAAAWJQFJUiTwAAADq0lEQVR42g3Sa0+TBwDF8X6C7cX2YurmTCVYpG2whQKlCh20QtrHrpZLlKI8tJSCFqhuAs4bFJApKIIwxAugIkNrBYPIRKwXZINKMVECYubi4mUmy77Cf88X+J2TkyNbCjWyON7L8oPrPL83wtJ8lMWnYeYmQizNPubNwjSfVhb4uPKSV3/8xqunI7yNjPF2/i6v58ZZWYry+t07Zmbvc2+0F1mvJ4kOj5ngMTfhjkMs3B5k+tc+Hg/8woP+Lv56MsF7CXodHuPh1QssTNzg/s0h5qXQ6IsIs8+mefIswpwEPo+GkVVuVXBxv51bfiMvu3JZak3nqqghXJvJ4bwUHjV+z7+TAZZDTXy44eN5Xyl7dgmMBq8QnhnlwcM7DEht5/57w/tP/yBrcsRxyr2Z6XYny1dKeXmphOCPViInBNp92wi32Pg47OLv0Qb+vOHnUXcxe11G7gT7GAt1MTVyiVBkkNDvw0QXI8jqxVW0FKsJNQo86XbR4xc457cxdFhgqLmca9IU0d5KplrdXA+Uc6veTneNhYnhDoYb3Iy0VjF53kdbtYXun8sk8EACpwNbGO4poLvWQVmumbKdBRypdtFSv5/zzQ38VO3n7A/FNFa5qRNzaa92MHjeT9Mekca6UvonjzHwqJmGTjMyT9BKzVwJ+0JONJr1xCg2kqXT0n90F8I2PeWe/Rz07KCqyMr2nCwqnXl0HqlAyPmO2HUxJGjk3IkUMf/By+3FXGT5/Sbs5wxkHNpIvOVr1qz5Cl9+Nt3HjpCsS8Zic5CdZZTCEjGkJiNut7DdYkW+Xs3aNXI0STE0Xc7g7JiJtluZyLIDKsxHFZjqNhC3aR1auZwUrRpvochWCTLptdSUOCRMgzEpAaVCzWeff8m3chV5ZhPJig2Y3Qp8F/SIp7TIhPp47MdVGHbHkmaNJX1HLAazlnJ7FrbMRNS6GLxOO6naJNbGrkalM6BNSMFsSEGhVJFql+M4EEfF2RTEkxKY15JAYbuW3V06yi6nUTGkxyamoTeoWa38AmORAUOmkfgNKjKcCuLNqxD0yVRY0lGbvsHVpsTbmUip1K74xCZkFX0GaoOZBMZzaJ2UvjclkOvcws69WRQc3Ey+z4bgEhD2KRE7Eqk6pyf/kBKbT5qqJA7X6U24O7WIbVKxpo3Ijt8V6J3J51q0kNCLIsaWRc6MF9I2uZtLK17OPPNSe1Ok500ZJ+cKCExZOXrXTM2Ikb0Daey5mIanJxWxU8fO4yr+B5bgbB5DkNAwAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"Placement Meme\" title=\"\" src=\"/static/16675411408cd4a3f027fde157aa25e6/fcda8/first_day.png\" srcset=\"/static/16675411408cd4a3f027fde157aa25e6/12f09/first_day.png 148w,\n/static/16675411408cd4a3f027fde157aa25e6/e4a3f/first_day.png 295w,\n/static/16675411408cd4a3f027fde157aa25e6/fcda8/first_day.png 590w,\n/static/16675411408cd4a3f027fde157aa25e6/efc66/first_day.png 885w,\n/static/16675411408cd4a3f027fde157aa25e6/c83ae/first_day.png 1180w,\n/static/16675411408cd4a3f027fde157aa25e6/c655d/first_day.png 1586w\" sizes=\"(max-width: 590px) 100vw, 590px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n  </a>\n    </span>\n</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\"></code></pre></div>\n<h2 id=\"selected\">How I got selected</h2>\n<h3 id=\"preparation\">Interview Preparation</h3>\n<p>After <a href=\"/blog/2018-ntu-intern\">my internship with ROSE Lab</a> in the summer of 2018,\nI immediately started to prepare for job interviews in the\n<a href=\"https://www.shiksha.com/b-tech/articles/iit-kharagpur-placements-blogId-20045#:~:text=placements%202018%20below.-,IIT%20Kharagpur%20placements,-are%20conducted%20for\">IIT Kharagpur Placement Season 2018</a>.\nPlacements are a time of extensive planning and preparation, and depending on their\ntargeted companies, students choose from various preparation strategies to get through.</p>\n<p>Fresh from a very interesting internship, I was determined to pursue a Machine Learning\n/ Software Engineering role, even though my major in Aerospace Engineering put me at a\nfair disadvantage compared to students of “circuit” (CS, EE, MA, etc) branches. In a batch\nof over 1000 applicants, companies had to apply such constraints in order to make the\nshortlist really “short”. This meant, that I would not be able to sit for the tests for\nthese companies, even though I was fairly confident in my skills. On top of this, the rules\nrestricted us from accepting more than one job offer in the entire season.</p>\n<p>Keeping in mind the above constraints, I decided to keep my preparation very general and\nopen to multiple opportunities. From information gathered from seniors as well as\ncompany introductory talks, I understood that most of the tests involved similar formats.\nOn the basis of this information, my preparation included brushing up on:</p>\n<blockquote>\n<ul>\n<li>Analytical/Probability skills: <a href=\"https://mbapreponline.files.wordpress.com/2013/07/fifty_challenging_problems_in__2.pdf\">Fifty Challenging Problems in Probability with Solutions</a></li>\n<li>Machine Learning theory: my notes from online courses on Coursera such as the <a href=\"https://www.coursera.org/learn/machine-learning\">Machine Learning course (Stanford University)</a> and the <a href=\"https://www.coursera.org/specializations/deep-learning\">Deep Learning Specialization (DeepLearning.AI)</a></li>\n<li>Data Structures/Algorithms: Practice questions on <a href=\"https://www.interviewbit.com/\">InterviewBit</a></li>\n</ul>\n</blockquote>\n<p>This way, I was able to prepare fairly well for both software and data science roles.</p>\n<p>Parallely, I drafted a one-page resume showcasing my projects and my internships, and got\nit reviewed by a lot of friends and seniors. After a few revisions, I submitted it to the\nplacement portal, and waited anxiously for the placement process to start.</p>\n<h3 id=\"placements\">Placement Interviews</h3>\n<p>Reality hit hard on Day 1, when I found out some companies I was hoping to be shortlisted\nin had actually used the “circuit branch” constraint, and I was only able to sit in 2 of\nthe 6 companies I applied for that day. My first interview went well, but I eventually\napologized and pulled out, since their Embedded Software Engineer role was not\nsomething I was very keen about. I was not able to perform well my second interview, for an\nanalyst position, since I had not prepared for the “Case Interview” style at all. In summary,\nDay 1 was a complete bust because of the restrictions on the placement process, and I was\ndisappointed with the limited options I had.</p>\n<p>Later that night, however, I was happy to find out that I cleared the shortlist for\nBizReach Inc., based in Tokyo, for the Machine Learning Engineer role. Day 2 is usually\nthe arena for companies that are new participants in the placements, and sometimes these\nopportunities are on-par or even better than the previous day. BizReach was one such\ncompany, and the job description was exactly what I had hoped for!</p>\n<p>I called up a senior who was working in Mercari, Tokyo (I had also called him up the\nprevious night, since I was furious Mercari was only looking for circuit branch students),\nand asked him some tips for interviewing with Japanese people. His input really helped\nme refocus my mind on this new opportunity. After some modest revision and self pep-talk,\nI called it a night.</p>\n<p align=\"center\">\n  <span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; margin: 0 0 30px;\">\n      <a class=\"gatsby-resp-image-link\" href=\"/static/b8282d589667f72957dc389589b5e6aa/cb1ac/thanos_meme.png\" style=\"display: block\" target=\"_blank\" rel=\"noopener\">\n    <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 39.86486486486486%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAYAAAD5nd/tAAAACXBIWXMAABYlAAAWJQFJUiTwAAACUUlEQVR42hWQ20uTARjGPw3Mw2zqNnUnv0/nNrZ0zdw8sAynzgOeymQz1DLzxrSJZaJ5qFwlGQWriwwxQbAgEAokdTqXFgUF0WVg/g/d2U39+rp63wfeh+f3vMLh4W8iO7tsRmNsbm8TiUaJffhIdHePtY1NtmO7/Pnzl/2DA9791+/3WI9ssxXdkX0xNrZ2ZN8Ob9fW+bH/E+HrSpjI/H3ehO8yP3ObZ/dmeBG6yVJogud3xnn58Bbri08Ih6aYHb/O8tykfHeDuclRwlMjLMh7eGqY8MQQi6FRhKVmA08bJGZrnQQsOfi0CvwmDT1mFQFTBv1OLcGTOpokBa0mJWOVBvrcWTjVR3Eo4qnKjqc8Mx6PKo6z+iMI3ydcrHYVEG4ro9NpocqYRofdyDV3LsNl+Ux67TxqLODVgJfFCy665IDWYokhn41aiwaLOpHC7FScmal02lQIv1Y8bFzMY6G9iF6XA68hQybJYrhYz3SFyON6K8v+41z1SHQWGekoM1Nj19EgJdNmT6POqsSlS6ZQlYDfrET4NpbH56CZ1+dtDJY7qJGy8Ikq6sV0mq0qHjRaifYX0OvW4xXVBEotXK62014iUiDTVRqS8eakUKxJoFE8Jv+wJYsvQYn1nlyuuESqZMJT2hSKVImUaJOozldy6UQG1dpEREGgzqjEXyrRXaLHpEhAnxRHbkq8PAU8+mSE1e4cYoM5fJq2MtKUT+dpK+dKDAQqjATP5NHnM1Bv1cghaZSKSgIuHQP1FkabzbS4tXhsaorkJlJ2Oq0OLf8ACkA/aolAnRwAAAAASUVORK5CYII='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"Placement Meme\" title=\"\" src=\"/static/b8282d589667f72957dc389589b5e6aa/fcda8/thanos_meme.png\" srcset=\"/static/b8282d589667f72957dc389589b5e6aa/12f09/thanos_meme.png 148w,\n/static/b8282d589667f72957dc389589b5e6aa/e4a3f/thanos_meme.png 295w,\n/static/b8282d589667f72957dc389589b5e6aa/fcda8/thanos_meme.png 590w,\n/static/b8282d589667f72957dc389589b5e6aa/efc66/thanos_meme.png 885w,\n/static/b8282d589667f72957dc389589b5e6aa/c83ae/thanos_meme.png 1180w,\n/static/b8282d589667f72957dc389589b5e6aa/cb1ac/thanos_meme.png 1936w\" sizes=\"(max-width: 590px) 100vw, 590px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n  </a>\n    </span>\n</p>\n<p>The next day, when I showed up for the interviews, and I was surprised to see only a few\ncandidates lined up. I had a quick judgmental glance at the candidates, and narcissitically\nreassured myself. I knew I was a slight bit overconfident, but in such a situation, it\nreally helped me to not ponder on my nerves. I ran simulations of my plan in my head while\nI waited.</p>\n<p>I was already impressed at how punctual the Japanese were at taking interviews. Each\ninterview went on for a total maximum of 30-minutes, with two exact 15-minute segments.\nThe first part was basically a 3-on-1 technical interview, involving a CV runthrough\nand some behavioral questions, whereas the second part was a coding round.</p>\n<p>My interview began at 1830hrs, and I went inside and greeted them, saying “Konnichiwa”\n(hello, in Japanese). They were delighted, and started talking in Japanese to test/tease me.\nI told them liked watching anime with subtitles, and I liked to pick up a few words here\nand there. I was happy with how the mood was from that point - light and cheery. They\nintroduced themselves as Takeuchi-san (CTO), Ikawa-san (VP of Engineering) and Dat-san\n(my future team lead).</p>\n<p>In the first part, I was mainly asked in detail about my thesis project, with some questions\nabout the image segmentation pipeline, and also my ROSE Lab internship. Other than\nwell-prepared answers to possible questions to my resume, I was quick to emphasize on my\nexperience working in an Asian working environment as well as my knowledge about Japan\nduring the behavioral questions. In general, I felt our interaction was positive and I was\noptimistic when I began the coding part. There were two questions of easy/medium difficulty,\ninvolving array manipulation and 2-pointers. I was easily able to work through them and\nwrite clean code in Python, which was the programming language I was most comfortable with\nas well as the language with which I was expected to work with if I were selected.</p>\n<p>I left at exactly 1900hrs after asking a few more questions I had about the company and the\nrole, and was told to wait outside for about an hour. At 2000hrs, all 3 interviewers came\noutside, and announced the three names they selected - and I was so relieved when I heard mine!\nThe interviewers left promptly after taking some pictures with us outside the venue. I called\nup my parents and my friends and shared the news. It was one of the happiest nights of my\nlife, and I did no injustice to celebrating that feeling! It was the result of hard work\nand meticulous planning, and most of all - being my genuine self.</p>","frontmatter":{"title":"A look back at my first full time job","date":"October 01, 2021","description":"2 years as an ML/AI Engineer at Visional Inc."}}},"pageContext":{"slug":"/blog/2021-visional/","previous":{"fields":{"slug":"/blog/2018-ntu-intern/"},"frontmatter":{"title":"Summer Internship at NTU Singapore"}},"next":{"fields":{"slug":"/blog/2020-aqi-dataviz/"},"frontmatter":{"title":"Diwali 2021 AQI Data Visualization"}}}},"staticQueryHashes":["63159454"],"slicesMap":{}}