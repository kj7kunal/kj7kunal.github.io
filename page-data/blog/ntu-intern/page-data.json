{"componentChunkName":"component---src-templates-blog-post-jsx","path":"/blog/ntu-intern/","result":{"data":{"site":{"siteMetadata":{"name":"Kunal Jain","title":"Kunal Jain | Machine Learning Engineer","description":"Machine Learning Engineer currently in Tokyo, Japan","about":"I'm a Machine Learning Engineer passionate about working in challenging and high-impact environments, and building automation pipelines that help save human time and effort. I take up side projects actively and like spending caffeinated lofi evenings working on them. I recently, and very reluctantly, started competitive programming for interviewing, but caved in to the benefits.","author":"kj7kunal","github":"https://github.com/kj7kunal","linkedin":"https://www.linkedin.com/in/kj7kunal","resume":"https://tinyurl.com/kunaljainresume","mail":"kjkunal7996@gmail.com"}},"markdownRemark":{"id":"c90705a5-1bed-57a3-acf1-4249b1f46b01","excerpt":"Having worked on 3 ML projects in my fourth year, I really wanted to take up the\nchallenge of collaborating within a distinguished research group, preferably…","html":"<p>Having worked on 3 ML projects in my fourth year, I really wanted to take up the\nchallenge of collaborating within a distinguished research group, preferably abroad.\nA foreign training or FT, as per my university lingo, was considered a prestigious\nopportunity for multifaceted exposure, as not only would you gain indispensable\nexperience working in an established institute set in an unfamiliar environment,\nit would also be really thrilling to interact with people from different cultural\nbackgrounds with similar interests to freely exchange ideas and experiences.</p>\n<p>Out of around 30 research labs that I applied to all over the world, I received\n3 offers. I was most excited to work at the\n<a href=\"https://www.ntu.edu.sg/rose\">Rapid-Rich Object Search (ROSE) Lab</a> at the\n<a href=\"https://www.ntu.edu.sg/index\">Nanyang Technological University, Singapore</a>,\nas not only did the lab’s research focus align with my interests, I would also be\nable to gain work experience in <a href=\"https://www.topuniversities.com/university-rankings/asian-university-rankings/2018\">one of the top universities in Asia</a>.</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">Contents\n\n- How I got selected\n  - Resume\n  - Application Process\n  - Interview Call\n\n- The Lab\n\n- The Project\n  - Objectives\n  - Motivation\n  - Implementation\n  - Result\n\n- My Experience</code></pre></div>\n<h2>How I got selected</h2>\n<h3>Resume</h3>\n<p>In my fourth year, I started to diverge my career away from aerospace engineering,\nto focus on my minor in Computer Science, especially in the areas of\nMachine Learning and Artificial Intelligence. As semester projects, I was able\nto work on some interesting applications of Machine Learning:</p>\n<ul>\n<li><a href=\"https://github.com/kj7kunal/ML_Project\">Mental Workload Estimation</a>:\nAs part of the Machine Learning course (CS60050), I trained various ML models to\nestimate mental workload of a subject playing the N-Back game while wearing an\nEmotiv BCI headset. The project included feature engineering using signal processing\ntechniques and resulted in a comparitive study between various ML models such\nas Decision Trees, Random Forests and Neural Nets.</li>\n<li><a href=\"https://github.com/kj7kunal/Artistic_Image_Rendering\">Artistic Image Rendering</a>:\nAs part of the Deep Learning course (CS60048), I decided to implement neural\nimage style transfer as described in the paper\n<a href=\"https://arxiv.org/pdf/1508.06576v2.pdf\"><em>A Neural Algorithm of Artistic Style by Gatys et al</em></a>.\nThis gave me a good understanding of how CNNs propagate signals, and how we can\nuse intermediate feature maps to extract representations of textures and color\nschemes from the lower layers, and of structure from the deeper layers. I also\nlearned about designing custom loss functions for the given optimization problem.</li>\n</ul>\n<p>Apart from curriculum coursework, I also independently took up online courses on\nCoursera such as the <a href=\"https://www.coursera.org/learn/machine-learning\">Machine Learning course (Stanford University)</a>\nand the <a href=\"https://www.coursera.org/specializations/deep-learning\">Deep Learning Specialization (DeepLearning.AI)</a>\ntaught by Prof. Andrew Ng, to improve my knowledge of basic ML/DL techniques\nthrough theory and coding assignments.</p>\n<h3>Application Process</h3>\n<p>The application process was pretty straightforward. Since I was mainly focussed on\ngetting research experience, I had compiled a list of around 30-40 research labs\nthat I wanted to work with and emailed their respective POCs.\nHowever, most of the replies were negative, partly because I applied very late\nin the year and partly because my background in aerospace engineering put me at\na disadvantage in a cohort of CS major students, which I feel was the case in my\napplications to research internship programs like DAAD and MITACS.</p>\n<p>Ultimately, I heard back from 3 places, one of which was ROSE Lab, a joint collaboration\nbetween Nanyang Technological University, Singapore, and Peking University, China.\nI was very surprised to have received a positive reply from them, as it was in the\ntop 10 of my list, which was basically an ambitious tier for me.</p>\n<p>After exchanging a few emails, we scheduled an hour-long interview call a week later,\nand I was asked to prepare a short presentation about 1~2 of my best projects.</p>\n<h3>Interview Call</h3>\n<p>I sent my presentation 2 days in advance of the interview call. On the Skype call, I\nconnected with the Deputy Director, <a href=\"https://sg.linkedin.com/in/dennissng\">Dr Dennis Sng</a>,\nwho oversaw projects undertaken by the ROSE Lab, <a href=\"https://warwick.ac.uk/fac/sci/dcs/people/xuuldl/\">Dr Lin Shan</a>,\na research fellow, who led the project I was supposed to work on, and of course\nthe lab’s POC, Ms Wang Qian, with whom I was so far in touch with.</p>\n<p>The interview call went on quite smoothly, and was structured mainly around my\npresentation. I was able to explain my contributions to my projects and answered\nrelated questions to the best of my knowledge. I was also questioned about my major,\nand why I was interested in Machine Learning, which I was also prepared to answer.</p>\n<p>I got the acceptance mail a week after the interview call, and I was super excited\nto start the visa process!</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\"></code></pre></div>\n<h2>The Lab</h2>\n<p>The <a href=\"https://www.ntu.edu.sg/rose\">Rapid-Rich Object Search (ROSE) Lab</a> is a joint\ncollaboration between NTU Singapore and Peking University, China. Its vision is to\ncreate the largest collection of structured domain object database in Asia, and to\nfurther image-based object search applications.</p>\n<p>The lab conducts research in the areas of computer vision, image processing, and\npattern recognition. It aims to develop scalable and robust mobile object search\nservices/applications involving areas of research like object recognition &#x26; retrieval,\ndeep learning &#x26; video analytics and multimedia forensics &#x26; biometrics. They have\ntaken the initiative of creating various <a href=\"https://www.ntu.edu.sg/rose/research-focus/datasets\">publicly-shared databases</a>\nto further research that they believe might create huge economic value and\nopportunities in the future.</p>\n<p>Projects undertaken by the lab were usually overseen by Dr Dennis Sng (Deputy Director\n&#x26; Principal Scientist​), and advised by <a href=\"https://dr.ntu.edu.sg/cris/rp/rp00653\">Prof Alex C. Kot</a> (Director).\nThe lab involves a workforce both from academia and industry, and hosts events\npertaining to the dissemination of knowledge on vision-based AI, particularly in\nobject search technology. </p>\n<p align=\"center\">\n  <span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; margin: 0 0 30px;\">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 53.37837837837838%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAALABQDASIAAhEBAxEB/8QAGAAAAgMAAAAAAAAAAAAAAAAAAAQCAwX/xAAWAQEBAQAAAAAAAAAAAAAAAAABAAL/2gAMAwEAAhADEAAAAa2lWAzyZZ//xAAbEAACAQUAAAAAAAAAAAAAAAABAgADERIUMv/aAAgBAQABBQKmmR1lhSxWFjH7/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPwE//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPwE//8QAGBAAAgMAAAAAAAAAAAAAAAAAAAERIDH/2gAIAQEABj8Cg2jP/8QAGRABAQADAQAAAAAAAAAAAAAAAQARMVGh/9oACAEBAAE/IQFSmeDsSmdW5MHLey//2gAMAwEAAgADAAAAEOw//8QAFxEAAwEAAAAAAAAAAAAAAAAAARARQf/aAAgBAwEBPxATV//EABURAQEAAAAAAAAAAAAAAAAAAAEQ/9oACAECAQE/EGf/xAAcEAEBAQABBQAAAAAAAAAAAAABEQAhMWFxgZH/2gAIAQEAAT8QRmDVfXfchXUSRnjHMNJyZIxuSlgWF+5uX//Z'); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"ROSE Lab\" title=\"ROSE Lab\" src=\"/static/f05d908a55236f3fa8c430978c4bb1f4/1c72d/ROSELabDoor.jpg\" srcset=\"/static/f05d908a55236f3fa8c430978c4bb1f4/a80bd/ROSELabDoor.jpg 148w,\n/static/f05d908a55236f3fa8c430978c4bb1f4/1c91a/ROSELabDoor.jpg 295w,\n/static/f05d908a55236f3fa8c430978c4bb1f4/1c72d/ROSELabDoor.jpg 590w,\n/static/f05d908a55236f3fa8c430978c4bb1f4/a8a14/ROSELabDoor.jpg 885w,\n/static/f05d908a55236f3fa8c430978c4bb1f4/fbd2c/ROSELabDoor.jpg 1180w,\n/static/f05d908a55236f3fa8c430978c4bb1f4/b536e/ROSELabDoor.jpg 1454w\" sizes=\"(max-width: 590px) 100vw, 590px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\">\n    </span>\n</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\"></code></pre></div>\n<h2>The Project</h2>\n<h3>Objectives</h3>\n<p>The project was a collaboration between ROSE Lab and the\n<a href=\"https://www.dsta.gov.sg/home\">Defence Science and Technology Agency (DSTA) Singapore</a>.</p>\n<p>The goal was to build a new dataset for Person Re-Identification (Person Re-ID) with\nthe aim of simulating the real world application domain as much as possible.\nThe plan involved capturing data with:</p>\n<ul>\n<li>\n<p>Scene Invariance</p>\n<ul>\n<li>Wide range of locations covered using outdoor public surveillance cameras</li>\n<li>Various times of the day (morning/afternoon/evening)</li>\n<li>Different weather conditions (sunny/cloudy/rainy)</li>\n</ul>\n</li>\n<li>\n<p>Clothing Invariance</p>\n<ul>\n<li>“Actors” advised to wear various types of clothing during data collection</li>\n</ul>\n</li>\n</ul>\n<p>My responsibilities in the project were to create tools to automate extraction and\nannotation of target “actor” images from surveillance video frames, to populate the\ninitial test dataset, which would later be benchmarked by existing Person Re-ID models.\nI worked closely with <a href=\"https://warwick.ac.uk/fac/sci/dcs/people/xuuldl/\">Dr Lin Shan</a>,\na PhD student who was leading the Re-ID project, as part of a two year secondment\nunder EU IDENTITY project between University of Warwick and NTU Singapore.</p>\n<h3>Motivation</h3>\n<p>Person Re-ID is defined as the problem of matching people across disjoint camera views\nin a multi-camera system. It is an important task in the field of intelligent security.\nA Re-ID system should be able to keep track of subjects (who are on a certain “watch-list”)\nin surveillance videos of multiple probable locations of re-appearance.</p>\n<p>It is an extremely challenging task due to a plethora of reasons:</p>\n<ul>\n<li>Pose/viewing angle difference</li>\n<li>Low Resolution CCTV footage</li>\n<li>Crowded Areas</li>\n<li>Occlusions</li>\n<li>Algorithm inadequacies (Detection errors / Real-time requirement)</li>\n<li>Unlimited/Open dataset task (Infinite number of classes for a classification problem)</li>\n</ul>\n<p align=\"center\">\n  <span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 350px; margin: 0 0 30px;\">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 64.86486486486486%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAANABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAAMBBf/EABUBAQEAAAAAAAAAAAAAAAAAAAEC/9oADAMBAAIQAxAAAAHsJ5BdBT//xAAcEAEAAgEFAAAAAAAAAAAAAAABAAIDERITISL/2gAIAQEAAQUC3aWVghWz7e5xUyT/xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/AT//xAAWEQADAAAAAAAAAAAAAAAAAAABEBH/2gAIAQIBAT8BNX//xAAbEAACAQUAAAAAAAAAAAAAAAAAASECEBFBYf/aAAgBAQAGPwJ7OCm0yZqR/8QAGxABAAICAwAAAAAAAAAAAAAAAQARIUExUYH/2gAIAQEAAT8hERu7itRljVocIMbY3mcBIJoYK6n/2gAMAwEAAgADAAAAEKvv/8QAFREBAQAAAAAAAAAAAAAAAAAAAQD/2gAIAQMBAT8QWL//xAAWEQADAAAAAAAAAAAAAAAAAAAAATH/2gAIAQIBAT8QeBQ//8QAHRABAAICAgMAAAAAAAAAAAAAAQARITFBsXGBof/aAAgBAQABPxBQAZHDWvjG68aC9PmOj9ABr3DYgZhRuzmUYKDZeOoZyMZJQ13P/9k='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"ReID system\" title=\"ReID system\" src=\"/static/b20075882272da83374658ea9f71803a/70ebb/reid_system.jpg\" srcset=\"/static/b20075882272da83374658ea9f71803a/a80bd/reid_system.jpg 148w,\n/static/b20075882272da83374658ea9f71803a/1c91a/reid_system.jpg 295w,\n/static/b20075882272da83374658ea9f71803a/70ebb/reid_system.jpg 350w\" sizes=\"(max-width: 350px) 100vw, 350px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\">\n    </span>\n</p>\n<p>My task in the first week of the internship was to familiarize myself with the current\nSOTA and identify strengths and shortcomings of popular public Person ReID datasets.</p>\n<p align=\"center\">\n  <span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; margin: 0 0 30px;\">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 31.756756756756754%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAGCAYAAADDl76dAAAACXBIWXMAABYlAAAWJQFJUiTwAAAA10lEQVQY012QbQ+DIAyE/f8/0wWdBlAB8TXpeGpYln1oWu6O3kHzalsJIWgZ08l5ntIZI957WZalYEbWdZW+7yXGVLBZhmGUfd9lHJ8+TZNqmRvvnQ7btunF67p0+X3fkssZDizGqB3NcRyldsk5awA6mC4kDQ4pRXVB4JxTAYvBmEnMBbCcH/PKLfOsGIub97svRFABKRGklDQNvTojvu4nKRjnf70udM4qQZEUsD4ZE11UxJWr30DVL/o1bqy1ZUFUkmdB1g8mDcUMx5yooqnGBIkhfM0/VTXOy7VraowAAAAASUVORK5CYII='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"Dataset Comparison\" title=\"Dataset Comparison\" src=\"/static/9ca886f117ee1bf5671a5074928286a4/fcda8/datasets.png\" srcset=\"/static/9ca886f117ee1bf5671a5074928286a4/12f09/datasets.png 148w,\n/static/9ca886f117ee1bf5671a5074928286a4/e4a3f/datasets.png 295w,\n/static/9ca886f117ee1bf5671a5074928286a4/fcda8/datasets.png 590w,\n/static/9ca886f117ee1bf5671a5074928286a4/efc66/datasets.png 885w,\n/static/9ca886f117ee1bf5671a5074928286a4/c83ae/datasets.png 1180w,\n/static/9ca886f117ee1bf5671a5074928286a4/6c86f/datasets.png 1720w\" sizes=\"(max-width: 590px) 100vw, 590px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\">\n    </span>\n</p>\n<p>I took a note of image sizes and attributes (such as gender, clothes, accessories, etc.)\nin these datasets. I found out that these datasets had small camera networks, therefore\nless variance in scenes. We were able to expand the network, as well as create a more\nrealistic surveillance setting, having secured access to public CCTV cameras:</p>\n<ul>\n<li>~80 cameras spread over 34 locations in NTU Singapore Campus (thanks to NTU)</li>\n<li>\n<p>~50 cameras spread over 23 locations in Singapore (thanks to DSTA)</p>\n<ul>\n<li>Orchard Road (Shopping Area), CBD (Business Area) and Civic District (Tourist Area)</li>\n</ul>\n</li>\n</ul>\n<p align=\"center\">\n  <span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; margin: 0 0 30px;\">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 14.864864864864865%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAADCAYAAACTWi8uAAAACXBIWXMAABYlAAAWJQFJUiTwAAAA5klEQVQI1w3Nz06CAADAYV6qB+jUoa2DmhdbjObIPyluUSKiksHgEIqrLBHN5TIiI1duHT36WL88f4dPkEsSmWya/HmGVqeIrtVIpY8oVQtUlQrBzKX35NA2VfYP9gjDB0zTwrq16WoKs+mYM0lEzB0TPI8QjHYTvzegvsMrTcZoXKNeGlycZmnpOrP1gpteHc9xyUspvDub3ImM43pYDZX5ZEitUmbQv2ez2SJ8LVeslgmL1ze+l58EI5/pJCKO/livt/Rf5oiFQ7qd5i5r8xG9Yzs+ckGhWK7y+5PwOAyJ44RxGPIPlSiYyvXI6g0AAAAASUVORK5CYII='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"CCTV footage\" title=\"CCTV footage\" src=\"/static/e8ec9b4d67d866c0348ad2ea3c12002b/fcda8/cctv.png\" srcset=\"/static/e8ec9b4d67d866c0348ad2ea3c12002b/12f09/cctv.png 148w,\n/static/e8ec9b4d67d866c0348ad2ea3c12002b/e4a3f/cctv.png 295w,\n/static/e8ec9b4d67d866c0348ad2ea3c12002b/fcda8/cctv.png 590w,\n/static/e8ec9b4d67d866c0348ad2ea3c12002b/efc66/cctv.png 885w,\n/static/e8ec9b4d67d866c0348ad2ea3c12002b/c83ae/cctv.png 1180w,\n/static/e8ec9b4d67d866c0348ad2ea3c12002b/b6e50/cctv.png 1862w\" sizes=\"(max-width: 590px) 100vw, 590px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\">\n    </span>\n</p>\n<p>Moreover, I learned that most of these datasets were hand-annotated, which ensured\nhigh accuracy, but led to a smaller dataset and longer time to create one. Since I had\nthe fortune of working on the project in 2018, I decided to automate Person Detection\nusing Machine Learning.</p>\n<h3>Implementation</h3>\n<p>After the literature reviews, I drafted a roadmap for the project with my mentor,\nwhich resulted the following to be implemented.</p>\n<h4>Dataset collection strategy: NTU ReID WebApp</h4>\n<p>We were considerate about privacy from an early stage, and developed what was called\nthe <strong>Privacy-aware user-driven dataset collection strategy</strong>. This involved a mobile\nweb-app designed using the <strong>Google Maps API</strong>, <strong>Flask</strong> and <strong>Gunicorn</strong> in <strong>Python</strong>.\nIt provided an easy interface for the volunteering “actors” in the dataset. </p>\n<p align=\"center\">\n  <span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; margin: 0 0 30px;\">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 37.16216216216216%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAYAAAAIy204AAAACXBIWXMAABYlAAAWJQFJUiTwAAABqUlEQVQoz0WRy27TQBhG84QseBk2gNTSFKqyKM+B2LBAQJGomqhcqjRKQqMGaBPbsWN7Zny348SGw6+w6EifNJs58106eZpgjEZFBq01OvDxRkOcwSWe55PnGf3eZw6eH3Pw4iXdw0Oe7HX58PGU9bqirmvyoiBOErabmk6WxmijCFwHfzJCacNqPML++oX5dIanYso8ptpsSestf9YpW/mkbhqyLKNp2h20rCrathFgkaHtBbPzM6zJGO+ih/PpPavrH4SWjVJLSeAS+iuuh0OWC5fp9CfLIKAUZ237H1hvNnLf0oniCMe2cL71sa8uGb95zd27twSWhU4UoXtDkiX4Et8WWJ5KNSYglqriOCZJUoqi3LltGwEa6U0JVN3eiAuXxWTC/Owc9+qCxMzxtUuWR6xWijT02JSGSPpK8hyTFYRG3uqINK8oqrUAZZBQFFhzllL+rbfk19136XRGqAKRFkCElsihY5MmhiD0yaoSNi0t8Jf7Iw4VypjdKM7ZKd6kj45+S5xUFN1LXHmRRNQrWT5mMOjx6MFDnj3do3t0zOP9fU5enfAP7zz98ZbXwTsAAAAASUVORK5CYII='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"Mobile WebApp\" title=\"Mobile WebApp\" src=\"/static/0ab020da5b819df56e10eb434f8e3a1d/fcda8/daq_ntu.png\" srcset=\"/static/0ab020da5b819df56e10eb434f8e3a1d/12f09/daq_ntu.png 148w,\n/static/0ab020da5b819df56e10eb434f8e3a1d/e4a3f/daq_ntu.png 295w,\n/static/0ab020da5b819df56e10eb434f8e3a1d/fcda8/daq_ntu.png 590w,\n/static/0ab020da5b819df56e10eb434f8e3a1d/efc66/daq_ntu.png 885w,\n/static/0ab020da5b819df56e10eb434f8e3a1d/c83ae/daq_ntu.png 1180w,\n/static/0ab020da5b819df56e10eb434f8e3a1d/3c492/daq_ntu.png 1300w\" sizes=\"(max-width: 590px) 100vw, 590px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\">\n    </span>\n</p>\n<ul>\n<li><em>Privacy-aware</em>: Only collected images of participants who accepted the privacy policy</li>\n<li>\n<p><em>User-driven</em>: The actor could indicate when they were passing through the FOV of\na given surveillance camera, reducing annotation effort:</p>\n<ul>\n<li>the web-app automatically recorded time-stamps which could be matched in the surveillance\nvideos to extract 1-min clips for the particular actor</li>\n<li>the actors enterred their own accurate appearance attributes into a form</li>\n</ul>\n</li>\n<li><em>Collection Strategy</em>: The web-app would display active paths which could be walked\non a particular day. This reduced the number of CCTV footage archives that had to\nbe accessed in a day.</li>\n</ul>\n<p>This approach proved to be an important USP of our system, since a year later, in 2019,\ndue to controveries about privacy and consent, the DukeMTMC dataset was\n<a href=\"https://www.dukechronicle.com/article/2019/06/duke-university-facial-recognition-data-set-study-surveillance-video-students-china-uyghur\">shut down</a>\nand MSMT17 has to release a new version to mask up the faces of all pedestrians involved.</p>\n<h4>Dataset creation: YOLOv3 -> ResNet50 -> Cosine Similarity</h4>\n<p>From the previous step, we could obtain 1-min video clips of the actors in the scene.\nThe next step was to extract the target actor’s images from the video frames and\nannotate them with the IDs and attributes of the target actor. The proposed pipeline\ninvolved two main components - Person Detection and Person Retrieval.</p>\n<p>The first task was to isolate regions corresponding to people within a video frame.\nA person detector was built using the <a href=\"https://pjreddie.com/darknet/yolo/\"><strong>YOLOv3 architecture</strong></a>\nin <strong>PyTorch</strong>, and modified to return region proposals (bounding boxes) corresponding\nto the “Person” class. The YOLOv3 system was much faster than Deformable Part Models\n(DPM) used in Market-1501 and Faster-RCNN architecture used in MSMT17, and had a\nmAP of 57.9% on <a href=\"https://paperswithcode.com/sota/object-detection-on-coco\">COCO test-dev</a>.\nFor more details, I recommend you to read the paper,\n<a href=\"https://pjreddie.com/media/files/papers/YOLOv3.pdf\">YOLOv3: An Incremental Improvement</a>,\nas it is a very interesting and fun read! </p>\n<p>The YOLOv3 Person Detection system was able to achieve 10fps processing speed on a\nNvidia GTX 1070 GPU, for detecting and annotating bounding boxes within the surveillance video.</p>\n<p align=\"center\">\n  <img src=\"/3b7f63496ff1b2ac1c4a440a161d2f44/person_detect.gif\" alt=\"Person Detection Demo\">\n</p>\n<p>The next step was to separate (retrieve) images of the target person from multiple YOLOv3\ndetections. Since this was basically a simple tracking problem in a single video, we use\na feature extraction with distance metric two-stage pipeline.</p>\n<p>A <a href=\"https://pytorch.org/hub/pytorch_vision_resnet/\">ResNet50 CNN (pre-trained on ImageNet)</a>\nwas used to extract robust feature representations of detected pedestrians. The user\nfirst had to select the target person image, called the probe/query image, and tracking\nwas achieved by ranking the <a href=\"https://en.wikipedia.org/wiki/Cosine_similarity#Definition\">cosine similarity score</a>\nof the 2048-D feature embeddings (after the global max-pooling layer) between query\nand YOLOv3 detected images. </p>\n<p>Since the video was continuous, a weighted average of the original query image and\nthe detected target image tracked in the last frame was used as the new query image.</p>\n<p align=\"center\">\n  <span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; margin: 0 0 30px;\">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 65.54054054054055%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAYAAACpUE5eAAAACXBIWXMAABYlAAAWJQFJUiTwAAAB/0lEQVQ4y41Ta2vbUAzt/4exL4UxaD+MwdhWUiiFdbC1SzbatAldlqRNHPLyK7bztp3YsWPHZ9J1kzUjaycQXElX5+roSnsgsSwLruvyEavVCkmSCOUzC8d6pgmtZ2A8HosYy/reY93jwFXuAkq3jaekImvI3kn84k6gLcDTj+9RKhYQhCGGwyH6gwEGoxGsfh+TyQSB76OmGbhpyf+sbAvw5OgIhes8bMeB3uuhK0mQa/foNJspKNHUBiN0Sf8L8DiTgVSv/+FHlcJ1OHPjutdMfK81nweMokiAMVX+hFUcI6ZATIkx2THZ7Pf8BWaej4TvPCjHNrqKU0Cf+jO1bWSzOZRKpSe+JcFzIgA9z4NDY7H/8gUOX+3Dnc1g0qewTqdTzOdzqKoqeqtoGnRdh00FRNESLbMOSa+irlVgTBUBShV6VKGDzLu3eHPwWlwedtoYkNoEyLaiKDA6HWgNCaqmkt/GIvBx27lCvnGBS+kcDauSAq7LrVYrKFfKqREE6ceQcI/TiY/Z2NBbLpe7KXMCH76cneHr50+iUwH5wigWSSEBi+156OJ6gzYPJX8BMiVZVfDt+BQfDg5hWCZUoqe02zBp3daVPB4NBnHnDu6Un7RBRZTlAvRJd5tyrXiNfO5cjE1oGgj71k6w9R4vQh+/uje4bV+i2PoBeZTO6G9zBN7+cZNMAAAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"Retrieval flow\" title=\"Retrieval flow\" src=\"/static/1167b2bcc6729a0c512f248ac434dd14/fcda8/retrieval.png\" srcset=\"/static/1167b2bcc6729a0c512f248ac434dd14/12f09/retrieval.png 148w,\n/static/1167b2bcc6729a0c512f248ac434dd14/e4a3f/retrieval.png 295w,\n/static/1167b2bcc6729a0c512f248ac434dd14/fcda8/retrieval.png 590w,\n/static/1167b2bcc6729a0c512f248ac434dd14/efc66/retrieval.png 885w,\n/static/1167b2bcc6729a0c512f248ac434dd14/ea64c/retrieval.png 1116w\" sizes=\"(max-width: 590px) 100vw, 590px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\">\n    </span>\n</p>\n<p>The Person Tracking system was able to a maximum of 5fps processing speed on a Nvidia\nGTX 1070 GPU, for the entire pipeline, which involved person detection, target matching\nand annotating bounding boxes within the surveillance video. Some of the developmental\nscripts are available in this <a href=\"https://github.com/kj7kunal/Person-detect-identify\">Github repo</a>.</p>\n<p align=\"center\">\n  <img src=\"/dae77eeedf2912c3b123f7771464ebb2/person_track.gif\" alt=\"Person Tracking demo\">\n</p>\n<h3>Result</h3>\n<p>The new Rose-IDentification-Outdoor (Re-ID-Outdoor) dataset was collected and\nannotated. The dataset was collected from 50 real surveillance cameras in NTU\nand came with privacy consideration from all participants (volunteers in the campus).\nOverall, the Re-ID-Outdoor dataset was considered the most realistic and also\nthe only privacy-aware public dataset for Person Re-ID research so far.</p>\n<p>A similar dataset was also created from surveillance cameras in different areas\nin Singapore, with actors consisting of ROSE Lab members. However, I am not sure\nif that dataset was processed or released.</p>\n<p><a href=\"http://wrap.warwick.ac.uk/143315/1/WRAP_Theses_Lin_2019.pdf\">Dr Lin Shan’s thesis</a>\nexplains his work on Person Re-ID meticulously, and is worth going through to understand\nhow the work was carried forward. Some of the images were taken directly from\nthe thesis for ease of writing this blog.</p>\n<p align=\"center\">\n  <span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; margin: 0 0 30px;\">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 20.27027027027027%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAECAYAAACOXx+WAAAACXBIWXMAABYlAAAWJQFJUiTwAAABOElEQVQY0w2PPUsCAQBA70e1tIYR0YeRVmQEfqV16p2H2oemHmQmQQ1xIEFQg0QN1hL0QQRRRNjXFTUkZoKVZEROUS2vGx5vecsTwk4HEc8Ijh4LQYcLyemh1zrEQiaBOK5xo98zEUuRW88zHZ0iFk1yeHDC0fEeIbcVNeQjKdkZtJmRZQVhuMOMrbObvpY27GYL1vZOmppNiN4x+kLrnBQekJUE8eQcs+osXrfC6somZ4VTMsExoqILLelFHHUwYLEiTEXCKJJEbDKCGo8T9PuZCIjklpcY6Leh63dk5xdJqTMossyIO8Da6gaFqwu0rEY6ncZlXPp9IqbWLoTXlyql4iOlUonyk4HhW13no16n8dXg7/eXz9qb0RQpGlQqFb6/f6i9V7m8Pue5XGZ/d4eD/T228tv8A+Nw2OaF06W0AAAAAElFTkSuQmCC'); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"Result\" title=\"Result\" src=\"/static/dc33986bf61d0e9dded6c3da5c8a9bfe/fcda8/result.png\" srcset=\"/static/dc33986bf61d0e9dded6c3da5c8a9bfe/12f09/result.png 148w,\n/static/dc33986bf61d0e9dded6c3da5c8a9bfe/e4a3f/result.png 295w,\n/static/dc33986bf61d0e9dded6c3da5c8a9bfe/fcda8/result.png 590w,\n/static/dc33986bf61d0e9dded6c3da5c8a9bfe/efc66/result.png 885w,\n/static/dc33986bf61d0e9dded6c3da5c8a9bfe/c83ae/result.png 1180w,\n/static/dc33986bf61d0e9dded6c3da5c8a9bfe/5ab15/result.png 2446w\" sizes=\"(max-width: 590px) 100vw, 590px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\">\n    </span>\n</p>\n```\n```\n<h2>My Experience</h2>\n<p>As it was my first time going abroad, I was super excited to fly to Singapore as soon\nas my fourth year ended. I had a list of action items sorted out for my first week\nafter talking to my lab’s POC, to settle in the new place.</p>\n<p>I was given a competitive stipend, and my lab had arranged for me to live in one of\nthe NTU hostels. NTU had a well-connected bus network, so getting around was no issue.\nI had a Vietnamese roommate, who was a Bachelor’s student in the university, and also a\ngreat tour guide around the campus.</p>\n<p>My first week at ROSE Lab was a mix of introductions and formal work. There was an\nintroductory session organized by Prof Kot and Dr Dennis, who introduced us to the\nROSE Lab team, talked about the various projects undertaken by the lab, and finally\nshowed us to our respective working areas. I had to take a day to visit the Singapore\nMinistry of Manpower to receive my work permit, which was a very smooth process. I\nalso had to register for a temporary student ID with NTU Singapore. </p>\n<p>My “cubicle” was well-equipped and I was free to personalize it for the duration of\nmy stay! Even though there was a PC, I liked to use my own laptop, as most of my work\nwas on the on-prem servers. The research scholars around me were really friendly and\napproachable, and they guided me through most of the setup process. </p>\n<p align=\"center\">\n  <span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; margin: 0 0 30px;\">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 70.94594594594594%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAYAAAAvxDzwAAAACXBIWXMAABYlAAAWJQFJUiTwAAAD9UlEQVQ4yx2TW0yTBxiG/8wL4xDnkFPBCTpBQA5OImiGbuqUeSixomwEkFMLUiynQoFCC4U6rKIGxrEcS6EItoyTAgMjwhQPqFvUuDjDdOx2u9lYdrHk2R8vnrsvT97vy/cKk/NPWFh4xNLSW+Zn77D48DF//7XC66U3TE7P4rDbmbo5xr07t3j15C5vnz3g/uwMre0WrD02bLYBBvrtXLtmp9dqQXj0/A09/QMs/bqM0WAkP7eIyZvTjI7N0NLeT8O3jTTWXabP3MjrBzP89nSOmfFhWs1dWLp66e60Yu22YbX00dXZiTDiGKapuY6piQkuGI2ocwq5bKqjoamLb0z1GCqqsFm7mR4Z5NXDWZaf32didJTrjjHs9lEsln56ewfpsdhE+hBaL5u47ehj5rqVuZEB5kXGetuxd5sZaG+iuUaP4nQMiXGnyZani6SiTE3ibNJX5KQlUpytoDzvHEZtCR1mceXm2os8mBpiYcLxjsfTI9y7MYAyOR7pkWhipUeIjgxlR2AA2z/ZR3hUDOERB4jZHUzK56FkHNxJ6qcBqGSHMDeLwkKViiFLG4Mdzdi7WhixttFVZyIiLBThvTUIwirWOzsRsecQYbsOEhy6m8jw3SiORqE7I6VGEYsx+RgXi3JpaepBSEhMRV+qp1xTgq6wiNqqahrOV7A3IhyXdc64rX8fH4mESvNdOm//wSm5lgAfCbFROyhJ+5raYhVXNUquVJ3naq0Z4XR8MrqyStRqLZqCYk7EyMShLOYsNeiyEog79gWJsTLi5BXo2xYI3nMU51UC26P2k5SVS5V4vysVZdRUmrhorEOQp2VSoavmrEJJxK4IQkJC2ebnT3rsYUZa9VSXF7L/oJS9+6LJSEklP+EoCV+GI+kexzurlKhAf+JOxmLQVmPUmRBylAWUaSuRnZDh7u7Kls0+BG7zw93DCz8fD+rLU7C3GjDkZ9BzqYiVRfH3ui/x0eAU6joDJTE7Ob7/M/TFVeg0FQgqpRpNoRbp8Rh8N/uy0VuCq+t6goMCCQoIwm/rxxRnnuLp2CV+X7Sx8tM1tJ31+Jp7mR+qp7NMjlqehianlJKCUoQ8sRm5qgIiIyPZssUXby83nNc54e+/lUy5giPRh5F4euK9wYlGbQL//Wzn+x9nOCm25/mNZjoMSvLkSnIz8ijMViNo1CVkpGci8fLEZcMHuLg4s2bNalG6FjfXD/EQ027aKGFnoA/j5mKeOcp4OW5ieXGQf158x59P7eQrVGQmnyVXkYNg0BuRSWWEhYbg4bkBp7Wr34k3+27CW+L2ju2BQeTHH2D51lX+/cXBy+Ey7rWfY+VFPz84WjhzKo0sUZiTns3/DGyWh4ecS7EAAAAASUVORK5CYII='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"My work area\" title=\"My work area\" src=\"/static/c035752d94d62669a164a40c8aa70331/fcda8/cubicle.png\" srcset=\"/static/c035752d94d62669a164a40c8aa70331/12f09/cubicle.png 148w,\n/static/c035752d94d62669a164a40c8aa70331/e4a3f/cubicle.png 295w,\n/static/c035752d94d62669a164a40c8aa70331/fcda8/cubicle.png 590w,\n/static/c035752d94d62669a164a40c8aa70331/ad12c/cubicle.png 856w\" sizes=\"(max-width: 590px) 100vw, 590px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\">\n    </span>\n</p>\n<p>There were no fixed working hours within the lab, and I was trusted to work enough\nhours to do my tasks well, at times that suit me and my team. Due to the extremely\nhot and humid weather in Singapore, I tried to spend the day in the air-conditioned\nlab. Sometimes I used to work through the nights, just to experiment. I had weekly\nsync meetings with Lin Shan, and if I had anything else to discuss, we preferred\ndoing it over a walk around the building. I enjoyed Professor Kot’s once-in-a-while\nsync-ups with the interns, where he used to coax us with the benefits of living and\nworking in Singapore.</p>\n<p>I lost a lot of weight during my stay in Singapore. No, it wasn’t just because I was\nsaving money by eating less - my job involved a lot of field-work to collect the dataset!\nAs I am a very passionate runner, I used to sync data collection paths with my running\nroutes. I was also able to do some sightseeing around Singapore with some colleagues\nin the process of collecting data!</p>\n<p align=\"center\">\n  <span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; margin: 0 0 30px;\">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 25%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAFCAYAAABFA8wzAAAACXBIWXMAABYlAAAWJQFJUiTwAAABkElEQVQY0x2O20tTAQCHzx/gk4RvoiRC3hXUvISZoAaCaA/B6MEHmYboi/WwUWgiaYTo3CS8sAaLIhELH4yCwFTGBAVdK3Vuc1OHu5ydXY8eFeJz9nv6Hj4+fsKx140ohkkmkpydnSPLMoqicDOr7S+r9kOsPoX14AVbQZkU4hWT2F2H/Es5RwERh8dDWAoTO48jXF5dEhJDiGGRUChALBolkYj/D5o+zFFTU4F2TI9K84rH3Sp6NT30DagZ0r1gYXGexeVv/FjbYHNnm/nPegSH18Jv50/+uFY4ObHhPNjCuW/DH/Cy8muWZ+3FTI48wDB3j1lzNV29+RjeNzE88ITW2jLe9D/FOKrl7ct+jDM6hB3Hd6z2BfZcFvw+D7t2C+6DXUS/m+kvS9S2pd6Md2E2F/DxUzkz70owmRtQd6roaGng0cM6KkuLuJNXQkXdfQQp5iMoeYjET4lEgiRiEtGohCInGZr+SkazBvXrQQy62+gnypkaK+S5tor0/HpyG7sR0m6RlZ1JdXEOjXdLuQa9qSbw539/VQAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"Data Walks\" title=\"Data Walks\" src=\"/static/018942a896ac6b6776078154555ecb16/fcda8/data_walk.png\" srcset=\"/static/018942a896ac6b6776078154555ecb16/12f09/data_walk.png 148w,\n/static/018942a896ac6b6776078154555ecb16/e4a3f/data_walk.png 295w,\n/static/018942a896ac6b6776078154555ecb16/fcda8/data_walk.png 590w,\n/static/018942a896ac6b6776078154555ecb16/efc66/data_walk.png 885w,\n/static/018942a896ac6b6776078154555ecb16/c83ae/data_walk.png 1180w,\n/static/018942a896ac6b6776078154555ecb16/30faf/data_walk.png 2802w\" sizes=\"(max-width: 590px) 100vw, 590px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\">\n    </span>\n</p>\n<p>The lab had a diverse demographic, and I interacted with people from China, Brazil,\nBelarus, Poland, etc. This was the first time I had mingled with people on a\nglobal scale, and it was a really thrilling and emboldening experience for me. As an\nintrovert, I found it slightly overwhelming to socialize (free-talk) with such a group,\nand I felt this experience helped me gain self-confidence and getting over my fears.</p>\n<p>I was very happy to find 3 more Indian interns, as well as an Indian research scholar\nworking in the lab. We bonded well both in and outside the lab, and frequently went\nout to explore Singapore over the weekends.</p>\n<p align=\"center\">\n  <span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; margin: 0 0 30px;\">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 29.054054054054056%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAGCAYAAADDl76dAAAACXBIWXMAABYlAAAWJQFJUiTwAAABuElEQVQY0w3Q708SAQDG8cOSH3HgwXGHcHfi3aAAAYkm+AunuNAA3RpOS2nZpKjmRvXGAqaj2SZrrVrZn9CL/oJe9KKt/of+nG/34nn77PM8QmV9HSO/ik/REQQH10tVfvz9x+9fP7mzvcvBzg7P2o/Za94lm8oQlXXShsm0lqCRXKY8lSeiRpBDk3jdBsJMwsDtCyBJQaoFCys5y2A45El/xKPuCaPzIV8+feTsbEB1pYx51UPQI1KYkEnJUTK+EPqEQjaoEBBNhO+9Q94+3OT2rRRHtQWWcnGeriU47TQ4fdel/6LD14tzBr2XjLptunKERSnEiRFnTY3S026QCySoB8NoYhDh8/E9PrTqlLIZRHu23z9Je6XIn8s23y7fU63UaDVbHHdesVVeRnd6qdmyXUXD7fKSuhbAFMPUVYt5xRZmtClmpTBhVWfM48flEokZafr7TS76r6ltVIlZBY4eHFKKJ1DdEzxPF6mEoggOp51xhDEnWRsy45PtDz0higsbaLqFcGUcOWISic3Zyn326ps0SjfJm0nu11ZZtKaZk1Te5ObZjsRw2GVLksJWSOMgrBN1ufkPDpTG7xIBj/EAAAAASUVORK5CYII='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"ROSE Indians\" title=\"ROSE Indians\" src=\"/static/13c4a9363d15bafbbb3bb1e12f81222a/fcda8/ROSE_indian.png\" srcset=\"/static/13c4a9363d15bafbbb3bb1e12f81222a/12f09/ROSE_indian.png 148w,\n/static/13c4a9363d15bafbbb3bb1e12f81222a/e4a3f/ROSE_indian.png 295w,\n/static/13c4a9363d15bafbbb3bb1e12f81222a/fcda8/ROSE_indian.png 590w,\n/static/13c4a9363d15bafbbb3bb1e12f81222a/efc66/ROSE_indian.png 885w,\n/static/13c4a9363d15bafbbb3bb1e12f81222a/c83ae/ROSE_indian.png 1180w,\n/static/13c4a9363d15bafbbb3bb1e12f81222a/fc99f/ROSE_indian.png 2702w\" sizes=\"(max-width: 590px) 100vw, 590px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\">\n    </span>\n</p>\n<p>The most unexpected experience during my internship was the 3-day summer school\norganized by ROSE Lab on research topics in\n<a href=\"https://rose1.ntu.edu.sg/event/VISVA2018/\">Visual Image Search and Video Analytics (VISVA)</a>.\nIt was an opportunity for me to get to know the state of the art technologies from\ndirect interaction and discussions with pioneers in academia and industry, in a\nfriendly and constructive environment. Research topics included Biometrics, Image\nForensics, Autonomous Vehicles, Object retrieval, as well as Person Re-ID in\nSurveillance, which gave me a lot of insight while working on my own project!</p>\n<p align=\"center\">\n  <a href=\"https://www.youtube.com/watch?v=w2_HG4WV-v0\">\n    <span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; margin: 0 0 30px;\">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 49.32432432432432%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAABYlAAAWJQFJUiTwAAAC3ElEQVQozx2R229TBRzHz4OyuEvXbaWX09PR29pzetqul9XW2gvb6eY2WVdL1k12TwljGYEtImZWQiQEBSGEhEuYEYGQDDRySeABE8AZLxAe4Al9MD4YHmRxxv/g4wkPv/x+L99vvr/vRxiK7kANJhBFN3V1DdTXG6h/o+HVNDUaEG1W/D4Pri0OJNGG3WpDtNjoUCK8XZpldOkQxSOX6CqM0tQqI7S1SLqwFWOzicZGE6oSRvbJNDYYMLdZcErtdKoqAZ8fyWbHanFgM4kU3pvhxFdfc/vyZyx+fpr44BwW91aEZoNuVG/ktdeb2FRnxGx2ssXh1YUizUbrK8NMOqOnDGC1tmPQA0QCQYbL07y/7yQjs8soA4fx93yIKzKKIJns2FutGPR3HW1m2m0eHDYnKTVIUvYyEO9g+p0udmoh5gfeRIsE+GRXhZO1A3x67h57Dn5DefI4uaEa2o6jCB5Jxif5qH10nPNnrjM5vkC5v8zC9G76U2mun5rjyZ1jPF9Z5moliRZPMZDvZqw8yn/rf/HvPy/Z2Njg2s0fSQ8vI+QTWb07B7t3HuRA7SzVqSXmR6qUBico5XLsr2S5+MVeLsxv5+OhLH3pPC0tHqKxPA8frPH48TNevPibBz88QU1XEbSMhnuzk5hLISEHkUUnuzxeOowmnBY7PtGOpsgsKB1s02vo0aH5vTESiT4e/fIr6y/X+f35H9y49ZBoYRGht7Adu6kd1R0mrUb020HK5SHTKRPSd4/qYiotUwkFCZu8hKQEkieFEi9RGqtx6Nh3XF79idVv1/DGxxEmymPEw0miahcxNUqmK0UsoCK73NjNOlGnSNDtRnHKZGN9dOaqKHKSUKLI4aNXOLPyPUdO3GVmaRV/TKecjMbo7+5mqFdjtqixOP4u+2eK7JsYZG6kwPi2LMO9W3krkUEN58nlinxQnWKyMsmX5y/y6OenPHv6J/fXfmNsYYX/AeT8dqlIPmtHAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"VISVA 2018\" title=\"VISVA 2018\" src=\"/static/9dd689202dba44bc9f4c103484517921/fcda8/visva_all.png\" srcset=\"/static/9dd689202dba44bc9f4c103484517921/12f09/visva_all.png 148w,\n/static/9dd689202dba44bc9f4c103484517921/e4a3f/visva_all.png 295w,\n/static/9dd689202dba44bc9f4c103484517921/fcda8/visva_all.png 590w,\n/static/9dd689202dba44bc9f4c103484517921/efc66/visva_all.png 885w,\n/static/9dd689202dba44bc9f4c103484517921/c83ae/visva_all.png 1180w,\n/static/9dd689202dba44bc9f4c103484517921/5cae2/visva_all.png 2332w\" sizes=\"(max-width: 590px) 100vw, 590px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\">\n    </span>\n  </a>\n</p>\n<p>As I was facing my final year after the end of the internship, I was hungry for\ncareer insight. Luckily, I was around research scholars who had and were taking such\ndecisions themselves. After multiple such interactions, as well as a career counseling\nwith Dr Dennis Sng, I found that I understood the merits to both academic and corporate life,\nbut also that I wanted to pursue higher studies only to get better jobs. However, one\nthing was different from my past internships, and that was my reinforced enthusiasm\nfor the field I was working in - Machine Learning and Artificial Intelligence!</p>","frontmatter":{"title":"Summer Internship at NTU Singapore","date":"September 01, 2020","description":"Research intern experience at NTU ROSE Lab"}}},"pageContext":{"slug":"/blog/ntu-intern/","previous":{"fields":{"slug":"/blog/isro-intern/"},"frontmatter":{"title":"The ISRO Internship"}},"next":null}},"staticQueryHashes":["63159454"]}