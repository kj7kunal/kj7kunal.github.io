<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[Kunal Jain's RSS Feed]]></title><description><![CDATA[Machine Learning Engineer currently in Tokyo, Japan]]></description><link>https://kj7kunal.github.io</link><generator>GatsbyJS</generator><lastBuildDate>Mon, 26 Aug 2024 09:35:05 GMT</lastBuildDate><item><title><![CDATA[ML Engineer - 5 Years of Experience - Reflection]]></title><description><![CDATA[Five years ago, I embarked on my journey at Visional Inc.,
as a fresh graduate eager to contribute to the AI and Machine Learning landscape…]]></description><link>https://kj7kunal.github.io/blog/2024-visional/</link><guid isPermaLink="false">https://kj7kunal.github.io/blog/2024-visional/</guid><pubDate>Tue, 01 Oct 2024 12:00:00 GMT</pubDate><content:encoded>&lt;p&gt;Five years ago, I embarked on my journey at &lt;a href=&quot;https://www.visional.inc/ja/index.html&quot;&gt;Visional Inc.&lt;/a&gt;,
as a fresh graduate eager to contribute to the AI and Machine Learning landscape in Tokyo, Japan.
Three years ago, I penned my &lt;a href=&quot;/blog/2021-visional&quot;&gt;2-Year milestone recap&lt;/a&gt; that I had documented
earlier and I believe that I have grown exponentially since then. At the time, I was excited about
the projects I had worked on, the growth I had experienced, and the challenges I had overcome.&lt;/p&gt;
&lt;p&gt;Now, as I stand at the cusp of my fifth anniversary, it’s a good time to reflect on the evolution
of my role, my achievements, and the exciting challenges that have shaped me into the engineer I am today.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Contents&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#revisiting&quot;&gt;Revisiting My Foundations&lt;/a&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&quot;#manager&quot;&gt;A New Manager: Better communication&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#okrs&quot;&gt;OKRs: Linking Personal and Professional Growth&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#learning&quot;&gt;Continuous “Deep” Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#tools&quot;&gt;Tools for Enhanced Productivity&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#projects&quot;&gt;My Projects&lt;/a&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&quot;#p1&quot;&gt;Resume Generation (Multiple Versions)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#p2&quot;&gt;AI MLOps: Feature Store PoC&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#p3&quot;&gt;Resume LLM Fine-tuning (R&amp;#x26;D Project)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#p4&quot;&gt;Financial Document OCR System (Improvements)&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#company&quot;&gt;About Visional&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#conclusion&quot;&gt;The Road Ahead&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id=&quot;revisiting&quot;&gt;Revisiting My Foundations&lt;/h2&gt;
&lt;p&gt;When I was writing my &lt;a href=&quot;/blog/2021-visional&quot;&gt;2-Year recap blog&lt;/a&gt;, I was still finding my
footing in the world of Machine Learning, juggling between different roles as a
Data Scientist, ML Engineer, and Research Scientist. I was focused on delivering value
through the projects I worked on, such as the Financial Document OCR System and research
projects like Employee Slack Analytics or Japanese Handwritten OCR research. These
projects were critical in building my technical foundation, allowing me to dive deep
into the practical aspects of ML workflows.&lt;/p&gt;
&lt;p&gt;As I transitioned into the next phase of my career, I began to realize the importance
of not just being an effective contributor but also taking on leadership roles within
projects, especially in shaping their direction and execution. This leads to what I
believe were &lt;strong&gt;key developments in the last three years&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&quot;manager&quot;&gt;1. A New Manager: Better communication&lt;/h3&gt;
&lt;p&gt;One of the most significant changes in my professional life was hands-down my
Vietnamese team lead becoming my manager. This transition had a profound impact on
how I approached my work. Being able to directly converse in English with my manager,
not only about technical issues but also management-related concerns made me more
comfortable in my place within the team. Communication within the team became more
seamless, and I found myself more involved in the early roadmapping phases of projects.&lt;/p&gt;
&lt;p&gt;A notable example was the &lt;a href=&quot;#p1&quot;&gt;Resume Generation project&lt;/a&gt;, a super
impactful project where I was able to contribute right from the brainstorming stage.
This involvement not only allowed me to secure projects more easily but also gave me
the confidence to push for innovative solutions to both technical and management
related issues. I am truly grateful for having him as my manager for the past 3 years.&lt;/p&gt;
&lt;h3 id=&quot;okrs&quot;&gt;2. OKRs: Linking Personal and Professional Growth&lt;/h3&gt;
&lt;p&gt;Another shift in my professional growth came from my adoption of personal OKRs
(Objectives and Key Results), a practice inspired by Google’s approach, which
I learned from reading the book &lt;a href=&quot;https://www.google.co.jp/books/edition/_/VEtTDwAAQBAJ&quot;&gt;Measure What Matters by John Doerr&lt;/a&gt;. I believe that I had initially struggled with the company’s
MBO (Management by Objectives) system, not knowing how they would link back to
the broader company objectives OR my own career goals. OKRs are designed to
be transparent and cascaded through organization levels.&lt;/p&gt;
&lt;p&gt;Adopting such a practice for my personal goals taught me how I could link my work with
my career goals, focusing on &lt;em&gt;outcomes rather than output&lt;/em&gt;. This personal system kept
me on track, ensuring that my work was always purposeful and aligned with both my
professional growth and the company’s vision.&lt;/p&gt;
&lt;h3 id=&quot;learning&quot;&gt;3. Continuous &quot;Deep&quot; Learning&lt;/h3&gt;
&lt;p&gt;Reading &lt;a href=&quot;https://www.oreilly.com/library/view/designing-machine-learning/9781098107956/&quot;&gt;Designing Machine Learning Systems by Chip Huyen&lt;/a&gt; was a game-changer for me. The book
provided me with a comprehensive understanding of ML systems, evolving my notion of
machine learning beyond just developing an &lt;em&gt;End-to-End Pipeline&lt;/em&gt;, to a holistic
&lt;em&gt;Machine Learning Life Cycle&lt;/em&gt; - from data collection to deployment. Learning best practices
in designing scalable and reliable ML systems is crucial for moving from experiments/internal
projects to production-grade applications.&lt;/p&gt;
&lt;p&gt;The book particularly emphasized the significance of MLOps — a field that was coincidentally
gaining momentum within our team thanks to a new colleague who moved to our company from AWS
as an MLOps Architect. I have learned a lot from our conversations on Teal Organizations,
Team Topologies, frameworks for Logical Thinking, and in general MLOps. Armed with this new
knowledge, we proposed a &lt;a href=&quot;#p2&quot;&gt;re-architecture of our AI MLOps&lt;/a&gt;, focusing on scaling
our infrastructure and improving the reliability of our models in production. This initiative
was a key milestone in my journey, marking the transition from being a contributor to a
thought leader in my team.&lt;/p&gt;
&lt;h3 id=&quot;tools&quot;&gt;4. Tools for Enhanced Productivity&lt;/h3&gt;
&lt;p&gt;When a person with ADHD who has always been an academic achiever finds themselves unable to change
jobs, be it due to fear of change, high standards or even emotional attachment, they begin to devise
personalized workflows to keep up with the increasing complexity of projects.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Google Meets’ closed captioning&lt;/strong&gt; and the &lt;strong&gt;Google Translate browser extension&lt;/strong&gt; allowed
me to better understand in-person and virtual meetings, ensuring that I remained an active
participant in all discussions, regardless of language barriers. The move from Confluence to
&lt;strong&gt;Scrapbox&lt;/strong&gt; as a documentation tool in our company, which is coupled with &lt;strong&gt;DeepL&lt;/strong&gt; for
translations, helped me bridge the language gap and contribute more effectively in Japanese.&lt;/p&gt;
&lt;p&gt;Outside the language barrier, &lt;strong&gt;Miro&lt;/strong&gt; became indispensable for system architecture designs
and big-picture mind maps, helping me and my team visualize and communicate complex ideas
effortlessly. Tools like &lt;strong&gt;ChatGPT/Copilot&lt;/strong&gt; for code improvement, debugging, and
documentation significantly boosted my productivity.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id=&quot;projects&quot;&gt;My Projects&lt;/h2&gt;
The past three years have been transformative, owing to key projects which not only 
tested my technical skills but also pushed me to grow as a leader and innovator in 
the ML space. I found myself going the extra mile, partly due to the demands of the 
project and partly due to the passion and satisfaction I discovered while working on them.
&lt;p&gt; &lt;/p&gt;
&lt;h3 id=&quot;p1&quot;&gt;1. Resume Generation (Multiple Versions)&lt;/h3&gt;
The Resume Generation project has been an ongoing effort, with several key milestones 
achieved over the last three years - even prior to the advent of ChatGPT. I can 
summarize the changes brought by each version, and how they were built upon the previous version.
&lt;p align=&quot;center&quot;&gt;
  &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; margin: 0 0 30px;&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/134d61170aa28b8d1d65eb664e11bc2c/9888a/resume_generation.jpg&quot; style=&quot;display: block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom: 71.62162162162163%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAOABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAAIBBf/EABQBAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhADEAAAAe5SC2D/xAAXEAADAQAAAAAAAAAAAAAAAAAAEBEx/9oACAEBAAEFAlSGL//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8BP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8BP//EABQQAQAAAAAAAAAAAAAAAAAAACD/2gAIAQEABj8CX//EABgQAAMBAQAAAAAAAAAAAAAAAAABESFR/9oACAEBAAE/IW4Urg0ZMq1PNEf/2gAMAwEAAgADAAAAEKAP/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPxA//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPxA//8QAHBABAQEAAgMBAAAAAAAAAAAAAREAITFBUWGB/9oACAEBAAE/EATFh23i4YNLDvCF4fuInX0yyjbHlfeCE43/2Q==&apos;); background-size: cover; display: block;&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;Resume Generation Project&quot; title=&quot;&quot; src=&quot;/static/134d61170aa28b8d1d65eb664e11bc2c/1c72d/resume_generation.jpg&quot; srcset=&quot;/static/134d61170aa28b8d1d65eb664e11bc2c/a80bd/resume_generation.jpg 148w,
/static/134d61170aa28b8d1d65eb664e11bc2c/1c91a/resume_generation.jpg 295w,
/static/134d61170aa28b8d1d65eb664e11bc2c/1c72d/resume_generation.jpg 590w,
/static/134d61170aa28b8d1d65eb664e11bc2c/a8a14/resume_generation.jpg 885w,
/static/134d61170aa28b8d1d65eb664e11bc2c/fbd2c/resume_generation.jpg 1180w,
/static/134d61170aa28b8d1d65eb664e11bc2c/9888a/resume_generation.jpg 1198w&quot; sizes=&quot;(max-width: 590px) 100vw, 590px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;&gt;
  &lt;/a&gt;
    &lt;/span&gt;
&lt;/p&gt;
&lt;h4&gt;&lt;em&gt;Version 0: Keyword Suggestion&lt;/em&gt;&lt;/h4&gt;
&lt;p&gt;The first and foremost AI assistance our platform could provide candidates in the
registration process was suggesting them relevant keywords to enhance their profiles.
This version used algorithms like tfidf and k-means clustering to recommend keywords
which candidates could simply incorporate into their project descriptions, thereby
increasing their scout rates with recruiters. I joined the resume generation
project with this version already in production.&lt;/p&gt;
&lt;h4&gt;&lt;em&gt;Version 1: Keyword Recommendation -&gt; Sentence Recommendation&lt;/em&gt;&lt;/h4&gt;
&lt;p&gt;Building on the basics, we moved towards a more sophisticated approach: recommending
sentences to candidates while they wrote their project descriptions. This version
introduced a two-step pipeline, each step trying to reduce the distance between inputs
and outputs in an embedding space. I focussed mainly on the first component.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1. Keyword Recommendation&lt;/strong&gt; [Resume text and categorical features] -&gt; [Relevant keywords]&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Resume features were vectorized and concatenated to form a resume vector. Topic modeling using LDA was used to create an embedding model for resume-keyword similarity&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;2. Sentence Recommendation&lt;/strong&gt; [User selected keywords] -&gt; [Relevant next sentences]&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A BERT Semantic Textual Similarity (STS) model was used to find possible next sentences similar to keywords selected in the previous step&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;em&gt;Version 2: Keyword Recommendation -&gt; Text Generation&lt;/em&gt;&lt;/h4&gt;
&lt;p&gt;ChatGPT was released to the world in the beginning of Dec 2022. Within days, our senior
engineers had built a prototype with the &lt;code class=&quot;language-text&quot;&gt;text-davinci-003&lt;/code&gt; API, which led to rapid
developments post-New year. A large project team was formed, including frontend,
backend, ML, QA, legal, etc., and I found myself sitting beside my manager on the
largest Japanese round table in my life, and between listening hard to navigate
fast-paced discussions and vaguely looking at my manager, I was trying to fit
together context like pieces of a jigsaw. I made through it - somehow, and was set
to release the first version of the new ML system within 2 months, the backbone
to what became one of the most intense, yet rewarding, phases of my career so far.&lt;/p&gt;
&lt;p&gt;Those two months of development felt like a throwback to my university days,
filled with late hours and deep focus. But that’s a story for another time—let’s
stick to the technical milestones. Buy me a coffee to hear more, I guess!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Keyword Recommendation: Improvements&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The embedding model for resume-keyword similarity was iteratively improved as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Resume features were transformed into BERT input strings. Semantic Textual Similarity was used to train resume-keyword similarities&lt;/li&gt;
&lt;li&gt;Reranking was introduced according to keyword selection rate and search popularity&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Text Generation: ML System Design&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;We transitioned to a microservices-based architecture, enabling independent handling of different components in the resume generation pipeline. Key features included:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Message-forwarding and queuing via AWS SQS and DynamoDB&lt;/li&gt;
&lt;li&gt;Multithreading capabilities, processing up to 60 Generation RPM, within OpenAI’s rate limits&lt;/li&gt;
&lt;li&gt;A robust internal mapping system to manage OpenAI’s success/error responses&lt;/li&gt;
&lt;li&gt;Moderation layers to act as guardrails to reinforce the &lt;code class=&quot;language-text&quot;&gt;harmless&lt;/code&gt; in the &lt;a href=&quot;https://arxiv.org/abs/2112.00861&quot;&gt;3Hs for LLMs&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; margin: 0 0 30px;&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/b456dea9e21ae83b789ed79b062b55dc/344cf/datadash.png&quot; style=&quot;display: block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom: 57.432432432432435%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAAAsTAAALEwEAmpwYAAACS0lEQVR42l2SS2vUYBSG87tcuHMj/gRx50bwB/gXFESw7lQUrVQoUrvwAkUX1oVYFNvaltrpxc5Mp2mTL5dJvtv58vomGVAMnMmQc87znlukq8o3IUBEcJob7CcOp6UgngomZYCqLCbFGIPsAPvFIcbVCUo3hXEO3nsE5vJpGv548XVkq9ofD8dQShHiMYg1jlKDY2X5duGsqGV5b1mebj6T+a3nMr+9IG8G7ySplKCBECpkevRPEdXaiKWa46ey8qiEik1vjXgxxjiW74z2LtNwTHJkOJHgKl3TB/dN7bp7ey/846PXOgr0SMs+W8LSpyEerKwDKkb7Tfp2kFDs2m7ApZ8NFlVfSmCTJ8OascD9jZe48OEqrny+iaj1lZzFk63baEyK9S9z2Bm86pKs9J3cGQG3DgNOtMfFDc6q6aE5S87HFmvDbTzaXsTCr7cEBorRubqyirnvMQZfH0JXa12C72vHxxy4vANc3wduHMymxQHGBG7+LmB930nDqiNjHZFUsxqL4wJnI90FhzCzbn8N3p873D3SKLrkpttrWhisHxQYJRWMd9DW+khlOcq8gK458XwfteFJGAtdlhDn+kq5qBDa9qUTaP6zELomGSdpdJ4qlDwZZx0mk01k2YggQZ2m8Mb0C+ByNEWmWsPO7s/xbW37X9q190BPYKIylFkGx4QkboExPG+oztRfII++jSvYiSO0hTnGVFUF24pozYsUGK1nQJplcjz5gSQhkMo1Rf6tMOVopnkOTWsFuiugP2d3hcoaz4JcrdM/OSpAUC9bUVUAAAAASUVORK5CYII=&apos;); background-size: cover; display: block;&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;First Datadog Dashboard&quot; title=&quot;&quot; src=&quot;/static/b456dea9e21ae83b789ed79b062b55dc/fcda8/datadash.png&quot; srcset=&quot;/static/b456dea9e21ae83b789ed79b062b55dc/12f09/datadash.png 148w,
/static/b456dea9e21ae83b789ed79b062b55dc/e4a3f/datadash.png 295w,
/static/b456dea9e21ae83b789ed79b062b55dc/fcda8/datadash.png 590w,
/static/b456dea9e21ae83b789ed79b062b55dc/efc66/datadash.png 885w,
/static/b456dea9e21ae83b789ed79b062b55dc/c83ae/datadash.png 1180w,
/static/b456dea9e21ae83b789ed79b062b55dc/344cf/datadash.png 2643w&quot; sizes=&quot;(max-width: 590px) 100vw, 590px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;&gt;
  &lt;/a&gt;
    &lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;The system required &lt;strong&gt;high cross-team collaboration&lt;/strong&gt; with product backend, frontend
and QA teams. I took the initiative to create the &lt;strong&gt;first Datadog dashboard for the AI ​​team&lt;/strong&gt;,
making it easy to share weekly insights, respond to incidents, improving the flow of communication on the AI side.&lt;/p&gt;
&lt;p id=&quot;award&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The business impact? A &lt;a href=&quot;https://www.bizreach.co.jp/pressroom/pressrelease/2023/070601.html&quot;&gt;40% improvement in our scout-reception KPIs&lt;/a&gt;!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Oh, and the team received the &lt;strong&gt;Best Project Team Award&lt;/strong&gt; at the BizReach Awards 2023.
Look at the size of the team! The memories of that project are still fresh, and the
award ceremony—a reminder of how collective effort and innovation can lead to extraordinary results.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; margin: 0 0 30px;&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/ca0be235ba75eca8c329a97f2ca127cf/18d85/award_team.png&quot; style=&quot;display: block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom: 60.13513513513513%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAYAAABiDJ37AAAACXBIWXMAABYlAAAWJQFJUiTwAAADGElEQVR42h2TyW4bRxCG5+BApiRKpEVSXMJ1SIrrcBlxhssM980UJSqyuFlSKEuCYSswrMAOEDuIjCALkKuDnHzKLU+Qcx4hb/Sl4QYKXUB3/dX1VbXkCcuEkll80STZaouM0cOXreLKt3FkDJyxPC45izOq4IpmCeZqPJJzbDj9bLt8hJUSSr2HO5zAG9lDsru97AYjwqJE0ip7pTqhgolHqbGb0ngUzuCJKfiEucJJnCLQHohj98l4IklixTKxgo5XTuAJx5A2bHa2nG5sbj+uUIJgWheCDeRiG39KF68t446m8cZVvAmN7UAOWyAt7sbxJ/LIhSpyVvssvu30IFk2NlnfsuH0BvBHU7jlPKn2Kfu1KUbvOUl9hH9/gDV1xJfGBQ7tHEvua3ayfXyZKiFRSSip4vAF2RA60kOLhQcPLVTKOg2zjl5rcvjkGKN/SevghnJjgjZakVQbpI0xxvCE8njF48v31HsTuoMhSi4v4susW7eQvlhbw7JpJZVIERdWb7U5Gg1Qyw0WsxnN7pBas0urN+JsOWc2W1LrHIlkC1FFl3i6QKXepT+ZsbZpRxILfb+IWdEIRaKcz2fE95I8ORyQLxTIKApaURVJHnP/+oqa2UAvGxjlCgHBcS+V42DYx6i3eLC+jWS1O2iMnpLOFJETCiWjT6PToz865uJsxc9v3zAf1JgfHvDHh++5Xp7yyw8/8vG339F0ncXJIbPpnNPTBfYdJ5JHjIHen9MR/PbzRVrVCjm1TKGg8ev9T7y/+46mWSNf1Pnr45/89+8/fHj3jovpU1H+jL8/fWJ1fsZieYbL60Oy7ewSE/Onq0XikTCpqIzFaiMajtMU7PKqRqduEg0FkX3ezxiUnIpVNKBUyHN/+4JKIUNElnG4dpECoQi+QFDAVciVdNSqidnpC/B9zO6A4eSEqvBHk6/oj8dogqHR7lCtN8kJtsGITEQ0M6uKHyOSSfFEhmyxhG40qDZ79A4mTKZLFqsrlpc3XNy84PbuLdffvObZy1c8f3XH7bdvxNkVx4tzsV8zPplSa/eomE3+B2VEjH7mitVeAAAAAElFTkSuQmCC&apos;); background-size: cover; display: block;&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;Bizreach Awards&quot; title=&quot;&quot; src=&quot;/static/ca0be235ba75eca8c329a97f2ca127cf/fcda8/award_team.png&quot; srcset=&quot;/static/ca0be235ba75eca8c329a97f2ca127cf/12f09/award_team.png 148w,
/static/ca0be235ba75eca8c329a97f2ca127cf/e4a3f/award_team.png 295w,
/static/ca0be235ba75eca8c329a97f2ca127cf/fcda8/award_team.png 590w,
/static/ca0be235ba75eca8c329a97f2ca127cf/efc66/award_team.png 885w,
/static/ca0be235ba75eca8c329a97f2ca127cf/c83ae/award_team.png 1180w,
/static/ca0be235ba75eca8c329a97f2ca127cf/18d85/award_team.png 2382w&quot; sizes=&quot;(max-width: 590px) 100vw, 590px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;&gt;
  &lt;/a&gt;
    &lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;Almost a year has passed since then, and with the system architecture in place,
the focus shifted to optimizing performance:&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Explored various ML models to improve keyword recommendation metrics for better user experience&lt;/li&gt;
&lt;li&gt;Improved prompt templates across different job categories, guided by &lt;a href=&quot;https://arxiv.org/html/2312.16171v1&quot;&gt;prompt engineering best practices&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Improved incident response, reducing time-to-detect (TTD) and time-to-mitigate (TTM) by adding more alerts and implementing redundancy measures, such as provider/region switching&lt;/li&gt;
&lt;li&gt;Redesigned the architecture to support multi-API integration, ensuring backward compatibility while introducing new features and enhanced prompts.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;The Resume Generation project has been one of the most impactful projects
I’ve worked on, significantly contributing to the company’s core offerings and
demonstrating my ability to lead and deliver in complex, multi-phase projects.&lt;/p&gt;
&lt;h3 id=&quot;p2&quot;&gt;2. AI MLOps: Feature Store PoC&lt;/h3&gt;
&lt;p&gt;As mentioned above I had started reading &lt;a href=&quot;https://www.oreilly.com/library/view/designing-machine-learning/9781098107956/&quot;&gt;Designing Machine Learning Systems by Chip Huyen&lt;/a&gt;
in the start of 2024. Learning about best practices in a holistic
&lt;em&gt;Machine Learning Life Cycle&lt;/em&gt; inspired me to connect more with our MLOps team.
In our discussions we identified a &lt;a href=&quot;https://research.google/pubs/machine-learning-the-high-interest-credit-card-of-technical-debt/&quot;&gt;technical debt&lt;/a&gt;
in the form of overlapping data pipelines within a significant portion of
our around 30 batch pipelines in production.&lt;/p&gt;
&lt;p&gt;As a young ML Engineer, I remember spending 2 weeks building a search ML pipeline
from scratch that I later found out could have just been modified from another
team’s project that used the same data. I can say, I have always had a passion
for automation and standardization.  This need to standardize and centralize features
across multiple pipelines and projects resulted a proposal for the re-architecture
of our AI MLOps systems, centered around developing AI Group’s first
Feature Store as a proof-of-concept (PoC).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Although it is still early in my MLOps journey, I strongly believe that Google
has set the standard in this space, and their documentation is a gold mine of
learning. They summarize &lt;a href=&quot;https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning&quot;&gt;a Golden Path to MLOps&lt;/a&gt;,
which became my go-to resource for understanding how to incrementally build an
organization’s MLOps capabilities across CI/CD/CT pipelines. It shows how
higher levels of automation could &lt;strong&gt;accelerate&lt;/strong&gt; the ML lifecycle. Higher the
level, higher the &lt;strong&gt;velocity&lt;/strong&gt; of deploying new models given new data/implementations.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;It is worth noting that each organization takes a different path, but as
mentioned in &lt;a href=&quot;https://sistel.medium.com/the-mlops-engineer-vertical-first-horizontal-second-306fa7b7a80b&quot;&gt;Vertical First Horizontal Second&lt;/a&gt;,
achieving early vertical breakthroughs is critical. This approach involves
“agile” deployment of vertical prototypes early in the development process
to detect risks, iterate quickly, and align closer with production settings
and stakeholders. However, at some point we do need to increase an
ML team’s breadth and move to horizontal splitting.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;While our journey towards MLOps maturity shares commonalities with
Google’s levels, it is not a direct testament to Google’s MLOps level
definitions.  Instead, it reflects &lt;strong&gt;our unique path&lt;/strong&gt;, tailored to our
specific challenges, infrastructure, and business needs. By understanding where
we stand and the steps we’ve taken, we can better appreciate our progress
and identify areas for further improvement. Here was my take on our starting point and destination:&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; margin: 0 0 30px;&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/34fd1d28ecd4cd6564a52693fa35eddf/a1e6a/mlops_maturity_levels.png&quot; style=&quot;display: block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom: 31.08108108108108%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAGCAYAAADDl76dAAAACXBIWXMAAAsTAAALEwEAmpwYAAABOklEQVR42i2QyXIbMQxE5/8/zcccU3a8xJE0moUcbuA2fIFUPnSRBBvoRk/Grpjjjot3QlLIii+JQ7KeZZjacaW5VOmla93PWHfB+Ssu3LBJcCKsUdhSYUr1wKedIyz4uDDfvtm9U2J6Dvy8OvaQnaP2S1owyRCLx4tjXTcVK4SSWWxiNokptE4eAzlPHvdQT8qA1Bo2hjGHis3Nvdnv/mt7Y2uiQh3fziei9poQmI/ILpVpV+3ZHlhdzWRtrk0/Ckep3Dc/Dn0ndKvSesyFz9XxsSfe98jHJvxVVzYWbibyiGeKffBn8byvD0J6kl7vnlVDk36Of1vESnUFejw7VoWuyfOy/eZV89yyoH/E3DQGdShqOZ6D8IP4g6T1DCOrG9GBogOzRmF05a0GQm98ycIlmwePEIQUMv8BxNrN9idsoEEAAAAASUVORK5CYII=&apos;); background-size: cover; display: block;&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;MLOps Maturity Levels&quot; title=&quot;&quot; src=&quot;/static/34fd1d28ecd4cd6564a52693fa35eddf/fcda8/mlops_maturity_levels.png&quot; srcset=&quot;/static/34fd1d28ecd4cd6564a52693fa35eddf/12f09/mlops_maturity_levels.png 148w,
/static/34fd1d28ecd4cd6564a52693fa35eddf/e4a3f/mlops_maturity_levels.png 295w,
/static/34fd1d28ecd4cd6564a52693fa35eddf/fcda8/mlops_maturity_levels.png 590w,
/static/34fd1d28ecd4cd6564a52693fa35eddf/efc66/mlops_maturity_levels.png 885w,
/static/34fd1d28ecd4cd6564a52693fa35eddf/c83ae/mlops_maturity_levels.png 1180w,
/static/34fd1d28ecd4cd6564a52693fa35eddf/a1e6a/mlops_maturity_levels.png 3581w&quot; sizes=&quot;(max-width: 590px) 100vw, 590px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;&gt;
  &lt;/a&gt;
    &lt;/span&gt;
&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Once our MLOps vision reached a practical stage, we attended a 2-day
&lt;strong&gt;Google MLOps Mini TAP (Tech Acceleration Program)&lt;/strong&gt; at the Google Tokyo Office to
refine our MLOps direction using Google Cloud’s solutions. We decided to
implement the first PoC using &lt;a href=&quot;https://cloud.google.com/vertex-ai/docs/featurestore&quot;&gt;Vertex AI&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;For the Feature Store PoC, I led the design and implementation of the feature store schema,
focusing on decoupling the data and model pipelines for an existing low-impact score
prediction project. This project had feature overlaps with others, making it the perfect
candidate for standardization. The process required a lot of self-study, but it
ultimately equipped me with a solid understanding of the core components:&lt;/p&gt;
&lt;blockquote&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Data Pipeline&lt;/strong&gt;: Apache Beam, Google Dataflow (flex-templates)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Model Monitoring&lt;/strong&gt;: Vertex AI Model Monitoring&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Online Feature Serving&lt;/strong&gt;: Vertex AI Feature Store&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Parallely, my colleague focussed on &lt;strong&gt;Model Training&lt;/strong&gt;, &lt;strong&gt;Model Evaluation&lt;/strong&gt; and
&lt;strong&gt;Model registration&lt;/strong&gt;. FWe later integrated everything into a coherent
workflow using &lt;strong&gt;Kubeflow Pipelines (kfp)&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Looking back, this PoC was a major step forward for us. It wasn’t just about
implementing new technology—it was about laying the groundwork for future
scalability and improving the overall velocity of our ML projects. There’s a lot to do next term!&lt;/p&gt;
&lt;h3 id=&quot;p3&quot;&gt;3. Resume LLM Fine-tuning (R&amp;D Project)&lt;/h3&gt;
&lt;p&gt;In an effort to stay at the cutting edge of the recent boom in AI and LLMs, I took
on the challenge for an R&amp;#x26;D project focused on fine-tuning a Large Language Model (LLM)
for resume generation, building upon my work on the Resume Generation project.&lt;/p&gt;
&lt;p&gt;This project was particularly challenging as it involved working with vast amounts
of unstructured text data and adapting a pre-trained LLM to our specific domain. The
main aim was to explore the feasibility of developing an in-house LLM to reduce the
dependence on external models like ChatGPT. A secondary outcome was to establish a
baseline PoC for LLM fine-tuning within our AI Group.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; margin: 0 0 30px;&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/94ce6d51443792ef9cfa255d0c17ba65/3fe40/llm_miro.png&quot; style=&quot;display: block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom: 15.54054054054054%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAADCAYAAACTWi8uAAAACXBIWXMAAAsTAAALEwEAmpwYAAAA+klEQVR42mPo7tbjPnzXvuHgHYf0/Tftcw/ccKjXc3XlZgACDbc7xrr+98p0A27lM/D/F7xxX1jjzDWtuqMXNHSOXlQPOXxeo8DCQoZz1zk7r1OPHXvbpxslMNQXagltP2E9aeshq6p9F2yKjt6xrwGaxfJ942yVHVMP75jZfv7/zPbTfy7MPFq78mJZ/Lyrlf+nnSmJmnq6ZPPUUyWvSjd08u6/YuNx4qHjxL55xvEMj8vUGz9Umx55X6F//F2psh8DFByeNt/ixeqVbf93Lin8tWN59ac1qypzVmx38Vy2Y1HA5oNm/it25/qt3t3jsXgxH0yPuLi4IgB0HnH3UmCrEQAAAABJRU5ErkJggg==&apos;); background-size: cover; display: block;&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;Resume LLM Fine-tuning&quot; title=&quot;&quot; src=&quot;/static/94ce6d51443792ef9cfa255d0c17ba65/fcda8/llm_miro.png&quot; srcset=&quot;/static/94ce6d51443792ef9cfa255d0c17ba65/12f09/llm_miro.png 148w,
/static/94ce6d51443792ef9cfa255d0c17ba65/e4a3f/llm_miro.png 295w,
/static/94ce6d51443792ef9cfa255d0c17ba65/fcda8/llm_miro.png 590w,
/static/94ce6d51443792ef9cfa255d0c17ba65/efc66/llm_miro.png 885w,
/static/94ce6d51443792ef9cfa255d0c17ba65/c83ae/llm_miro.png 1180w,
/static/94ce6d51443792ef9cfa255d0c17ba65/3fe40/llm_miro.png 2517w&quot; sizes=&quot;(max-width: 590px) 100vw, 590px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;&gt;
  &lt;/a&gt;
    &lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;As the sole contributor to this R&amp;#x26;D task, I approached the project with a flexible proposal and roadmap, which led to the following structured approach:&lt;/p&gt;
&lt;blockquote&gt;
&lt;ol&gt;
&lt;li&gt;Self-learning and Methodology review
&lt;ul&gt;
&lt;li&gt;Conducted an in-depth review of &lt;a href=&quot;https://qiita.com/mshinoda88/items/fc562ec6a84f45e89e70#llm%E3%81%AEfine-tuning%E6%89%8B%E6%B3%95%E3%81%BE%E3%81%A8%E3%82%81&quot;&gt;LLM fine-tuning methods&lt;/a&gt; such as LoRA and PEFT&lt;/li&gt;
&lt;li&gt;Surveyed various pre-trained and fine-tuned models in the &lt;a href=&quot;https://huggingface.co/learn/nlp-course/chapter3/1?fw=pt&quot;&gt;HuggingFace documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Completed the &lt;a href=&quot;https://www.linkedin.com/feed/update/urn:li:activity:7160184133703372800/&quot;&gt;Generative AI with Large Language Models&lt;/a&gt; course offered by DeepLearning.AI and Amazon Web Services to enhance my understanding of these techniques&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;ol start=&quot;2&quot;&gt;
&lt;li&gt;Data Engineering and training data curation
&lt;ul&gt;
&lt;li&gt;Data sampling ensuring high alignment with business KPIs along with diversity and representativeness&lt;/li&gt;
&lt;li&gt;Extracting features from candidate resumes using NLP techniques&lt;/li&gt;
&lt;li&gt;Structured these features into prompts with the &lt;code class=&quot;language-text&quot;&gt;Alpaca format&lt;/code&gt; for instruction fine-tuning (supervised)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;ol start=&quot;3&quot;&gt;
&lt;li&gt;Base model selection: &lt;a href=&quot;https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b-fast-instruct&quot;&gt;elyza/ELYZA-Japanese-Llama-2-7b-fast-instruct&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;open-source 7B parameter model with Japanese language capabilities&lt;/li&gt;
&lt;li&gt;fine-tuned on Meta’s &lt;a href=&quot;https://llama.meta.com/llama2/&quot;&gt;Llama2&lt;/a&gt; Large Language Model&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;ol start=&quot;4&quot;&gt;
&lt;li&gt;Fine-tuning Experiments
&lt;ul&gt;
&lt;li&gt;QLoRA (using bitsandbytes and PEFT) to optimize compute budget&lt;/li&gt;
&lt;li&gt;Tuning batch sizes to optimize memory and training time&lt;/li&gt;
&lt;li&gt;Filtering prompt lengths to reduce memory usage (irregular padding) and increase batch sizes&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;ol start=&quot;5&quot;&gt;
&lt;li&gt;LLMOps
&lt;ul&gt;
&lt;li&gt;Running large parametric TrainingJobs from small SageMaker notebook instances&lt;/li&gt;
&lt;li&gt;Logging results and metrics (eg. ROUGE score) to S3 for quality feedback from domain experts, crucial in refining the model and improving output quality&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;In the end, as expected, there was much room for improvement. It requires a lot more effort
to achieve expected metrics and quality to beat a proprietary external LLM. Enhancing
data size and quality, tuning model parameters, implementing iterative evaluation with
human feedback (RLHF) were some of the ways in which I could have refined performance.
The project sucessfully laid the groundwork for future LLM fine-tuning efforts within
our AI Group, showcasing the potential for developing specialized in-house models
tailored to our unique needs.&lt;/p&gt;
&lt;h3 id=&quot;p4&quot;&gt;4. Financial Document OCR System Improvements&lt;/h3&gt;
&lt;p&gt;Building on the success of the &lt;a href=&quot;/blog/2021-visional#p1&quot;&gt;initial Financial Document OCR system&lt;/a&gt;,
I led 2 major improvement phases that further enhanced the system’s accuracy and usability.&lt;/p&gt;
&lt;p&gt;Improvement 1:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The first phase focused on improving the text recognition accuracy, particularly for
documents with poor scan quality or complex layouts. I incorporated advanced image
preprocessing techniques such as noise reduction, skew correction, and adaptive
thresholding. The qualitative evaluation score passed for 90% of the test files
as a result, increasing from around 85% last term. As an extra, the output format was
changed from XLS to XLSX for better compatibility.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Improvement 2:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The second phase aimed at expanding the support for 4 new types of documents. I
designed image processing algorithms able to deal with tables with defined cell
boundaries as well as processing wrapped text in such cells. The format detection
logic was updated to support both old and new algorithms.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;While I am aware that new and faster methods using multi-modal LLMs are now available, but at that time, these improvements had not only enhanced the system’s performance but also reduced the manual effort required in processing financial documents, leading to substantial time savings and increased productivity for the team. A benchmarking by the Succeed team proved the internal tool to be better than &lt;a href=&quot;https://line-works.com/ai-product/ocr/&quot;&gt;Line Works OCR&lt;/a&gt; tool in terms of data privacy, ease of use and specialized usecase.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; margin: 0 0 30px;&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/a1bcd07a05bdf262fc28224f19e40278/37114/focr.png&quot; style=&quot;display: block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom: 45.27027027027027%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAABYlAAAWJQFJUiTwAAABYklEQVR42k1Sy3KDMAzk/z8uJc0AoecEUoa3bTDYoGrlMVMu2JJ2tVo5cc7ReZx0HAft+06444z/tm3XHTncvfcc2yWW3u90u91oGEeq6orarqUEifM8Bbgsi4DiHSTeg8zRuloyZrnuB4u4M2FRFIRvnCauWSkBeOQObdtKou976R6UOLLbTq/XS3Jaa1JKCynyWZbT8/kUAUopbmgCodaGSQchmrhTGN3LH2rQDAQAzPMsMZCmaUplWQoO0wkhOoOkaRqy1sr55Ng6T6S7jjYGV++35ACwPBbI1W9D3zxynufSGOpRIwqHYRBC+NcxiSwFS+ACazeq68+lDkD4h3z6lfLYGZ8dGVaIZkn0CsXwIfikro1j9EiGsZCHRWgK/zByjBujg0IEAEARwGgSnwf86vvgr+VNo9a5sJSy/JEtx9ehWcil8P/7i38UYqvTNMuTwTluE0t5PB5C6LkeViH3B1lHtdyi/PVPAAAAAElFTkSuQmCC&apos;); background-size: cover; display: block;&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;old image since I cannot share proprietary example&quot; title=&quot;&quot; src=&quot;/static/a1bcd07a05bdf262fc28224f19e40278/fcda8/focr.png&quot; srcset=&quot;/static/a1bcd07a05bdf262fc28224f19e40278/12f09/focr.png 148w,
/static/a1bcd07a05bdf262fc28224f19e40278/e4a3f/focr.png 295w,
/static/a1bcd07a05bdf262fc28224f19e40278/fcda8/focr.png 590w,
/static/a1bcd07a05bdf262fc28224f19e40278/efc66/focr.png 885w,
/static/a1bcd07a05bdf262fc28224f19e40278/c83ae/focr.png 1180w,
/static/a1bcd07a05bdf262fc28224f19e40278/37114/focr.png 2300w&quot; sizes=&quot;(max-width: 590px) 100vw, 590px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;&gt;
  &lt;/a&gt;
    &lt;/span&gt;
&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id=&quot;company&quot;&gt;About Visional&lt;/h2&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; margin: 0 0 30px;&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/7dcdbba96ae2789a28d90c974627ddb4/3ec44/visional_cafe.jpg&quot; style=&quot;display: block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom: 47.972972972972975%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAKABQDASIAAhEBAxEB/8QAGAAAAwEBAAAAAAAAAAAAAAAAAAIEAwX/xAAWAQEBAQAAAAAAAAAAAAAAAAADAQL/2gAMAwEAAhADEAAAAdWoUlhOoXP/xAAaEAEAAwADAAAAAAAAAAAAAAABAgMRABMx/9oACAEBAAEFAuyYyscm26huHDz/xAAVEQEBAAAAAAAAAAAAAAAAAAAQEf/aAAgBAwEBPwGH/8QAFREBAQAAAAAAAAAAAAAAAAAAEBH/2gAIAQIBAT8Bp//EABkQAAMAAwAAAAAAAAAAAAAAAAABIRAiMf/aAAgBAQAGPwKsjZq4cFj/xAAaEAEAAgMBAAAAAAAAAAAAAAABABEhQVGR/9oACAEBAAE/ISgwBoisG1wj70o5NI8joEIYFT//2gAMAwEAAgADAAAAENDf/8QAFhEBAQEAAAAAAAAAAAAAAAAAAAER/9oACAEDAQE/EMq//8QAFhEBAQEAAAAAAAAAAAAAAAAAAAER/9oACAECAQE/ENo//8QAHBABAQACAgMAAAAAAAAAAAAAAREAITFBUaGx/9oACAEBAAE/EHMgCkt75xxvgB5jv7rB7gEJZ6wrASdDJluvjIohbomf/9k=&apos;); background-size: cover; display: block;&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;Visional&quot; title=&quot;&quot; src=&quot;/static/7dcdbba96ae2789a28d90c974627ddb4/1c72d/visional_cafe.jpg&quot; srcset=&quot;/static/7dcdbba96ae2789a28d90c974627ddb4/a80bd/visional_cafe.jpg 148w,
/static/7dcdbba96ae2789a28d90c974627ddb4/1c91a/visional_cafe.jpg 295w,
/static/7dcdbba96ae2789a28d90c974627ddb4/1c72d/visional_cafe.jpg 590w,
/static/7dcdbba96ae2789a28d90c974627ddb4/a8a14/visional_cafe.jpg 885w,
/static/7dcdbba96ae2789a28d90c974627ddb4/fbd2c/visional_cafe.jpg 1180w,
/static/7dcdbba96ae2789a28d90c974627ddb4/3ec44/visional_cafe.jpg 1715w&quot; sizes=&quot;(max-width: 590px) 100vw, 590px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;&gt;
  &lt;/a&gt;
    &lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;Established in April 2009, BizReach has been operating a variety of Internet services
that support the future of work in Japan with a mission that roughly translates into
“creating a society where everyone can believe in their own potential”. The company,
headquartered in (Shibuya) Tokyo, has regional offices in Osaka, Nagoya, Fukuoka, Shizuoka, and Hiroshima.&lt;/p&gt;
&lt;p&gt;Visional was born in February 2020, when Bizreach Inc. shifted to a group management
structure. The company has continued its significant role in Japan’s HR Tech and
SaaS sectors that promote digital transformation (DX) of industry and support the
improvement of productivity in Japan.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Within the last 3 years, Visional Inc. was also listed on the Tokyo Stock Exchange.
Initially listed on the TSE’s Mothers segment on April 22, 2021, Visional successfully
transitioned from the Growth Market segment to the Prime Market segment on December 14, 2023.
The company has demonstrated strong financial performance. As of late July 2024, Visional
was trading at a price around 9,370 JPY with a market cap of approximately 367.44 billion JPY.
Their P/E ratio stands around 24.66 and a significant profit growth of 53.2% YoY in FY2024 Q3​.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Visional has strengthened its HR Tech segment, and platforms like BizReach and HRMOS
are central to helping companies utilize human resources based on &lt;strong&gt;data-driven insights&lt;/strong&gt;.
The incubation segment has also seen significant growth, focusing on developing new businesses
in areas with high market potential, particularly in logistics and B2B services​.&lt;/p&gt;
&lt;p&gt;The group broadly has a heirarchical structure with the following products and services:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://www.visional.inc/ja/index.html&quot;&gt;Visional Corporation&lt;/a&gt;, the holding company, supports the group management&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://www.bizreach.co.jp/&quot;&gt;Bizreach Corporation&lt;/a&gt;, responsible for the management of the original HR Tech and SaaS businesses&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://www.bizreach.jp/&quot;&gt;Bizreach&lt;/a&gt;, a professional HR platform that connects companies with available talent&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://hrmos.co/hr&quot;&gt;HRMOS Talent Management&lt;/a&gt;, an employee management platform&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://hrmos.co/ats&quot;&gt;HRMOS Recruitment&lt;/a&gt;, an recruitment management system&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://hrmos.co/kintai&quot;&gt;HRMOS Attendance&lt;/a&gt;, an attendance management system&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.ezsoft.co.jp/ekeihi/&quot;&gt;HRMOS Expenses&lt;/a&gt;, an expense settlement system&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://hrmos.co/payroll&quot;&gt;HRMOS Payroll&lt;/a&gt;, a payroll management system&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://br-campus.jp/&quot;&gt;Bizreach Campus&lt;/a&gt;, an alumni network service for career consultation&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://www.visional.inc/ja/visional-incubation.html&quot;&gt;Visional Incubation Corporation&lt;/a&gt;, responsible for new business development and acquisitions&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://ma-succeed.jp/&quot;&gt;M&amp;#x26;A Succeed&lt;/a&gt;, an M&amp;#x26;A platform for business succession&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://yamory.io/&quot;&gt;Yamory&lt;/a&gt;, security vulnerability management cloud for IT systems&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.trabox.ne.jp/&quot;&gt;Trabox&lt;/a&gt;, a platform focused on digital transformation in the logistics industry&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://assured.jp/ja/&quot;&gt;Assured&lt;/a&gt;, a cloud security risk assessment service&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://jp.stanby.com/&quot;&gt;StanBy&lt;/a&gt;, a job-search engine jointly owned with Yahoo Japan&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Each of the products have their own specific engineering teams that follow custom
Agile software development life cycles (SDLCs) they are comfortable with.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;AI Team&lt;/strong&gt; is a team of around 20 ML/AI engineers that collaborate with one or
more of the above mentioned products to understand their business requirements, and leverage
data to deploy AI features in production environments to drive business solutions.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id=&quot;conclusion&quot;&gt;The Road Ahead&lt;/h2&gt;
&lt;p&gt;The last three years have not just been about professional growth; they have also been
about personal development. Adopting OKRs, improving my communication skills in a multilingual
environment, and taking ownership of more complex projects have all contributed to a more
holistic growth trajectory. I’ve learned to balance my technical expertise with strategic
thinking, ensuring that I can contribute effectively both as an engineer and as a leader.&lt;/p&gt;
&lt;p&gt;As I look forward to the next phase of my career, I am excited about the possibilities that
lie ahead. After a quarter of my life, I am well aware that I shall always be a Jack-of-all-Trades.
Whether it’s further deepening my expertise in MLOps, taking on more leadership roles, or exploring new domains
within LLM applications, I am confident that the foundation I’ve built over these five years will serve me well.&lt;/p&gt;
&lt;p&gt;Also, after all these years, I am happy that I can say &lt;code class=&quot;language-text&quot;&gt;頑張りましたね！&lt;/code&gt;, instead of my usual &lt;code class=&quot;language-text&quot;&gt;eigo tabemasen..&lt;/code&gt; lol.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This blog serves as a continuation of my reflection from three years ago. If you’re interested in understanding how it all began, feel free to check out &lt;a href=&quot;/blog/2021-visional&quot;&gt;A Look Back at My First Full-Time Job&lt;/a&gt;.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Docker Basics]]></title><description><![CDATA[As a software developer, especially an ML Engineer working in a cross-functioning team, I have come to understand the importance of creating…]]></description><link>https://kj7kunal.github.io/blog/2021-docker-basics/</link><guid isPermaLink="false">https://kj7kunal.github.io/blog/2021-docker-basics/</guid><pubDate>Sat, 11 Dec 2021 12:00:00 GMT</pubDate><content:encoded>&lt;p&gt;As a software developer, especially an ML Engineer working in a cross-functioning team, I have come to understand the importance of creating reproducible code, that can be transferred from your dev machine to the staging or production environment with minimal required settings.&lt;/p&gt;
&lt;p&gt;Docker is one of the most popular tools used to containerize and deploy applications. It makes building, deploying and managing applications extremely simple. It does this by using something called containers, which allow the developer to package up an application with all the parts it needs, with their own set of software, libraries and configuration file associated with them. Not only is each container isolated from other containers, they are easily reproducible entities that allow for easy abstraction during testing and deployment.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://12factor.net/&quot;&gt;The Twelve-Factor App&lt;/a&gt; comes up with a general guide of basic requirements any developer who builds, deploys or manages applications which run as a service should follow. And using Docker means you’re already satisfying the majority of them.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Contents&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#why&quot;&gt;Why do you need docker?&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#devops&quot;&gt;DevOps in Industry&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#prerequisite&quot;&gt;Basic Prerequisites&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#os&quot;&gt;Operating System&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#containers&quot;&gt;Containers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#docker&quot;&gt;Docker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#vm&quot;&gt;VMs vs Containers&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#using&quot;&gt;Using Docker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#commands&quot;&gt;Docker Commands&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#docker-run&quot;&gt;docker run command&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#ports&quot;&gt;ports&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#volumes&quot;&gt;volumes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#envvars&quot;&gt;environment variables&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#docker-ps&quot;&gt;Container related commands&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#docker-images&quot;&gt;Image related commands&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#docker-inspect&quot;&gt;Monitoring commands&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#containerize&quot;&gt;Containerising an application&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#guide&quot;&gt;“Setup Guide Translation”&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#dockerfile&quot;&gt;Dockerfile&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#instructions&quot;&gt;Instructions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#docker-build&quot;&gt;Building an image&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#docker-compose&quot;&gt;docker-compose&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#example&quot;&gt;Example Application&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#more&quot;&gt;Further Reading&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id=&quot;why&quot;&gt;Why do you need docker?&lt;/h2&gt;
&lt;p&gt;Any fully functioning web-app is comprised of multiple constituent services. For example an end-to-end application stack might consist of the following components:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Web server (eg. Node.js)&lt;/li&gt;
&lt;li&gt;Database (eg. mongoDB, sql)&lt;/li&gt;
&lt;li&gt;Frontend/Messaging (eg. Redis)&lt;/li&gt;
&lt;li&gt;Orchestration tool (eg. Ansible)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Before Docker came around, when you wanted to test your code on a different machine, you had to carefully read a big setup manual and install all the required dependencies on each machine before being able to modify or run it. Moreover, different stack require compatibility with the underlying OS and different services require different versions of dependencies to be installed, which are themselves OS dependent. Therefore, these installations clutter up your dev machine and it surely is not a reproducible environment. For example, if two systems use different versions of the same dependency, then there could be a dependency collision.&lt;/p&gt;
&lt;p&gt;Further, when applications are updated, the associated architecture might require minor or major changes. Architecture changes require dependency/compatibility checks, and new dependencies might collide with older dependencies, which leads to what is called the &lt;code class=&quot;language-text&quot;&gt;compatibility matrix&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;And in a cross-functioning setting like my team, where different product teams have their own dev, staging and production environments, and each product team sets their own priorities for different projects, these dependency collisions might have led to delays in deployments and the requirement of modifications to support certain dependency versions. Does upgrading the production machines to the latest distribution add value or money to the company? No. From the outset, the opposite is true: it costs time and thus, money. Thankfully, instead of frustrating code modifications, we use Docker to address all these issues.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The main purpose of Docker is to package and containerise applications, and to ship them, and run them anywhere, any number of times you want.&lt;/strong&gt; It provides a &lt;code class=&quot;language-text&quot;&gt;containerised&lt;/code&gt; solution for developing, building and shipping an application:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Separate libraries and dependencies for each service&lt;/li&gt;
&lt;li&gt;All within same VM and OS, but within isolated environments (containers)&lt;/li&gt;
&lt;li&gt;Easily installable with just a docker run command&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;devops&quot;&gt;DevOps in Industry&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Traditional
&lt;ul&gt;
&lt;li&gt;Dev team develops applications&lt;/li&gt;
&lt;li&gt;Hands over to Ops team to deploy and manage in PRD environment&lt;/li&gt;
&lt;li&gt;Dev team provides a guide of information to Ops team
&lt;ul&gt;
&lt;li&gt;Basic setup on host&lt;/li&gt;
&lt;li&gt;Prerequisite libraries and dependencies&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Ops team can easily face issues due to
&lt;ul&gt;
&lt;li&gt;Incorrect/incomplete instructions from Dev team&lt;/li&gt;
&lt;li&gt;Mistakes in following instructions&lt;/li&gt;
&lt;li&gt;Dev and Ops have to work together to resolve issues&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;With Docker
&lt;ul&gt;
&lt;li&gt;Dockerfile is analogous to the Dev guide in traditional DevOps
&lt;ul&gt;
&lt;li&gt;(Dev + Ops) work hand in hand to create the Dockerfile&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Dockerfile is used to create the image for the application&lt;/li&gt;
&lt;li&gt;Since this image already runs on dev system, it is supposed to work on any other host that has Docker installed&lt;/li&gt;
&lt;li&gt;Ops deploys the application using the image and manages PRD environment&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Docker transforms Dev + Ops -&gt; DevOps
&lt;ul&gt;
&lt;li&gt;Every application can be containerised&lt;/li&gt;
&lt;li&gt;In the future, it is very probable that no one will need to install anything
&lt;ul&gt;
&lt;li&gt;Just use docker to run a container for the image&lt;/li&gt;
&lt;li&gt;Stop the container when it’s no longer required&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id=&quot;prerequisite&quot;&gt;Basic Prerequisites&lt;/h2&gt;
&lt;p&gt;Before we go any forward, just in case, I will introduce some basic prerequisites in brief bullet points. If you need more information, please go wild on google.&lt;/p&gt;
&lt;h3 id=&quot;os&quot;&gt;Operating System&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Any OS consists of 2 parts: the OS Kernel and Application Softwares&lt;/li&gt;
&lt;/ul&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/thumb/8/8f/Kernel_Layout.svg/1920px-Kernel_Layout.svg.png&quot; width=&quot;200&quot; alt=&quot;OS Components&quot;&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;OS kernel interacts with underlying hardware and is machine-dependent eg. Linux, Windows, MacOS&lt;/li&gt;
&lt;li&gt;Application softwares like user interfaces, compilers, etc., differentiate the OS into different flavours. eg. ubuntu, fedora, Debian, SUSE are different flavours of the OS which uses the Linux kernel&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;containers&quot;&gt;Containers&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Completely isolated environments with their own processes for services, own network interfaces and own mounts&lt;/li&gt;
&lt;li&gt;Different containers share the same OS kernel&lt;/li&gt;
&lt;li&gt;Setting up containers can be hard since they are basically very low level code&lt;/li&gt;
&lt;li&gt;Not meant to host a standalone OS (unlike VMs)&lt;/li&gt;
&lt;li&gt;Life of a container is straightforward:
&lt;ul&gt;
&lt;li&gt;install required dependencies over the OS kernel&lt;/li&gt;
&lt;li&gt;run a specific task/process/command (servers, databases, computation, analysis tasks)&lt;/li&gt;
&lt;li&gt;finish the task&lt;/li&gt;
&lt;li&gt;clean up and exit&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;docker&quot;&gt;Docker&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Docker uses LXC (Linux containers), but containers can be LXD, LXCFS, etc&lt;/li&gt;
&lt;li&gt;Has powerful tools for setting up containers easily for end users that abstract the low level code for setting up and managing containers&lt;/li&gt;
&lt;li&gt;Docker can run containers based on a different flavoured OS on any OS, as long as they share the same kernel&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;vm&quot;&gt;VMs vs Containers&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;VM&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;High resource utilization&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Heavy size (in GBs)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Has to boot-up entire OS (in minutes)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Structure:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;- Hardware Infra 
  - Hypervisor (or virtual machine monitor/VMM -&gt; similar to emulator)
    - VMs
      - Application services
      - Libraries/Dependencies
      - OS&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Containers&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Low resource utilization&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Lightweight (in MBs)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Low boot-up time (in seconds)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Structure:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;- Hardware Infra 
  - OS
    - Docker installed on the OS
      - Containers
        - Applications services
        - Libraries/Dependencies&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://images.contentstack.io/v3/assets/blt300387d93dabf50e/bltb6200bc085503718/5e1f209a63d1b6503160c6d5/containers-vs-virtual-machines.jpg&quot; width=&quot;600&quot; alt=&quot;VM vs Containers&quot;&gt;
&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id=&quot;using&quot;&gt;Using Docker&lt;/h2&gt;
&lt;p&gt;Here is a simple process for using Docker:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Install Docker Engine on your host&lt;/li&gt;
&lt;li&gt;Containerised applications readily available on dockerhub (public docker registry) as &lt;code class=&quot;language-text&quot;&gt;images&lt;/code&gt;
&lt;ul&gt;
&lt;li&gt;Most common OSes, databases, services, tools, etc are available&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;docker run &amp;lt;image name&gt;&lt;/code&gt;
&lt;ul&gt;
&lt;li&gt;Runs an instance (container) of an image&lt;/li&gt;
&lt;li&gt;Each run command creates a new instance (new container)&lt;/li&gt;
&lt;li&gt;If one instance breaks, destroy it and launch new instance&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Docker Image&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Package/template used to create one or more containers&lt;/li&gt;
&lt;li&gt;Containers are basically instances of image that are isolated and have their own environment that runs the services/processes defined in the image&lt;/li&gt;
&lt;li&gt;Containers based on the same image may run differently depending on the values supplied during the docker run command &lt;a href=&quot;#docker-run&quot;&gt;(see below for options)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id=&quot;commands&quot;&gt;Docker Commands&lt;/h2&gt;
&lt;p&gt;Following is a description of commonly used docker commands. Again, it is presented in a bullet-point summary&lt;/p&gt;
&lt;h3 id=&quot;docker-run&quot;&gt;docker run&lt;/h3&gt; 
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;code class=&quot;language-text&quot;&gt;docker run &amp;lt;image name&gt;&lt;/code&gt;&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Base command to run a container instance of an image&lt;/li&gt;
&lt;li&gt;Looks for image
&lt;ul&gt;
&lt;li&gt;Checks locally first&lt;/li&gt;
&lt;li&gt;If no local image, searches docker hub and pulls that image&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Runs container in
&lt;ul&gt;
&lt;li&gt;attached state&lt;/li&gt;
&lt;li&gt;non-interactive mode&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;docker run &amp;#x3C;image name&gt;&lt;code class=&quot;language-text&quot;&gt;:&amp;lt;image tag&gt;&lt;/code&gt;&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Specifies version of image to run
&lt;ul&gt;
&lt;li&gt;eg. docker run redis:4.0 -&gt; pulls latest 4.0 image&lt;/li&gt;
&lt;li&gt;Specify short or long tags as per requirement&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;If tag is not supplied, docker pulls latest available image&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;docker run &amp;#x3C;image name&gt; &lt;code class=&quot;language-text&quot;&gt;&amp;lt;command&gt;&lt;/code&gt;&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;If image does not run any service by default (eg. ubuntu), append a command so container doesn’t exit immediately&lt;/li&gt;
&lt;li&gt;Examples
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;docker run ubuntu sleep 100&lt;/em&gt;
&lt;ul&gt;
&lt;li&gt;runs an instance of ubuntu OS -&gt; sleeps for 100s -&gt; exits&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;em&gt;docker run python:3.6 cat /etc/*release*&lt;/em&gt;
&lt;ul&gt;
&lt;li&gt;Runs an instance of python:3.6 image -&gt; outputs OS information -&gt; exits&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;docker run &lt;code class=&quot;language-text&quot;&gt;-d&lt;/code&gt; &amp;#x3C;image name&gt;&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Runs instance in background (detached mode)
&lt;ul&gt;
&lt;li&gt;So we can use the host terminal for other commands&lt;/li&gt;
&lt;li&gt;If -d is not specified (attached mode), need to open a new terminal or explicitly exit (ctrl + c) the container&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;docker attach &amp;lt;container_id/name&gt;&lt;/code&gt;
&lt;ul&gt;
&lt;li&gt;Reattaches detached container to terminal console to view outputs&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;docker run &lt;code class=&quot;language-text&quot;&gt;-it&lt;/code&gt; &amp;#x3C;image name&gt;&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;i&lt;/code&gt;: Runs container in interactive mode
&lt;ul&gt;
&lt;li&gt;Maps the STDIN of host to the docker container&lt;/li&gt;
&lt;li&gt;Necessary if service asks for input from the user (interactive mode)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;t&lt;/code&gt;: Attaches host’s terminal to container
&lt;ul&gt;
&lt;li&gt;Able to use STDOUT of host to output container outputs&lt;/li&gt;
&lt;li&gt;Necessary if service has to show prompt to the user (in interactive mode)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;s id=&quot;ports&quot;&gt;&lt;/s&gt;&lt;strong&gt;docker run &lt;code class=&quot;language-text&quot;&gt;-p &amp;lt;free port on docker host&gt;:&amp;lt;container’s listening port&gt;&lt;/code&gt; &amp;#x3C;image name&gt;&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Maps ports on localhost to listen to container ports
&lt;ul&gt;
&lt;li&gt;Each container within the docker host has it’s own internal IP where it can be accessed
&lt;ul&gt;
&lt;li&gt;Internal IP can only be accessed inside the docker host&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;External users can use the IP of the docker host but with a &lt;strong&gt;free port&lt;/strong&gt; on the docker host mapped to the container’s port&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;eg. 80:5000 means port 80 on docker host is mapped to listen to port 5000 on the container service&lt;/li&gt;
&lt;li&gt;Multiple instances of applications (same or different) can be mapped to different ports on the docker host. Example:
&lt;ul&gt;
&lt;li&gt;docker run -p 80:5000 web_app&lt;/li&gt;
&lt;li&gt;docker run -p 8000:5000 web_app&lt;/li&gt;
&lt;li&gt;docker run -p 3306:3306 mysql:2.3 (3306 is mysql’s default port)&lt;/li&gt;
&lt;li&gt;docker run -p 8306:3306 mysql&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;s id=&quot;volumes&quot;&gt;&lt;/s&gt;&lt;strong&gt;docker run &lt;code class=&quot;language-text&quot;&gt;-v &amp;lt;directory on docker host&gt;:&amp;lt;container’s directory&gt;&lt;/code&gt; &amp;#x3C;image name&gt;&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Maps directories on localhost to the container directories&lt;/li&gt;
&lt;li&gt;Docker containers have their own isolated filesystems
&lt;ul&gt;
&lt;li&gt;Services may create and store data within the containers
&lt;ul&gt;
&lt;li&gt;eg. mysql stores databases and tables in /var/lib/mysql&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;If container is deleted, this storage is deleted with the container&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;To persist the data, you need to mount an external directory on the docker host to a directory inside the container
&lt;ul&gt;
&lt;li&gt;eg. &lt;em&gt;/opt/datadir:/var/lib/mysql&lt;/em&gt; means &lt;em&gt;/opt/datadir&lt;/em&gt; on docker host is mounted as &lt;em&gt;/var/lib/mysql&lt;/em&gt; on the container&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;More general command:
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;docker run \
--mount type=&amp;lt;bind or volume&gt;,source=&amp;lt;directory on docker host&gt;,target=&amp;lt;container’s directory&gt; \
\&amp;lt;image name&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;docker volume create d_v&lt;/code&gt;
&lt;ul&gt;
&lt;li&gt;creates persistent volume in &lt;em&gt;/var/lib/docker/volumes/d_v&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;d_v:/var/lib/mysql&lt;/em&gt; means &lt;em&gt;/var/lib/docker/volumes/d_v&lt;/em&gt; on docker host is mounted as &lt;em&gt;/var/lib/mysql&lt;/em&gt; on the container&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;s id=&quot;envvars&quot;&gt;&lt;/s&gt;&lt;strong&gt;docker run &lt;code class=&quot;language-text&quot;&gt;-e &amp;lt;ENV_VAR&gt;=&amp;lt;value&gt;&lt;/code&gt; &amp;#x3C;image name&gt;&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Runs container with specified value of environment variable&lt;/li&gt;
&lt;li&gt;Can run multiple containers with different environment variable values in the same docker host&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;docker-ps&quot;&gt;Container related commands&lt;/h3&gt; 
&lt;ul&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;docker ps&lt;/code&gt;
&lt;ul&gt;
&lt;li&gt;Lists all running containers and basic information
&lt;ul&gt;
&lt;li&gt;Container ID: random id for the container assigned by docker&lt;/li&gt;
&lt;li&gt;Image: name of the image used to run the container&lt;/li&gt;
&lt;li&gt;Command: command specified by the image&lt;/li&gt;
&lt;li&gt;Created: time elapsed after creation of container&lt;/li&gt;
&lt;li&gt;Status: how much time the container has been “up”, “exited”, etc&lt;/li&gt;
&lt;li&gt;Ports: ports being listened to by the instance&lt;/li&gt;
&lt;li&gt;Names: random name for the container assigned by docker&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;docker ps -a&lt;/code&gt;
&lt;ul&gt;
&lt;li&gt;Lists all running and previously exited containers&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;docker stop &amp;lt;container_id/name&gt; [container_id/name...]&lt;/code&gt;
&lt;ul&gt;
&lt;li&gt;Shows container name on success and exits container&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;docker rm &amp;lt;container_id/name&gt; [container_id/name...]&lt;/code&gt;
&lt;ul&gt;
&lt;li&gt;Removes exited containers from storage&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;docker exec &amp;lt;container_id/name&gt; &amp;lt;command&gt;&lt;/code&gt;
&lt;ul&gt;
&lt;li&gt;Runs a command on a running container&lt;/li&gt;
&lt;li&gt;eg. docker exec hoge_hoge cat /etc/hosts
&lt;ul&gt;
&lt;li&gt;Lists the contents of the /etc/hosts file (localhost, etc)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;docker-images&quot;&gt;Image related commands&lt;/h3&gt; 
&lt;ul&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;docker images&lt;/code&gt;
&lt;ul&gt;
&lt;li&gt;Lists all available images on host docker and basic information
&lt;ul&gt;
&lt;li&gt;Repository: image name&lt;/li&gt;
&lt;li&gt;Tag: image version (default: latest)&lt;/li&gt;
&lt;li&gt;Image ID: docker assigned random id&lt;/li&gt;
&lt;li&gt;Created: when created/pulled to local&lt;/li&gt;
&lt;li&gt;Size: storage size&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;docker rmi &amp;lt;image name&gt;&lt;/code&gt;
&lt;ul&gt;
&lt;li&gt;deletes image from local&lt;/li&gt;
&lt;li&gt;MUST STOP all dependent containers before removing image&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;docker pull &amp;lt;image name&gt;&lt;/code&gt;
&lt;ul&gt;
&lt;li&gt;Pulls image from docker hub and stores on host&lt;/li&gt;
&lt;li&gt;DOES NOT run container&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;docker-inspect&quot;&gt;Monitoring commands&lt;/h3&gt; 
&lt;ul&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;docker inspect &amp;lt;container_id/name&gt;&lt;/code&gt;
&lt;ul&gt;
&lt;li&gt;Lists details of the container such as state, mounts, config, network settings, etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;docker log &amp;lt;container_id/name&gt;&lt;/code&gt;
&lt;ul&gt;
&lt;li&gt;View the logs written to the STDOUT of a detached container&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;docker history &amp;lt;image name&gt;&lt;/code&gt;
&lt;ul&gt;
&lt;li&gt;History of image build&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id=&quot;containerize&quot;&gt;Containerising an application&lt;/h2&gt;
&lt;p&gt;There are three levels of understanding when we containerize an application. As a summary, think of it this way:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You’re a developer who is happy with your code, and now wants the ops team to deploy it on the staging/production environment. You can easily send them a &lt;code class=&quot;language-text&quot;&gt;detailed explanation of the build steps in English&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;But its the 2020s, so instead you translate that traditional “guide book” into sequential instructions in a Dockerfile. It defines how a service is run. A &lt;code class=&quot;language-text&quot;&gt;docker image can be *built* from this Dockerfile&lt;/code&gt; that will containerize the service you built&lt;/li&gt;
&lt;li&gt;If your application stack consists of multiple services running parallely, you create a Dockerfile for each service or use an existing image from the docker registry. Instead of writing down run instructions for each service, you compile these instructions into a &lt;code class=&quot;language-text&quot;&gt;docker-compose.yml file that puts together options for all the constituent services&lt;/code&gt; and can be used to bring up entire the application stack.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;guide&quot;&gt;&quot;Setup Guide Translation&quot;&lt;/h3&gt;
&lt;p&gt;The first step to containerizing any service or application is to understand what we are containerising or what application are we creating the image for and how it is ‘built’&lt;/p&gt;
&lt;p&gt;Traditionally the instructions would have answered the question: &lt;code class=&quot;language-text&quot;&gt;What would you do if you want to deploy it manually? (eg. simple web app)&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;1. Decide OS (eg. Ubuntu)
2. Update OS source repos (eg. apt-get update)
3. Install dependencies (eg. apt-get install) (eg. Python)
4. Install application dependencies (eg. pip install)
5. Copy source code of the application to the specified path inside the running container (eg. /opt)
6. Finally, run the service (eg. Using the flask command)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;To build a docker image, we need to translate these instructions into a set of commands called the &lt;code class=&quot;language-text&quot;&gt;Dockerfile&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;FROM Ubuntu 
RUN apt-get update
RUN apt-get install python
RUN pip install flask flask-mysql
COPY . /opt/src
ENTRYPOINT FLASK_APP=/opt/src/app.py flask run&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id=&quot;dockerfile&quot;&gt;Dockerfile&lt;/h3&gt;
&lt;p&gt;A Dockerfile is a text file written in &lt;code class=&quot;language-text&quot;&gt;&amp;lt;INSTRUCTION&gt; &amp;lt;argument&gt;&lt;/code&gt; format that docker can understand.&lt;/p&gt;
&lt;p&gt;&lt;span id=&quot;instructions&quot;&gt;Following are few of the required instructions Docker may require to build an image:&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;FROM&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Defines the Base OS or Base image&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Every docker image must have exactly one FROM clause&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Examples
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;FROM Ubuntu&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;FROM python:3.6&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;RUN&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Instructs docker to run commands on base images&lt;/li&gt;
&lt;li&gt;Used to install dependencies&lt;/li&gt;
&lt;li&gt;Example
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;RUN apt-get update
RUN apt-get install python
RUN pip install flask flask-mysql&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;COPY&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Copies local files onto the docker image&lt;/li&gt;
&lt;li&gt;Places source code from local to required path inside container&lt;/li&gt;
&lt;li&gt;Example
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;COPY . /opt/src
COPY ../data /opt/data&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ENTRYPOINT&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Specifies the command that runs when image is run as a container&lt;/li&gt;
&lt;li&gt;Format
&lt;ul&gt;
&lt;li&gt;Shell format -&gt; &lt;code class=&quot;language-text&quot;&gt;ENTRYPOINT command&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;JSON array format -&gt; &lt;code class=&quot;language-text&quot;&gt;ENTRYPOINT [“command”]&lt;/code&gt; (preferred)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Examples
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;ENTRYPOINT FLASK_APP=/opt/src/app.py flask run&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;ENTRYPOINT [&quot;python&quot;, &quot;app.py&quot;]&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;Anything specified on the command line while using docker run will get &lt;strong&gt;appended to the end and run as command&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Example Dockerfile (ubuntu-sleeper)
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;FROM Ubuntu
ENTRYPOINT [“sleep”]&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;docker run ubuntu-sleeper 10&lt;/em&gt;
&lt;ul&gt;
&lt;li&gt;sleep 10 will be executed at container startup&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;It is recommended to supply a default param1 in case user forgets to supply it during docker run
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Always use JSON Array format for this&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Example Dockerfile (ubuntu-sleeper-default)
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;FROM Ubuntu
ENTRYPOINT [“sleep”]
CMD [“5”]&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;docker run ubuntu-sleeper-default&lt;/em&gt; -&gt; sleeps for 5s&lt;/li&gt;
&lt;li&gt;&lt;em&gt;docker run ubuntu-sleeper-default 10&lt;/em&gt; -&gt; overrides command and sleeps for 10s&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Overriding ENTRYPOINT
&lt;ul&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;docker run --entrypoint command2 ubuntu-sleeper 10&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CMD&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Defines the program that will be run within the container when it starts&lt;/li&gt;
&lt;li&gt;Format
&lt;ul&gt;
&lt;li&gt;Shell format -&gt; &lt;code class=&quot;language-text&quot;&gt;CMD command1 param1&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;JSON array format -&gt; &lt;code class=&quot;language-text&quot;&gt;CMD [“command1”, “param1”]&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;CMD [“command1”, “param1”]&lt;/code&gt; is equivalent to &lt;code class=&quot;language-text&quot;&gt;ENTRYPOINT[“command1”] CMD[“param1”]&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Examples (base images)
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;# for nginx image
CMD [“nginx”] &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;# for ubuntu image
CMD [“bash”]

# Not a process -&gt; bash shell that listens for inputs from a terminal, and exits immediately if no terminal (by default, no terminal)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;Example (custom image)
&lt;ul&gt;
&lt;li&gt;Dockerfile to run &lt;em&gt;sleep 5&lt;/em&gt; on ubuntu image
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;FROM Ubuntu
CMD sleep 5&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Overriding CMD
&lt;ul&gt;
&lt;li&gt;Overriding Param1 (use ENTRYPOINT)
&lt;ul&gt;
&lt;li&gt;Dockerfile: &lt;em&gt;FROM Ubuntu ENTRYPOINT [“sleep”]&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;docker run ubuntu 10&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Overriding entire CMD
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;docker run ubuntu sleep 10&lt;/em&gt;
&lt;ul&gt;
&lt;li&gt;Command line &lt;code class=&quot;language-text&quot;&gt;command2 param2&lt;/code&gt; will &lt;strong&gt;replace&lt;/strong&gt; any CMD specified in Dockerfile&lt;/li&gt;
&lt;li&gt;However, this is &lt;strong&gt;not recommended&lt;/strong&gt; as it doesn’t look very good&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;EXPOSE&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;WORKDIR&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span id=&quot;docker-build&quot;&gt;Once the Dockerfile has been written, we can build the image and push it to the docker registry&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;docker build &amp;lt;root directory of Dockerfile&gt; -t &amp;lt;image name&gt;&lt;/code&gt;
&lt;ul&gt;
&lt;li&gt;Builds your image using the Dockerfile and makes the image locally available on the host system&lt;/li&gt;
&lt;li&gt;You can see the various steps involved and the result of each task&lt;/li&gt;
&lt;li&gt;Images are built &lt;em&gt;layer by layer&lt;/em&gt; following the Dockerfile sequentially
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Current layer only stores changes from the previous layer&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Image size depends on last layer&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;All layers are cached
&lt;ul&gt;
&lt;li&gt;Layers cached as intermediate containers&lt;/li&gt;
&lt;li&gt;Helps in restarting build from the last step where the build fails
&lt;ul&gt;
&lt;li&gt;You can see ”---&gt; Using cache” in log for some steps&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;In case new steps are added/updated to Dockerfile, rebuilding the image only needs to run the updated tasks
&lt;ul&gt;
&lt;li&gt;Since source code is added after dependencies, updated source code does not need to reinstall dependencies&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;docker build &lt;code class=&quot;language-text&quot;&gt;-f &amp;lt;path to Dockerfile&gt;&lt;/code&gt; -t &amp;#x3C;image name&gt;
&lt;ul&gt;
&lt;li&gt;Use -f to specify path to Dockerfile&lt;/li&gt;
&lt;li&gt;Else need to provide context directory (root of Dockerfile)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;docker push &amp;lt;image name&gt;&lt;/code&gt;
&lt;ul&gt;
&lt;li&gt;Push to public dockerhub registry&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;docker-compose&quot;&gt;docker-compose&lt;/h3&gt;
&lt;p&gt;The &lt;code class=&quot;language-text&quot;&gt;docker-compose.yml&lt;/code&gt; file is a set of configurations defined in yaml file format that puts together a complex application running multiple services and options specific to run them.&lt;/p&gt;
&lt;p&gt;Once all the images have been built, and the docker-compose file is written, the entire application can be brought up using a single command:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;docker-compose up&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;So far, there are three formats available, in which the docker-compose file can be written:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Version 1 (default version, no need to specify)
&lt;ul&gt;
&lt;li&gt;Very basic&lt;/li&gt;
&lt;li&gt;Cannot change to a different network
&lt;ul&gt;
&lt;li&gt;attaches to default bridge network and needs to use links for communication between containers&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;em&gt;HAVE&lt;/em&gt; to mention applications in a &lt;em&gt;sequential&lt;/em&gt; yaml format
&lt;ul&gt;
&lt;li&gt;the order you would run them using docker run&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Version 2
&lt;ul&gt;
&lt;li&gt;Need to specify the version at the top of the file for versions 2+ (default 1)&lt;/li&gt;
&lt;li&gt;Encapsulates stack information into an outer “services” dictionary&lt;/li&gt;
&lt;li&gt;Automatically creates a new dedicated bridge network for the application
&lt;ul&gt;
&lt;li&gt;No need for links, since all the container names are application-specific&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Specifies a startup order using depends_on
&lt;ul&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;s_2: depends_on: s_1&lt;/code&gt; -&gt; s_2 will only start after s_1&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Version 3
&lt;ul&gt;
&lt;li&gt;Latest as of today&lt;/li&gt;
&lt;li&gt;Structure similar to version 2&lt;/li&gt;
&lt;li&gt;Adds support for Docker Swarm&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Following is a template for a version 3 docker-compose.yml file:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;version: &quot;3[.x]&quot;
services:
  &amp;lt;container name&gt;
    build:
      context: &amp;lt;relative path of directory containing Dockerfile OR git repository url&gt;
      dockerfile: &amp;lt;path to alternate Dockerfile&gt;
      args:
        ... &amp;lt;env vars and values valid during build&gt;
    image: &amp;lt;image name&gt;
    ports:
      - “&amp;lt;host port1&gt;:&amp;lt;container port1&gt;”
      - “&amp;lt;host port2&gt;:&amp;lt;container port2&gt;”
    volumes:
      - &amp;lt;volume mounting name&gt;:&amp;lt;container directory&gt;
      - &amp;lt;bind mounting path&gt;:&amp;lt;container directory&gt;
    networks:
      - &amp;lt;network name&gt;
    depends_on:
      - &amp;lt;other container name&gt;
    deploy:
      ...
  ...
networks:
  &amp;lt;network name&gt;:
  frontend:
  backend:
  ...
volumes:
  &amp;lt;volume mounting name&gt;:
  db-data:
  ...&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id=&quot;example&quot;&gt;Example Application: Setting up a Voting App&lt;/h3&gt;
&lt;p&gt;Designing and containerizing a voting app is a very popular example to illustrate docker to a beginner. The architecture looks something like this:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A &lt;code class=&quot;language-text&quot;&gt;python&lt;/code&gt; (flask) voting app collects votes from users&lt;/li&gt;
&lt;li&gt;The votes are stored first in an in-memory queue implemented using &lt;code class=&quot;language-text&quot;&gt;redis&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;A worker application based on &lt;code class=&quot;language-text&quot;&gt;.NET&lt;/code&gt; framework then collects the queued votes&lt;/li&gt;
&lt;li&gt;The queued votes are stored in a &lt;code class=&quot;language-text&quot;&gt;PostgreSQL&lt;/code&gt; database&lt;/li&gt;
&lt;li&gt;A &lt;code class=&quot;language-text&quot;&gt;node.js&lt;/code&gt; result app queries the database and displays the running result&lt;/li&gt;
&lt;/ul&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://raw.githubusercontent.com/docker/labs/master/beginner/images/bd3-architecture.png&quot; width=&quot;600&quot; alt=&quot;VM vs Containers&quot;&gt;
&lt;/p&gt;
&lt;p&gt;Assuming we have already built the images, then to bring up the application, we can either use a series of docker run commands or use docker-compose.&lt;/p&gt;
&lt;p&gt;The following docker run commands have to be implemented sequentially:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;First, deploy data layers
&lt;ul&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;docker run -d —name=redis redis&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;docker run -d —name=db postgres:9.4&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Then, run application services
&lt;ul&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;docker run -d —name=vote -p 5000:80 —link redis:redis voting-app&lt;/code&gt;
&lt;ul&gt;
&lt;li&gt;This web server has a Web UI instance running on port 80
&lt;ul&gt;
&lt;li&gt;Publish it to port 5000 to access it from outside host&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Voting-App source code searches for a Redis service named &lt;code class=&quot;language-text&quot;&gt;redis&lt;/code&gt;:
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;redis = Redis(host=“redis”, db=0, socket_timeout=5)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;Need to link container redis to the required service (host=“redis”)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;docker run -d —name=result -p 5001:80 —link db:db result-app&lt;/code&gt;
&lt;ul&gt;
&lt;li&gt;This web server has a Web UI instance running on port 80
&lt;ul&gt;
&lt;li&gt;Publish it to port 5001 to access it from outside host &lt;em&gt;(5000 is occupied)&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Result-App source code searches for a PostgreSQL service named &lt;code class=&quot;language-text&quot;&gt;db&lt;/code&gt;:
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;pg.connect(‘postgres://postgres@db/postgres&apos;, function(err, client, done) {callback(err, client)}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;Need to link container db to the required service (“db”)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Finally, deploy the worker
&lt;ul&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;docker run -d —name=worker —link redis:redis —link db:db worker&lt;/code&gt;
&lt;ul&gt;
&lt;li&gt;Worker source code looks for &lt;code class=&quot;language-text&quot;&gt;redis&lt;/code&gt; and &lt;code class=&quot;language-text&quot;&gt;db&lt;/code&gt; containers:
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;Jedis redis = connectToRedis(“redis”)
Connection dbConn = connectToDB(“db”)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;Need to link both data layer containers to worker service&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Now that we can deploy the application using docker run commands, it is very easy to translate the above to docker-compose.yml&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;version: &quot;1&quot; # no need to specify
redis:
  image: redis
db:
  image: postgres:9.4
vote:
  image: voting-app
  ports:
    - 5000:80
  links:
    - redis
result:
  image: result-app
  ports:
    - 5001:80
  links:
    - db
worker:
  image: worker
  links:
    - redis
    - db&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;version: &quot;3&quot;
services:
  redis:
    image: redis:alpine
    networks:
      - frontend
  db:
    image: postgres:9.4
    volumes:
      - db-data:/var/lib/postgresql/data
    networks:
      - backend
  vote:
    image: voting-app
    ports:
      - 5000:80
    networks:
      - frontend
    depends_on:
      - redis
  result:
    image: result-app
    ports:
      - 5001:80
    networks:
      - backend
    depends_on:
      - db
  worker:
    image: worker
    networks:
      - frontend
      - backend

networks:
  frontend:
  backend:

volumes:
  db-data:
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And of course, to bring up the application, type in a single command:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;docker-compose up&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Congratulations! Your first containerized application is up and running on your localhost.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id=&quot;more&quot;&gt;Further Reading&lt;/h2&gt;
&lt;p&gt;Here are some topics that can be explored next, when delving deeper into how Docker works, to be even closer to the “real” environment your code runs in:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Networks and links&lt;/li&gt;
&lt;li&gt;Docker Swarm&lt;/li&gt;
&lt;li&gt;Kubernetes (k8s)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Obviously this is far too much for this article, so let me just link to a few “distributions” of k8s:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;minikube: &lt;a href=&quot;https://kubernetes.io/docs/setup/learning-environment/minikube/&quot;&gt;https://kubernetes.io/docs/setup/learning-environment/minikube/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;k3s: &lt;a href=&quot;https://k3s.io/&quot;&gt;https://k3s.io/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;kind: &lt;a href=&quot;https://github.com/kubernetes-sigs/kind&quot;&gt;https://github.com/kubernetes-sigs/kind&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;mircok8s: &lt;a href=&quot;https://microk8s.io/&quot;&gt;https://microk8s.io/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content:encoded></item><item><title><![CDATA[Diwali 2021 AQI Data Visualization]]></title><description><![CDATA[As I was visiting my home in Delhi during Diwali 2021, I was very much looking forward
to celebrate the festival of lights together with my…]]></description><link>https://kj7kunal.github.io/blog/2020-aqi-dataviz/</link><guid isPermaLink="false">https://kj7kunal.github.io/blog/2020-aqi-dataviz/</guid><pubDate>Mon, 08 Nov 2021 12:00:00 GMT</pubDate><content:encoded>&lt;p&gt;As I was visiting my home in Delhi during Diwali 2021, I was very much looking forward
to celebrate the festival of lights together with my family after almost 7 years!&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Life Stages of The Firework&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#birth&quot;&gt;Birth&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#maturing&quot;&gt;Maturing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#ignition&quot;&gt;Ignition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#illumination&quot;&gt;Illumination&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#fading&quot;&gt;Fading&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#memory&quot;&gt;Memory&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id=&quot;birth&quot;&gt;History&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Birth&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Diwali is a festival celebrated all over India and holds significance for Hindus, Jains,
Sikhs and Buddhists. For example, for Hindus, it is said to mark the return of Lord Rama to
Ayodhya, after his 14-year exile and defeat of King Ravana. Since they returned on
a new moon night (Amavasya), the citizens of Ayodhya lit diyas all over to illuminate the
city in their jubilant welcome.&lt;/p&gt;
&lt;h2 id=&quot;maturing&quot;&gt;Present Day&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Maturing&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;As is the nature of history, traditions evolve. Since then, modern India looks to
celebrate the festival of lights with as much pomp and glamour as one can imagine. Fireworks
are lit all over the world as part of celebrations, and they have always been quite
a pleasing sight for the eyes.&lt;/p&gt;
&lt;h2 id=&quot;ignition&quot;&gt;Impact&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Ignition&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;However, as pleasing as it is, a look at the statistics quickly reveals the environmental impact
of Diwali celebrations. Fireworks produce high levels of air pollution, and the smoke and
particulate matter can cause serious health issues, especially for those with pre-existing
respiratory problems.&lt;/p&gt;
&lt;p&gt;Consequences of the celebration last all winter. The current AQI is deemed “SEVERE” - even
healthy individuals can develop respiratory illnesses.&lt;/p&gt;
&lt;p&gt;A &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6625239/&quot;&gt;2019 study&lt;/a&gt; conducted by
researchers in Pune shows that a person can be exposed to anywhere between 4.8 and
64.5 PPM of PM 2.5 within a few minutes.
&lt;a href=&quot;https://en.wikipedia.org/wiki/Air_quality_guideline&quot;&gt;WHO’s air quality guidelines&lt;/a&gt;
recommend an average of 0.025 PPM as safe.&lt;/p&gt;
&lt;h2 id=&quot;illumination&quot;&gt;Visualization&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Illumination&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Now imagine a population of 1.408B Indian people collectively celebrate Diwali by bursting
crackers and fireworks at the same time.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Can’t? Well, that’s why I made this visualization!&lt;/em&gt;&lt;/p&gt;
&lt;h3&gt;India&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Note: These are not fireworks.&lt;/em&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://i.redd.it/mm3r1dyboyx71.gif&quot; alt=&quot;Diwali PM2.5 in India&quot;&gt;
&lt;/p&gt;
&lt;p&gt;&lt;em&gt;But they are.&lt;/em&gt;&lt;/p&gt;
&lt;h3&gt;Delhi&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Here’s one that zooms in on my city.&lt;/em&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://i.redd.it/tixxtw94zwka1.gif&quot; alt=&quot;Diwali PM2.5 in Delhi&quot;&gt;
&lt;/p&gt;
&lt;h2 id=&quot;fading&quot;&gt;Discussion&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Fading&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Data Source&lt;/strong&gt;: OpenAQ API   &lt;strong&gt;Tools&lt;/strong&gt;: Python (pandas, matplotlib)&lt;/p&gt;
&lt;p&gt;What you saw above were the average hourly PM2.5 emissions before and after Diwali (04-11-2021).&lt;/p&gt;
&lt;p&gt;The larger “fireworks” seem to be on the day after Diwali. &lt;em&gt;Go figure.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;I also &lt;a href=&quot;https://www.reddit.com/r/dataisbeautiful/comments/qnyfng/oc_diwali_2021_in_india_pm25_levels_or_fireworks/&quot;&gt;posted the dataviz in &lt;strong&gt;r/dataisbeautiful&lt;/strong&gt;&lt;/a&gt; where I discussed about the inferences
we could make from the visualization.&lt;/p&gt;
&lt;h2 id=&quot;memory&quot;&gt;Footnote&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Memory&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;There have been several initiatives in recent years to encourage eco-friendly Diwali celebrations.
For example, people are now opting for handmade diyas or using electric lights instead of fireworks.
Some communities have even come together to organize Diwali celebrations that do not involve any fireworks at all.&lt;/p&gt;
&lt;p&gt;It’s important to remember that while it’s great to celebrate our traditions and culture, it’s equally
important to be mindful of the impact that our actions have on the environment and the health of others.
By choosing to celebrate Diwali in an eco-friendly and responsible way, we can still enjoy the
festival while also preserving the planet for future generations.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[A look back at my first full time job]]></title><description><![CDATA[I have been working with Visional Inc., based in
Tokyo, Japan, since October 2019. This was my first job after graduation, and I have been…]]></description><link>https://kj7kunal.github.io/blog/2021-visional/</link><guid isPermaLink="false">https://kj7kunal.github.io/blog/2021-visional/</guid><pubDate>Fri, 01 Oct 2021 12:00:00 GMT</pubDate><content:encoded>&lt;p&gt;I have been working with &lt;a href=&quot;https://www.visional.inc/ja/index.html&quot;&gt;Visional Inc.&lt;/a&gt;, based in
Tokyo, Japan, since October 2019. This was my first job after graduation, and I have been
working as an ML/AI Engineer in the AI Team. As it is my &lt;strong&gt;second work anniversary&lt;/strong&gt;, I
think it would be worthwhile to look back and reflect on my journey so far.&lt;/p&gt;
&lt;p&gt;My work mainly involves the development of end-to-end Machine Learning workflows as well as
their deployment in the various services and products offered by the company.&lt;/p&gt;
&lt;p&gt;I also contribute to Machine Learning research undertaken by the AI team, and had initiated
and led the research and development of an OCR system for Japanese handwritten addresses.&lt;/p&gt;
&lt;p&gt;I have limited work proficiency in Japanese, so I prefer to use English as much as possible.
I am able to use English for work within the AI team, but often need to switch to Japanese
for cross-team communication.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Contents&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#company&quot;&gt;About Visional&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#projects&quot;&gt;My Projects&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#p1&quot;&gt;Financial Document OCR System&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#p2&quot;&gt;Research: Japanese handwritten OCR&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#p3&quot;&gt;Employee Slack Analytics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#p4&quot;&gt;HeadHunter Recommendation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#p5&quot;&gt;Security Bug Report Classification&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#experience&quot;&gt;My Experience&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#selected&quot;&gt;How I got selected&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#preparation&quot;&gt;Interview Preparation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#placements&quot;&gt;Placement Interviews&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id=&quot;company&quot;&gt;About Visional&lt;/h2&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; margin: 0 0 30px;&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/e750cc63f74625ca0094f46ebeacaf20/8a8a2/visional_way.jpg&quot; style=&quot;display: block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom: 56.08108108108109%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAALABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAAUDBP/EABYBAQEBAAAAAAAAAAAAAAAAAAACA//aAAwDAQACEAMQAAABpo2mau4Ev//EABkQAAIDAQAAAAAAAAAAAAAAAAABAxITMv/aAAgBAQABBQLVM0LMm6jLM//EABURAQEAAAAAAAAAAAAAAAAAAAAS/9oACAEDAQE/AVP/xAAVEQEBAAAAAAAAAAAAAAAAAAAAEv/aAAgBAgEBPwGlv//EABgQAAMBAQAAAAAAAAAAAAAAAAAQMQER/9oACAEBAAY/ArwqxU//xAAaEAEAAwEBAQAAAAAAAAAAAAABABEhMUFx/9oACAEBAAE/IVCGTVXMNb2uRKvm4xNvrMHfIVx//9oADAMBAAIAAwAAABA87//EABcRAAMBAAAAAAAAAAAAAAAAAAABEVH/2gAIAQMBAT8QqSpGH//EABYRAQEBAAAAAAAAAAAAAAAAABEAAf/aAAgBAgEBPxDdDK//xAAZEAEAAwEBAAAAAAAAAAAAAAABABEhMaH/2gAIAQEAAT8QJitAD6QHU1Acc6kOy2hogMZcaNTrWRtgrbreuwIBT//Z&apos;); background-size: cover; display: block;&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;Visional&quot; title=&quot;&quot; src=&quot;/static/e750cc63f74625ca0094f46ebeacaf20/1c72d/visional_way.jpg&quot; srcset=&quot;/static/e750cc63f74625ca0094f46ebeacaf20/a80bd/visional_way.jpg 148w,
/static/e750cc63f74625ca0094f46ebeacaf20/1c91a/visional_way.jpg 295w,
/static/e750cc63f74625ca0094f46ebeacaf20/1c72d/visional_way.jpg 590w,
/static/e750cc63f74625ca0094f46ebeacaf20/a8a14/visional_way.jpg 885w,
/static/e750cc63f74625ca0094f46ebeacaf20/8a8a2/visional_way.jpg 1008w&quot; sizes=&quot;(max-width: 590px) 100vw, 590px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;&gt;
  &lt;/a&gt;
    &lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;Established in April 2009, BizReach has been operating a variety of Internet services
that support the future of work in Japan with a mission that roughly translates into
“creating a society where everyone can believe in their own potential”. The company,
headquartered in (Shibuya) Tokyo, has regional offices in Osaka, Nagoya, Fukuoka, Shizuoka,
and Hiroshima.&lt;/p&gt;
&lt;p&gt;Visional was born in February 2020, when Bizreach Inc. shifted to a group management
structure. The company’s main focus is primarily in the HR Tech and SaaS business that promote
digital transformation (DX) of industry and support the improvement of productivity in Japan.
The group broadly has a heirarchical structure with the following products and services:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://www.visional.inc/ja/index.html&quot;&gt;Visional Corporation&lt;/a&gt;, the holding company, supports the group management&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://www.bizreach.co.jp/&quot;&gt;Bizreach Corporation&lt;/a&gt;, responsible for the management of the original HR Tech and SaaS businesses&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://www.bizreach.jp/&quot;&gt;Bizreach&lt;/a&gt;, a job-change website that connects companies with human resources with immediate availability&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.careertrek.com/&quot;&gt;CareerTrek&lt;/a&gt;, a job search site for young graduates&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://hrmos.co/&quot;&gt;HRMOS&lt;/a&gt;, a human resources management cloud platform&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://br-campus.jp/&quot;&gt;Campus&lt;/a&gt;, an alumni network service for career consultation&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://www.visional.inc/ja/visional-incubation.html&quot;&gt;Visional Incubation Corporation&lt;/a&gt;, responsible for new business development and acquisitions&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://br-succeed.jp/&quot;&gt;Succeed&lt;/a&gt;, an M&amp;#x26;A platform for business succession&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://yamory.io/&quot;&gt;Yamory&lt;/a&gt;, security vulnerability management cloud for IT systems&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://bizhint.jp/&quot;&gt;BizHint&lt;/a&gt;, a website providing “business tips” in the form of keywords, case studies and solutions&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://binar.jp/&quot;&gt;BINAR&lt;/a&gt;, a career change platform for highly specialized IT Engineers&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://jp.stanby.com/&quot;&gt;StanBy&lt;/a&gt;, a job-search engine jointly owned with Yahoo Japan&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Each of the products have their own specific engineering teams that follow custom Agile
software development life cycles (SDLCs) they are comfortable with.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;AI Team&lt;/strong&gt; is a team of around 20 ML/AI engineers that collaborate with one or more of the
above mentioned teams to understand their business requirements, and leverage data to deploy
AI features in production environments to drive business solutions.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id=&quot;projects&quot;&gt;My Projects&lt;/h2&gt;
&lt;p&gt;As an engineer in the AI team, I have to actively communicate requirements and decisions on
various ML pipelines with multiple service teams simultaneously. I usually have 1-2
service-related projects and an independent research project in a single half-year term.&lt;/p&gt;
&lt;p&gt;From my experience, I believe that the engineers working in the AI Team at Visional switch
frequently between the roles and responsibilities of a &lt;strong&gt;Data Scientist&lt;/strong&gt;, an &lt;strong&gt;ML Engineer&lt;/strong&gt;,
as well as a &lt;strong&gt;Research Scientist&lt;/strong&gt; for different projects undertaken at a time.&lt;/p&gt;
&lt;p&gt;An average ML project includes the following:&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;understanding the requirements of the service teams&lt;/li&gt;
&lt;li&gt;analyzing data to optimize and improve services&lt;/li&gt;
&lt;li&gt;developing custom data models, algorithms and full-stack systems&lt;/li&gt;
&lt;li&gt;collaborating with service teams to deploy the AI features in production&lt;/li&gt;
&lt;li&gt;considering evaluation metrics/systems to quantify/monitor outcomes&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;The projects require a good understanding of ML techniques, algorithms and statistics. Most
of the work also involves Natural Language Processing techniques. I have also been able to
apply my Computer Vision skills to a few projects.&lt;/p&gt;
&lt;p&gt;Following are few of the projects I have worked on so far.&lt;/p&gt;
&lt;h3 id=&quot;p1&quot;&gt;Financial Document OCR System&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://br-succeed.jp/service&quot;&gt;BizReach Succeed&lt;/a&gt; provides an M&amp;#x26;A platform to match companies
looking for succession with potential buyers. As part of their services, they have to analyze
financial documents, which are most often personally handed over as hard-copies by the clients.
These documents are then scanned and the data is manually entered into spreadsheets for
financial analysis typically resulting in business valuations.&lt;/p&gt;
&lt;p&gt;Since Succeed is a relatively small team, the manual data-entry efforts can take upto
&lt;strong&gt;100hrs/month&lt;/strong&gt;. Data-entry basically involves entering tabular data into spreadsheet. This
tabular data is mostly present within PDF scans. Current OCR systems are well equipped to
detect text in an image, however, they fail to preserve spatial information such as table cells.&lt;/p&gt;
&lt;p&gt;I developed an &lt;strong&gt;OCR system&lt;/strong&gt; capable of &lt;strong&gt;preserving tabular structure&lt;/strong&gt; within the image.
The system was built using &lt;a href=&quot;https://pypi.org/project/opencv-python/&quot;&gt;OpenCV&lt;/a&gt; and
&lt;a href=&quot;https://cloud.google.com/vision/docs/ocr&quot;&gt;Google Vision API&lt;/a&gt; in &lt;strong&gt;Python&lt;/strong&gt;, and deployed
using &lt;strong&gt;Docker&lt;/strong&gt;, &lt;strong&gt;Terraform&lt;/strong&gt; and &lt;strong&gt;AWS&lt;/strong&gt;. Some text post-processing was implemented using
a dictionary of Japanese financial terms.&lt;/p&gt;
&lt;p&gt;The pipeline roughly consists of the following steps:&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Parsing OCR request from AWS SQS and AWS S3&lt;/li&gt;
&lt;li&gt;Getting page images from PDF and fixing text orientation&lt;/li&gt;
&lt;li&gt;Table detection and extraction from page image&lt;/li&gt;
&lt;li&gt;OCR Text Detection request to Google Vision API&lt;/li&gt;
&lt;li&gt;Alignment of Google Vision API annotations&lt;/li&gt;
&lt;li&gt;Proofreading result text using a dictionary&lt;/li&gt;
&lt;li&gt;Save output XLS workbook to AWS S3&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; margin: 0 0 30px;&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/a1bcd07a05bdf262fc28224f19e40278/37114/focr.png&quot; style=&quot;display: block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom: 45.27027027027027%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAABYlAAAWJQFJUiTwAAABYklEQVR42k1Sy3KDMAzk/z8uJc0AoecEUoa3bTDYoGrlMVMu2JJ2tVo5cc7ReZx0HAft+06444z/tm3XHTncvfcc2yWW3u90u91oGEeq6orarqUEifM8Bbgsi4DiHSTeg8zRuloyZrnuB4u4M2FRFIRvnCauWSkBeOQObdtKou976R6UOLLbTq/XS3Jaa1JKCynyWZbT8/kUAUopbmgCodaGSQchmrhTGN3LH2rQDAQAzPMsMZCmaUplWQoO0wkhOoOkaRqy1sr55Ng6T6S7jjYGV++35ACwPBbI1W9D3zxynufSGOpRIwqHYRBC+NcxiSwFS+ACazeq68+lDkD4h3z6lfLYGZ8dGVaIZkn0CsXwIfikro1j9EiGsZCHRWgK/zByjBujg0IEAEARwGgSnwf86vvgr+VNo9a5sJSy/JEtx9ehWcil8P/7i38UYqvTNMuTwTluE0t5PB5C6LkeViH3B1lHtdyi/PVPAAAAAElFTkSuQmCC&apos;); background-size: cover; display: block;&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;Financial Document OCR&quot; title=&quot;&quot; src=&quot;/static/a1bcd07a05bdf262fc28224f19e40278/fcda8/focr.png&quot; srcset=&quot;/static/a1bcd07a05bdf262fc28224f19e40278/12f09/focr.png 148w,
/static/a1bcd07a05bdf262fc28224f19e40278/e4a3f/focr.png 295w,
/static/a1bcd07a05bdf262fc28224f19e40278/fcda8/focr.png 590w,
/static/a1bcd07a05bdf262fc28224f19e40278/efc66/focr.png 885w,
/static/a1bcd07a05bdf262fc28224f19e40278/c83ae/focr.png 1180w,
/static/a1bcd07a05bdf262fc28224f19e40278/37114/focr.png 2300w&quot; sizes=&quot;(max-width: 590px) 100vw, 590px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;&gt;
  &lt;/a&gt;
    &lt;/span&gt;
&lt;/p&gt;
&lt;h3 id=&quot;p2&quot;&gt;Research: Japanese handwritten OCR&lt;/h3&gt;
&lt;p&gt;In many ways, hand-filled forms are still a part of Japan’s official processes. For instance,
our company requires candidates to fill out their details for registration. Since it is
imperative that we make no mistakes while entering the details into our systems, this information
is then manually filled by the operator. However, to me, it looked like an opportunity to explore and further research on Japanese
handwritten OCR systems for the AI team.&lt;/p&gt;
&lt;p&gt;The research focusses on the address field in the BizReach registration forms, and aims to
harness the structure present in addresses to explore domain augmentation methods. A
&lt;a href=&quot;http://jusyo.jp/index.html&quot;&gt;list of Japanese addresses&lt;/a&gt; distributed by Japan Post was used
to synthetically generate the train dataset. The textual addresses were transformed into
address images using character images available in the &lt;a href=&quot;http://etlcdb.db.aist.go.jp/&quot;&gt;ETL&lt;/a&gt;
and &lt;a href=&quot;https://www.nist.gov/itl/products-and-services/emnist-dataset&quot;&gt;EMNIST&lt;/a&gt; datasets.
Data augmentation was introduced to account for variances in handwriting:&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;shape of characters&lt;/li&gt;
&lt;li&gt;spacing between characters (narrow or wide)&lt;/li&gt;
&lt;li&gt;rotation of characters (straight or slant)&lt;/li&gt;
&lt;li&gt;size of characters (big or small)&lt;/li&gt;
&lt;li&gt;stroke strength of characters (thick or thin)&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; margin: 0 0 30px;&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/c43fe12e2dd8fa820d1e6f547436e982/cb93d/hocr_sent.png&quot; style=&quot;display: block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom: 10.81081081081081%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAACCAYAAABYBvyLAAAACXBIWXMAABYlAAAWJQFJUiTwAAAAfklEQVR42h2MsQqFIAAA+/+fag60jCi1SJEoCmtoaAiDezynu+Wu2Pedvu+Z5xlrLVprtm1jWRamaUIIgVIK7z12HJF1zTAMuYkxZoYQ+L6PlBLF8zyc58l93xzHkYd/rutK13WUZYmUkrZtMcZQVRVN02S/riu7c473ffPwB3Q1kAXeRNNMAAAAAElFTkSuQmCC&apos;); background-size: cover; display: block;&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;Sentence Image&quot; title=&quot;&quot; src=&quot;/static/c43fe12e2dd8fa820d1e6f547436e982/fcda8/hocr_sent.png&quot; srcset=&quot;/static/c43fe12e2dd8fa820d1e6f547436e982/12f09/hocr_sent.png 148w,
/static/c43fe12e2dd8fa820d1e6f547436e982/e4a3f/hocr_sent.png 295w,
/static/c43fe12e2dd8fa820d1e6f547436e982/fcda8/hocr_sent.png 590w,
/static/c43fe12e2dd8fa820d1e6f547436e982/efc66/hocr_sent.png 885w,
/static/c43fe12e2dd8fa820d1e6f547436e982/c83ae/hocr_sent.png 1180w,
/static/c43fe12e2dd8fa820d1e6f547436e982/cb93d/hocr_sent.png 2304w&quot; sizes=&quot;(max-width: 590px) 100vw, 590px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;&gt;
  &lt;/a&gt;
    &lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;The &lt;a href=&quot;https://arxiv.org/abs/1507.05717&quot;&gt;&lt;strong&gt;Convolutional Recurrent Neural Network&lt;/strong&gt; (CRNN)&lt;/a&gt;
architecture was chosen. The end-to-end model, that combines CNN (image feature extraction),
RNN (sequence recogntion) and CTC loss, was implemented in &lt;strong&gt;PyTorch&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The lexicon-free model was able to achieve a &lt;strong&gt;44% lower CER (character error rate) than the
&lt;a href=&quot;https://cloud.google.com/vision/docs/handwriting&quot;&gt;Google Vision API&lt;/a&gt;&lt;/strong&gt; on a test set of actual
handwritten Japanese addresses. Currently, I am trying to improve the CER further using
lexicon-based algorithms, and simultaneously documenting the methodology and results aimed at a
future publication.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; margin: 0 0 30px;&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/7b7d457b3595fa4f7980ade8951acec4/f5aa5/hocr_cer.png&quot; style=&quot;display: block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom: 14.18918918918919%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAADCAYAAACTWi8uAAAACXBIWXMAABYlAAAWJQFJUiTwAAAAiklEQVR42oWOzQrEIAyE+/7vpwfpQVS0liq2VfojMksCu9cNDHGS8dMphICUEtZ15S6EwDzPWJYF1lporVmlFOScsW0baq3oveN5np/I024iUIyRgQSRUsIYg73sPKMQPUq71hqu62KNMfC+L8O+Z5pP3nv+AV1wzkEpxf48T9z3zSECHcfB/l99AEJX5EX8ubeuAAAAAElFTkSuQmCC&apos;); background-size: cover; display: block;&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;CRNN Result CER&quot; title=&quot;&quot; src=&quot;/static/7b7d457b3595fa4f7980ade8951acec4/fcda8/hocr_cer.png&quot; srcset=&quot;/static/7b7d457b3595fa4f7980ade8951acec4/12f09/hocr_cer.png 148w,
/static/7b7d457b3595fa4f7980ade8951acec4/e4a3f/hocr_cer.png 295w,
/static/7b7d457b3595fa4f7980ade8951acec4/fcda8/hocr_cer.png 590w,
/static/7b7d457b3595fa4f7980ade8951acec4/efc66/hocr_cer.png 885w,
/static/7b7d457b3595fa4f7980ade8951acec4/c83ae/hocr_cer.png 1180w,
/static/7b7d457b3595fa4f7980ade8951acec4/f5aa5/hocr_cer.png 2312w&quot; sizes=&quot;(max-width: 590px) 100vw, 590px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;&gt;
  &lt;/a&gt;
    &lt;/span&gt;
&lt;/p&gt;
&lt;h3 id=&quot;p3&quot;&gt;Employee Slack Analytics&lt;/h3&gt;
&lt;p&gt;The &lt;a href=&quot;https://hrmos.co/&quot;&gt;HRMOS&lt;/a&gt; team develops a variety of HR-Tech services, such as employee
databases, attendance logging and performance tracking systems. Their focus is the improvement
of productivity within client business teams.&lt;/p&gt;
&lt;p&gt;Each company has data on their employees which can be leveraged using AI to measure, in an
abstract sense, the productivity of the employees. This data is usually available within the
employee database of the company and has a well-defined structure to it. In recent years,
Slack, Workplace, Teams, etc have gained preference over emails, especially in the tech
industry.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Public conversations&lt;/strong&gt; can be viewed as unconventional sources of data, which can
be processed using NLP techniques to quantify productivity. For instance, messages and
replies can be considered entities that signal a connection link between two employees.
Frequency of interactions on public channels can signal the degree of involvement of a
certain employee in a certain topic.&lt;/p&gt;
&lt;p&gt;I built a simple GUI in &lt;strong&gt;Vue.js&lt;/strong&gt; and &lt;strong&gt;Python&lt;/strong&gt;, and deployed it using &lt;strong&gt;Docker&lt;/strong&gt; for
internal HR to visualize activity and messaging trends on the Visional Slack workspace.
&lt;strong&gt;Japanese text processing&lt;/strong&gt; using &lt;a href=&quot;https://taku910.github.io/mecab/&quot;&gt;MeCab&lt;/a&gt; tagger and
&lt;a href=&quot;https://github.com/neologd/mecab-ipadic-neologd#overview&quot;&gt;NEologd&lt;/a&gt; dictionary were used
to clean the unstructured conversational data. The following are some of the features have
been implemented in the GUI:&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Conversation topic transitions, to visualize the change over time in underlying topics of conversation, discovered using &lt;strong&gt;LDA topic modeling&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Interactive User/Channel wordclouds, to visualize a summarized history of conversation keywords&lt;/li&gt;
&lt;li&gt;Matching similar users, based on high topic probabilities in conversations&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;p4&quot;&gt;HeadHunter Recommendation&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://www.bizreach.jp/&quot;&gt;BizReach&lt;/a&gt;, Visional’s primary service, is a job-change website that
connects individuals with recruiters and companies through job listings. Other than job search
results, the website’s homepage also recommends job listings to the candidates on the basis of
their preferences, past activity and searches. From point of view of the recruiters, BizReach
aims to recommend candidates that are most likely to be scouted by them.&lt;/p&gt;
&lt;p&gt;The HeadHunter recommendation feature was proposed by the AI team, with the aim to increase
scouting rate for headhunters. The feature was developed to assist existing recommendation
systems by using &lt;strong&gt;implicit feedback&lt;/strong&gt; datasets like candidate-job access log data for newly
registered candidates, for whom there is insufficient scout data. The same data could be
used to recommend candidates to recruiters who had posted the jobs.&lt;/p&gt;
&lt;p&gt;I was involved in the assessment of the effectiveness of using access log data for the problem
and comparing the performance of various recommendation models in offline testing.
A 25% hit rate was obtained using &lt;a href=&quot;https://implicit.readthedocs.io/en/latest/quickstart.html&quot;&gt;Implicit&lt;/a&gt;’s
&lt;a href=&quot;https://implicit.readthedocs.io/en/latest/als.html&quot;&gt;Alternating Least Squares&lt;/a&gt; &lt;strong&gt;collaborative
filtering&lt;/strong&gt; model for demonstration of the proposed project.&lt;/p&gt;
&lt;h3 id=&quot;p5&quot;&gt;Security Bug Report Classification&lt;/h3&gt;
&lt;p&gt;Publicly disclosed cybersecurity vulnerabilities are assigned a
&lt;a href=&quot;https://www.cve.org/About/Overview&quot;&gt;Common Vulnerabilities and Exposures (CVE)&lt;/a&gt; ID, and
the &lt;a href=&quot;https://nvd.nist.gov/&quot;&gt;National Vulnerability Database&lt;/a&gt; performs analysis on CVEs by
aggregating data points from the description, references supplied and any supplemental
data that can be found publicly at that time.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://yamory.io/&quot;&gt;Yamory&lt;/a&gt; develops a security vulnerability management cloud for IT systems.
The vulnerability database used by yamory consists of information analyzed and evaluated manually
by a dedicated security analyst. To build this database, analysts have to scour multiple sources
like tweets, bug report websites, security articles, etc.&lt;/p&gt;
&lt;p&gt;We developed a system to scrape data from such sources and classify them as security vulnerability
reports. Tweets mentioning a CVE ID were scraped and stored and analyzed using &lt;strong&gt;Elasticsearch&lt;/strong&gt;
whereas &lt;strong&gt;Scrapy&lt;/strong&gt; was used to scrape bug reports from active bug reporting websites.
&lt;strong&gt;Named Entity Recognition&lt;/strong&gt; was used to identify software names within the descriptions.
A 0.81 F-score was achieved in classifying bug reports using a &lt;strong&gt;bigram language model&lt;/strong&gt;
trained over NVD/CVE Descriptions as ground truth.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id=&quot;experience&quot;&gt;My Experience&lt;/h2&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; margin: 0 0 30px;&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/16675411408cd4a3f027fde157aa25e6/c655d/first_day.png&quot; style=&quot;display: block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom: 66.89189189189189%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAYAAACpUE5eAAAACXBIWXMAABYlAAAWJQFJUiTwAAADq0lEQVR42g3Sa0+TBwDF8X6C7cX2YurmTCVYpG2whQKlCh20QtrHrpZLlKI8tJSCFqhuAs4bFJApKIIwxAugIkNrBYPIRKwXZINKMVECYubi4mUmy77Cf88X+J2TkyNbCjWyON7L8oPrPL83wtJ8lMWnYeYmQizNPubNwjSfVhb4uPKSV3/8xqunI7yNjPF2/i6v58ZZWYry+t07Zmbvc2+0F1mvJ4kOj5ngMTfhjkMs3B5k+tc+Hg/8woP+Lv56MsF7CXodHuPh1QssTNzg/s0h5qXQ6IsIs8+mefIswpwEPo+GkVVuVXBxv51bfiMvu3JZak3nqqghXJvJ4bwUHjV+z7+TAZZDTXy44eN5Xyl7dgmMBq8QnhnlwcM7DEht5/57w/tP/yBrcsRxyr2Z6XYny1dKeXmphOCPViInBNp92wi32Pg47OLv0Qb+vOHnUXcxe11G7gT7GAt1MTVyiVBkkNDvw0QXI8jqxVW0FKsJNQo86XbR4xc457cxdFhgqLmca9IU0d5KplrdXA+Uc6veTneNhYnhDoYb3Iy0VjF53kdbtYXun8sk8EACpwNbGO4poLvWQVmumbKdBRypdtFSv5/zzQ38VO3n7A/FNFa5qRNzaa92MHjeT9Mekca6UvonjzHwqJmGTjMyT9BKzVwJ+0JONJr1xCg2kqXT0n90F8I2PeWe/Rz07KCqyMr2nCwqnXl0HqlAyPmO2HUxJGjk3IkUMf/By+3FXGT5/Sbs5wxkHNpIvOVr1qz5Cl9+Nt3HjpCsS8Zic5CdZZTCEjGkJiNut7DdYkW+Xs3aNXI0STE0Xc7g7JiJtluZyLIDKsxHFZjqNhC3aR1auZwUrRpvochWCTLptdSUOCRMgzEpAaVCzWeff8m3chV5ZhPJig2Y3Qp8F/SIp7TIhPp47MdVGHbHkmaNJX1HLAazlnJ7FrbMRNS6GLxOO6naJNbGrkalM6BNSMFsSEGhVJFql+M4EEfF2RTEkxKY15JAYbuW3V06yi6nUTGkxyamoTeoWa38AmORAUOmkfgNKjKcCuLNqxD0yVRY0lGbvsHVpsTbmUip1K74xCZkFX0GaoOZBMZzaJ2UvjclkOvcws69WRQc3Ey+z4bgEhD2KRE7Eqk6pyf/kBKbT5qqJA7X6U24O7WIbVKxpo3Ijt8V6J3J51q0kNCLIsaWRc6MF9I2uZtLK17OPPNSe1Ok500ZJ+cKCExZOXrXTM2Ikb0Daey5mIanJxWxU8fO4yr+B5bgbB5DkNAwAAAAAElFTkSuQmCC&apos;); background-size: cover; display: block;&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;Placement Meme&quot; title=&quot;&quot; src=&quot;/static/16675411408cd4a3f027fde157aa25e6/fcda8/first_day.png&quot; srcset=&quot;/static/16675411408cd4a3f027fde157aa25e6/12f09/first_day.png 148w,
/static/16675411408cd4a3f027fde157aa25e6/e4a3f/first_day.png 295w,
/static/16675411408cd4a3f027fde157aa25e6/fcda8/first_day.png 590w,
/static/16675411408cd4a3f027fde157aa25e6/efc66/first_day.png 885w,
/static/16675411408cd4a3f027fde157aa25e6/c83ae/first_day.png 1180w,
/static/16675411408cd4a3f027fde157aa25e6/c655d/first_day.png 1586w&quot; sizes=&quot;(max-width: 590px) 100vw, 590px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;&gt;
  &lt;/a&gt;
    &lt;/span&gt;
&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id=&quot;selected&quot;&gt;How I got selected&lt;/h2&gt;
&lt;h3 id=&quot;preparation&quot;&gt;Interview Preparation&lt;/h3&gt;
&lt;p&gt;After &lt;a href=&quot;/blog/2018-ntu-intern&quot;&gt;my internship with ROSE Lab&lt;/a&gt; in the summer of 2018,
I immediately started to prepare for job interviews in the
&lt;a href=&quot;https://www.shiksha.com/b-tech/articles/iit-kharagpur-placements-blogId-20045#:~:text=placements%202018%20below.-,IIT%20Kharagpur%20placements,-are%20conducted%20for&quot;&gt;IIT Kharagpur Placement Season 2018&lt;/a&gt;.
Placements are a time of extensive planning and preparation, and depending on their
targeted companies, students choose from various preparation strategies to get through.&lt;/p&gt;
&lt;p&gt;Fresh from a very interesting internship, I was determined to pursue a Machine Learning
/ Software Engineering role, even though my major in Aerospace Engineering put me at a
fair disadvantage compared to students of “circuit” (CS, EE, MA, etc) branches. In a batch
of over 1000 applicants, companies had to apply such constraints in order to make the
shortlist really “short”. This meant, that I would not be able to sit for the tests for
these companies, even though I was fairly confident in my skills. On top of this, the rules
restricted us from accepting more than one job offer in the entire season.&lt;/p&gt;
&lt;p&gt;Keeping in mind the above constraints, I decided to keep my preparation very general and
open to multiple opportunities. From information gathered from seniors as well as
company introductory talks, I understood that most of the tests involved similar formats.
On the basis of this information, my preparation included brushing up on:&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Analytical/Probability skills: &lt;a href=&quot;https://mbapreponline.files.wordpress.com/2013/07/fifty_challenging_problems_in__2.pdf&quot;&gt;Fifty Challenging Problems in Probability with Solutions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Machine Learning theory: my notes from online courses on Coursera such as the &lt;a href=&quot;https://www.coursera.org/learn/machine-learning&quot;&gt;Machine Learning course (Stanford University)&lt;/a&gt; and the &lt;a href=&quot;https://www.coursera.org/specializations/deep-learning&quot;&gt;Deep Learning Specialization (DeepLearning.AI)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Data Structures/Algorithms: Practice questions on &lt;a href=&quot;https://www.interviewbit.com/&quot;&gt;InterviewBit&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;This way, I was able to prepare fairly well for both software and data science roles.&lt;/p&gt;
&lt;p&gt;Parallely, I drafted a one-page resume showcasing my projects and my internships, and got
it reviewed by a lot of friends and seniors. After a few revisions, I submitted it to the
placement portal, and waited anxiously for the placement process to start.&lt;/p&gt;
&lt;h3 id=&quot;placements&quot;&gt;Placement Interviews&lt;/h3&gt;
&lt;p&gt;Reality hit hard on Day 1, when I found out some companies I was hoping to be shortlisted
in had actually used the “circuit branch” constraint, and I was only able to sit in 2 of
the 6 companies I applied for that day. My first interview went well, but I eventually
apologized and pulled out, since their Embedded Software Engineer role was not
something I was very keen about. I was not able to perform well my second interview, for an
analyst position, since I had not prepared for the “Case Interview” style at all. In summary,
Day 1 was a complete bust because of the restrictions on the placement process, and I was
disappointed with the limited options I had.&lt;/p&gt;
&lt;p&gt;Later that night, however, I was happy to find out that I cleared the shortlist for
BizReach Inc., based in Tokyo, for the Machine Learning Engineer role. Day 2 is usually
the arena for companies that are new participants in the placements, and sometimes these
opportunities are on-par or even better than the previous day. BizReach was one such
company, and the job description was exactly what I had hoped for!&lt;/p&gt;
&lt;p&gt;I called up a senior who was working in Mercari, Tokyo (I had also called him up the
previous night, since I was furious Mercari was only looking for circuit branch students),
and asked him some tips for interviewing with Japanese people. His input really helped
me refocus my mind on this new opportunity. After some modest revision and self pep-talk,
I called it a night.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; margin: 0 0 30px;&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/b8282d589667f72957dc389589b5e6aa/cb1ac/thanos_meme.png&quot; style=&quot;display: block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom: 39.86486486486486%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAYAAAD5nd/tAAAACXBIWXMAABYlAAAWJQFJUiTwAAACUUlEQVR42hWQ20uTARjGPw3Mw2zqNnUnv0/nNrZ0zdw8sAynzgOeymQz1DLzxrSJZaJ5qFwlGQWriwwxQbAgEAokdTqXFgUF0WVg/g/d2U39+rp63wfeh+f3vMLh4W8iO7tsRmNsbm8TiUaJffhIdHePtY1NtmO7/Pnzl/2DA9791+/3WI9ssxXdkX0xNrZ2ZN8Ob9fW+bH/E+HrSpjI/H3ehO8yP3ObZ/dmeBG6yVJogud3xnn58Bbri08Ih6aYHb/O8tykfHeDuclRwlMjLMh7eGqY8MQQi6FRhKVmA08bJGZrnQQsOfi0CvwmDT1mFQFTBv1OLcGTOpokBa0mJWOVBvrcWTjVR3Eo4qnKjqc8Mx6PKo6z+iMI3ydcrHYVEG4ro9NpocqYRofdyDV3LsNl+Ux67TxqLODVgJfFCy665IDWYokhn41aiwaLOpHC7FScmal02lQIv1Y8bFzMY6G9iF6XA68hQybJYrhYz3SFyON6K8v+41z1SHQWGekoM1Nj19EgJdNmT6POqsSlS6ZQlYDfrET4NpbH56CZ1+dtDJY7qJGy8Ikq6sV0mq0qHjRaifYX0OvW4xXVBEotXK62014iUiDTVRqS8eakUKxJoFE8Jv+wJYsvQYn1nlyuuESqZMJT2hSKVImUaJOozldy6UQG1dpEREGgzqjEXyrRXaLHpEhAnxRHbkq8PAU8+mSE1e4cYoM5fJq2MtKUT+dpK+dKDAQqjATP5NHnM1Bv1cghaZSKSgIuHQP1FkabzbS4tXhsaorkJlJ2Oq0OLf8ACkA/aolAnRwAAAAASUVORK5CYII=&apos;); background-size: cover; display: block;&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;Placement Meme&quot; title=&quot;&quot; src=&quot;/static/b8282d589667f72957dc389589b5e6aa/fcda8/thanos_meme.png&quot; srcset=&quot;/static/b8282d589667f72957dc389589b5e6aa/12f09/thanos_meme.png 148w,
/static/b8282d589667f72957dc389589b5e6aa/e4a3f/thanos_meme.png 295w,
/static/b8282d589667f72957dc389589b5e6aa/fcda8/thanos_meme.png 590w,
/static/b8282d589667f72957dc389589b5e6aa/efc66/thanos_meme.png 885w,
/static/b8282d589667f72957dc389589b5e6aa/c83ae/thanos_meme.png 1180w,
/static/b8282d589667f72957dc389589b5e6aa/cb1ac/thanos_meme.png 1936w&quot; sizes=&quot;(max-width: 590px) 100vw, 590px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;&gt;
  &lt;/a&gt;
    &lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;The next day, when I showed up for the interviews, and I was surprised to see only a few
candidates lined up. I had a quick judgmental glance at the candidates, and narcissitically
reassured myself. I knew I was a slight bit overconfident, but in such a situation, it
really helped me to not ponder on my nerves. I ran simulations of my plan in my head while
I waited.&lt;/p&gt;
&lt;p&gt;I was already impressed at how punctual the Japanese were at taking interviews. Each
interview went on for a total maximum of 30-minutes, with two exact 15-minute segments.
The first part was basically a 3-on-1 technical interview, involving a CV runthrough
and some behavioral questions, whereas the second part was a coding round.&lt;/p&gt;
&lt;p&gt;My interview began at 1830hrs, and I went inside and greeted them, saying “Konnichiwa”
(hello, in Japanese). They were delighted, and started talking in Japanese to test/tease me.
I told them liked watching anime with subtitles, and I liked to pick up a few words here
and there. I was happy with how the mood was from that point - light and cheery. They
introduced themselves as Takeuchi-san (CTO), Ikawa-san (VP of Engineering) and Dat-san
(my future team lead).&lt;/p&gt;
&lt;p&gt;In the first part, I was mainly asked in detail about my thesis project, with some questions
about the image segmentation pipeline, and also my ROSE Lab internship. Other than
well-prepared answers to possible questions to my resume, I was quick to emphasize on my
experience working in an Asian working environment as well as my knowledge about Japan
during the behavioral questions. In general, I felt our interaction was positive and I was
optimistic when I began the coding part. There were two questions of easy/medium difficulty,
involving array manipulation and 2-pointers. I was easily able to work through them and
write clean code in Python, which was the programming language I was most comfortable with
as well as the language with which I was expected to work with if I were selected.&lt;/p&gt;
&lt;p&gt;I left at exactly 1900hrs after asking a few more questions I had about the company and the
role, and was told to wait outside for about an hour. At 2000hrs, all 3 interviewers came
outside, and announced the three names they selected - and I was so relieved when I heard mine!
The interviewers left promptly after taking some pictures with us outside the venue. I called
up my parents and my friends and shared the news. It was one of the happiest nights of my
life, and I did no injustice to celebrating that feeling! It was the result of hard work
and meticulous planning, and most of all - being my genuine self.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Summer Internship at NTU Singapore]]></title><description><![CDATA[Having worked on 3 ML projects in my fourth year, I really wanted to take up the
challenge of collaborating within a distinguished research…]]></description><link>https://kj7kunal.github.io/blog/2018-ntu-intern/</link><guid isPermaLink="false">https://kj7kunal.github.io/blog/2018-ntu-intern/</guid><pubDate>Tue, 01 Sep 2020 12:00:00 GMT</pubDate><content:encoded>&lt;p&gt;Having worked on 3 ML projects in my fourth year, I really wanted to take up the
challenge of collaborating within a distinguished research group, preferably abroad.
A foreign training or FT, as per my university lingo, was considered a prestigious
opportunity for multifaceted exposure, as not only would you gain indispensable
experience working in an established institute set in an unfamiliar environment,
it would also be really thrilling to interact with people from different cultural
backgrounds with similar interests to freely exchange ideas and experiences.&lt;/p&gt;
&lt;p&gt;Out of around 30 research labs that I applied to all over the world, I received
3 offers. I was most excited to work at the
&lt;a href=&quot;https://www.ntu.edu.sg/rose&quot;&gt;Rapid-Rich Object Search (ROSE) Lab&lt;/a&gt; at the
&lt;a href=&quot;https://www.ntu.edu.sg/index&quot;&gt;Nanyang Technological University, Singapore&lt;/a&gt;,
as not only did the lab’s research focus align with my interests, I would also be
able to gain work experience in &lt;a href=&quot;https://www.topuniversities.com/university-rankings/asian-university-rankings/2018&quot;&gt;one of the top universities in Asia&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Contents&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;#selected&quot;&gt;How I got selected&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#resume&quot;&gt;Resume&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#application&quot;&gt;Application Process&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#interview&quot;&gt;Interview Call&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;#lab&quot;&gt;The Lab&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;#project&quot;&gt;The Project&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#objectives&quot;&gt;Objectives&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#motivation&quot;&gt;Motivation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#implementation&quot;&gt;Implementation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#result&quot;&gt;Result&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;#experience&quot;&gt;My Experience&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id=&quot;selected&quot;&gt;How I got selected&lt;/h2&gt;
&lt;h3 id=&quot;resume&quot;&gt;Resume&lt;/h3&gt;
&lt;p&gt;In my fourth year, I started to diverge my career away from aerospace engineering,
to focus on my minor in Computer Science, especially in the areas of
Machine Learning and Artificial Intelligence. As semester projects, I was able
to work on some interesting applications of Machine Learning:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Mental Workload Estimation&lt;/strong&gt; &lt;a href=&quot;https://github.com/kj7kunal/ML_Project&quot;&gt;[Github]&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;As part of the Machine Learning course (CS60050), I trained various ML models to estimate mental workload of a subject playing the N-Back game while wearing an Emotiv BCI headset. The project included feature engineering using signal processing techniques and resulted in a comparitive study between various ML models such as Decision Trees, Random Forests and Neural Nets.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Artistic Image Rendering&lt;/strong&gt; &lt;a href=&quot;https://github.com/kj7kunal/Artistic_Image_Rendering&quot;&gt;[Github]&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;As part of the Deep Learning course (CS60048), I decided to implement neural image style transfer as described in the paper &lt;a href=&quot;https://arxiv.org/pdf/1508.06576v2.pdf&quot;&gt;&lt;em&gt;A Neural Algorithm of Artistic Style by Gatys et al&lt;/em&gt;&lt;/a&gt;. This gave me a good understanding of how CNNs propagate signals, and how we can use intermediate feature maps to extract representations of textures and color schemes from the lower layers, and of structure from the deeper layers. I also learned about designing custom loss functions for the given optimization problem.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Apart from curriculum coursework, I also independently took up online courses on
Coursera such as the &lt;a href=&quot;https://www.coursera.org/learn/machine-learning&quot;&gt;Machine Learning course (Stanford University)&lt;/a&gt;
and the &lt;a href=&quot;https://www.coursera.org/specializations/deep-learning&quot;&gt;Deep Learning Specialization (DeepLearning.AI)&lt;/a&gt;
taught by Prof. Andrew Ng, to improve my knowledge of basic ML/DL techniques
through theory and coding assignments.&lt;/p&gt;
&lt;h3 id=&quot;application&quot;&gt;Application Process&lt;/h3&gt;
&lt;p&gt;The application process was pretty straightforward. Since I was mainly focussed on
getting research experience, I had compiled a list of around 30-40 research labs
that I wanted to work with and emailed their respective POCs.
However, most of the replies were negative, partly because I applied very late
in the year and partly because my background in aerospace engineering put me at
a disadvantage in a cohort of CS major students, which I feel was the case in my
applications to research internship programs like DAAD and MITACS.&lt;/p&gt;
&lt;p&gt;Ultimately, I heard back from 3 places, one of which was ROSE Lab, a joint collaboration
between Nanyang Technological University, Singapore, and Peking University, China.
I was very surprised to have received a positive reply from them, as it was in the
top 10 of my list, which was basically an ambitious tier for me.&lt;/p&gt;
&lt;p&gt;After exchanging a few emails, we scheduled an hour-long interview call a week later,
and I was asked to prepare a short presentation about 1~2 of my best projects.&lt;/p&gt;
&lt;h3 id=&quot;interview&quot;&gt;Interview Call&lt;/h3&gt;
&lt;p&gt;I sent my presentation 2 days in advance of the interview call. On the Skype call, I
connected with the Deputy Director, &lt;a href=&quot;https://sg.linkedin.com/in/dennissng&quot;&gt;Dr Dennis Sng&lt;/a&gt;,
who oversaw projects undertaken by the ROSE Lab, &lt;a href=&quot;https://warwick.ac.uk/fac/sci/dcs/people/xuuldl/&quot;&gt;Dr Lin Shan&lt;/a&gt;,
a research fellow, who led the project I was supposed to work on, and of course
the lab’s POC, Ms Wang Qian, with whom I was so far in touch with.&lt;/p&gt;
&lt;p&gt;The interview call went on quite smoothly, and was structured mainly around my
presentation. I was able to explain my contributions to my projects and answered
related questions to the best of my knowledge. I was also questioned about my major,
and why I was interested in Machine Learning, which I was also prepared to answer.&lt;/p&gt;
&lt;p&gt;I got the acceptance mail a week after the interview call, and I was super excited
to start the visa process!&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id=&quot;lab&quot;&gt;The Lab&lt;/h2&gt;
The [Rapid-Rich Object Search (ROSE) Lab](https://www.ntu.edu.sg/rose) is a joint
collaboration between NTU Singapore and Peking University, China. Its vision is to
create the largest collection of structured domain object database in Asia, and to
further image-based object search applications.
&lt;p&gt;The lab conducts research in the areas of computer vision, image processing, and
pattern recognition. It aims to develop scalable and robust mobile object search
services/applications involving areas of research like object recognition &amp;#x26; retrieval,
deep learning &amp;#x26; video analytics and multimedia forensics &amp;#x26; biometrics. They have
taken the initiative of creating various &lt;a href=&quot;https://www.ntu.edu.sg/rose/research-focus/datasets&quot;&gt;publicly-shared databases&lt;/a&gt;
to further research that they believe might create huge economic value and
opportunities in the future.&lt;/p&gt;
&lt;p&gt;Projects undertaken by the lab were usually overseen by Dr Dennis Sng (Deputy Director
&amp;#x26; Principal Scientist​), and advised by &lt;a href=&quot;https://dr.ntu.edu.sg/cris/rp/rp00653&quot;&gt;Prof Alex C. Kot&lt;/a&gt; (Director).
The lab involves a workforce both from academia and industry, and hosts events
pertaining to the dissemination of knowledge on vision-based AI, particularly in
object search technology.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; margin: 0 0 30px;&quot;&gt;
      &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom: 53.37837837837838%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAALABQDASIAAhEBAxEB/8QAGAAAAgMAAAAAAAAAAAAAAAAAAAQCAwX/xAAWAQEBAQAAAAAAAAAAAAAAAAABAAL/2gAMAwEAAhADEAAAAa2lWAziRZ//xAAbEAACAQUAAAAAAAAAAAAAAAABAgADEhQhMv/aAAgBAQABBQKmtxxlhXawsY/X/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPwE//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPwE//8QAGBAAAgMAAAAAAAAAAAAAAAAAAAERIDH/2gAIAQEABj8Cg2jP/8QAGxABAQACAwEAAAAAAAAAAAAAAQARITFRYaH/2gAIAQEAAT8hEVKZ6e7jEZ4lskG2+6//2gAMAwEAAgADAAAAEM//AP/EABcRAAMBAAAAAAAAAAAAAAAAAAEQEUH/2gAIAQMBAT8QE1f/xAAVEQEBAAAAAAAAAAAAAAAAAAABEP/aAAgBAgEBPxBn/8QAGxABAQACAwEAAAAAAAAAAAAAAREAITGBkWH/2gAIAQEAAT8QVmDVevuBWtKmvjB6Gk2YwxyONtthYX3E97P/2Q==&apos;); background-size: cover; display: block;&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;ROSE Lab&quot; title=&quot;&quot; src=&quot;/static/f05d908a55236f3fa8c430978c4bb1f4/1c72d/ROSELabDoor.jpg&quot; srcset=&quot;/static/f05d908a55236f3fa8c430978c4bb1f4/a80bd/ROSELabDoor.jpg 148w,
/static/f05d908a55236f3fa8c430978c4bb1f4/1c91a/ROSELabDoor.jpg 295w,
/static/f05d908a55236f3fa8c430978c4bb1f4/1c72d/ROSELabDoor.jpg 590w,
/static/f05d908a55236f3fa8c430978c4bb1f4/a8a14/ROSELabDoor.jpg 885w,
/static/f05d908a55236f3fa8c430978c4bb1f4/fbd2c/ROSELabDoor.jpg 1180w,
/static/f05d908a55236f3fa8c430978c4bb1f4/b536e/ROSELabDoor.jpg 1454w&quot; sizes=&quot;(max-width: 590px) 100vw, 590px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;&gt;
    &lt;/span&gt;
&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id=&quot;project&quot;&gt;The Project&lt;/h2&gt;
&lt;h3 id=&quot;objectives&quot;&gt;Objectives&lt;/h3&gt;
&lt;p&gt;The project was a collaboration between ROSE Lab and the
&lt;a href=&quot;https://www.dsta.gov.sg/home&quot;&gt;Defence Science and Technology Agency (DSTA) Singapore&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The goal was to build a new dataset for Person Re-Identification (Person Re-ID) with
the aim of simulating the real world application domain as much as possible.
The plan involved capturing data with:&lt;/p&gt;
&lt;p&gt;Scene Invariance&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Wide range of locations covered using outdoor public surveillance cameras&lt;/li&gt;
&lt;li&gt;Various times of the day (morning/afternoon/evening)&lt;/li&gt;
&lt;li&gt;Different weather conditions (sunny/cloudy/rainy)&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;Clothing Invariance&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;“Actors” advised to wear various types of clothing during data collection&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;My responsibilities in the project were to create tools to automate extraction and
annotation of target “actor” images from surveillance video frames, to populate the
initial test dataset, which would later be benchmarked by existing Person Re-ID models.
I worked closely with &lt;a href=&quot;https://warwick.ac.uk/fac/sci/dcs/people/xuuldl/&quot;&gt;Dr Lin Shan&lt;/a&gt;,
a PhD student who was leading the Re-ID project, as part of a two year secondment
under EU IDENTITY project between University of Warwick and NTU Singapore.&lt;/p&gt;
&lt;h3 id=&quot;motivation&quot;&gt;Motivation&lt;/h3&gt;
&lt;p&gt;Person Re-ID is defined as the problem of matching people across disjoint camera views
in a multi-camera system. It is an important task in the field of intelligent security.
A Re-ID system should be able to keep track of subjects (who are on a certain “watch-list”)
in surveillance videos of multiple probable locations of re-appearance.&lt;/p&gt;
&lt;p&gt;It is an extremely challenging task due to a plethora of reasons:&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Pose/viewing angle difference&lt;/li&gt;
&lt;li&gt;Low Resolution CCTV footage&lt;/li&gt;
&lt;li&gt;Crowded Areas&lt;/li&gt;
&lt;li&gt;Occlusions&lt;/li&gt;
&lt;li&gt;Algorithm inadequacies (Detection errors / Real-time requirement)&lt;/li&gt;
&lt;li&gt;Unlimited/Open dataset task (Infinite number of classes for a classification problem)&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 350px; margin: 0 0 30px;&quot;&gt;
      &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom: 64.86486486486486%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAANABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAAMBBf/EABUBAQEAAAAAAAAAAAAAAAAAAAEC/9oADAMBAAIQAxAAAAHsJ5BdBT//xAAcEAEAAgEFAAAAAAAAAAAAAAABAAIDERITISL/2gAIAQEAAQUC3aWVghWz7e5xUyT/xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/AT//xAAWEQADAAAAAAAAAAAAAAAAAAABEBH/2gAIAQIBAT8BNX//xAAbEAACAQUAAAAAAAAAAAAAAAAAASECEBFBYf/aAAgBAQAGPwJ7OMU2mTNSP//EABsQAQACAgMAAAAAAAAAAAAAAAEAESFBMVGB/9oACAEBAAE/IREbu4rUZY1A4QY2xvM4CQTQwV1P/9oADAMBAAIAAwAAABCr7//EABURAQEAAAAAAAAAAAAAAAAAAAEA/9oACAEDAQE/EFi//8QAFhEAAwAAAAAAAAAAAAAAAAAAAAEx/9oACAECAQE/EHgUP//EAB0QAQADAAIDAQAAAAAAAAAAAAEAETEhQXGBsaH/2gAIAQEAAT8QUAGR01n4xuvCgvHzHR+ABr3DGgMlG2dyqBQbL4+QzydxKGfZ/9k=&apos;); background-size: cover; display: block;&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;ReID system&quot; title=&quot;&quot; src=&quot;/static/b20075882272da83374658ea9f71803a/70ebb/reid_system.jpg&quot; srcset=&quot;/static/b20075882272da83374658ea9f71803a/a80bd/reid_system.jpg 148w,
/static/b20075882272da83374658ea9f71803a/1c91a/reid_system.jpg 295w,
/static/b20075882272da83374658ea9f71803a/70ebb/reid_system.jpg 350w&quot; sizes=&quot;(max-width: 350px) 100vw, 350px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;&gt;
    &lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;My task in the first week of the internship was to familiarize myself with the current
SOTA and identify strengths and shortcomings of popular public Person ReID datasets.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; margin: 0 0 30px;&quot;&gt;
      &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom: 31.756756756756754%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAGCAYAAADDl76dAAAACXBIWXMAABYlAAAWJQFJUiTwAAAA7ElEQVR42j2R6Q6EIAyEff+3NFE5XC8QBY/Mdrphf5DitPO11MaYAXHfsW0r+r5HKUWjc060DW3bSgwwxmBdV9WMsTiOA846nOeJeZ41x3vTdR1CiIgxYvSjAr3AUkoIYp6mCTlneO8VQuCyLH8QIzX6FTh9PlrIQ8h931rwSOR31UIIuK4LaU8ouWgT5qjt8kIOQq0ZR68C6Tzv+2o3Rjah9jyP1jBSYwOaa642JbQxwyDv31Ss4CjTsIBgGnl4LyXrpKyp0xPyyxVt1lhZNndImLVWDdwbC3ivT60/KcTfvk8xr7JLNqNOP4Ff2N/NsTVretIAAAAASUVORK5CYII=&apos;); background-size: cover; display: block;&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;Dataset Comparison&quot; title=&quot;&quot; src=&quot;/static/9ca886f117ee1bf5671a5074928286a4/fcda8/datasets.png&quot; srcset=&quot;/static/9ca886f117ee1bf5671a5074928286a4/12f09/datasets.png 148w,
/static/9ca886f117ee1bf5671a5074928286a4/e4a3f/datasets.png 295w,
/static/9ca886f117ee1bf5671a5074928286a4/fcda8/datasets.png 590w,
/static/9ca886f117ee1bf5671a5074928286a4/efc66/datasets.png 885w,
/static/9ca886f117ee1bf5671a5074928286a4/c83ae/datasets.png 1180w,
/static/9ca886f117ee1bf5671a5074928286a4/6c86f/datasets.png 1720w&quot; sizes=&quot;(max-width: 590px) 100vw, 590px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;&gt;
    &lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;I took a note of image sizes and attributes (such as gender, clothes, accessories, etc.)
in these datasets. I found out that these datasets had small camera networks, therefore
less variance in scenes. We were able to expand the network, as well as create a more
realistic surveillance setting, having secured access to public CCTV cameras:&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;~80 cameras spread over 34 locations in NTU Singapore Campus (thanks to NTU)&lt;/li&gt;
&lt;li&gt;~50 cameras spread over 23 locations in Singapore (thanks to DSTA)
&lt;ul&gt;
&lt;li&gt;Orchard Road (Shopping Area), CBD (Business Area) and Civic District (Tourist Area)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; margin: 0 0 30px;&quot;&gt;
      &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom: 14.864864864864865%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAADCAYAAACTWi8uAAAACXBIWXMAABYlAAAWJQFJUiTwAAAA5UlEQVR42g3J206CAACAYd6sF/Ci5dLGMpej0LCVB6S5CFAS3Owc5FCndjClMFxt9gRetPVMf323n7C7lyG9tYmUT9K0S5SPC6TSSfJFGU2r0h2dc9+7oFTZJ7G+xmDwgGnaNAyLtqHS73XJbIvksiLTyQRBN3Rurm4pV/PU9SInNRVNPUUWN3Adh2AxonVdp2VaHMgpXNdmJ1vAcTq0dY1hcMfRoYLvBaxWvwjRe0wczXkZP7OI3v7jkqfHOdPXJcvvH2zPR1ISNM4MLN1kNguxmh1ykkJFrfH1GeP5fcLwg+F4wB/YZplbDFLttAAAAABJRU5ErkJggg==&apos;); background-size: cover; display: block;&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;CCTV footage&quot; title=&quot;&quot; src=&quot;/static/e8ec9b4d67d866c0348ad2ea3c12002b/fcda8/cctv.png&quot; srcset=&quot;/static/e8ec9b4d67d866c0348ad2ea3c12002b/12f09/cctv.png 148w,
/static/e8ec9b4d67d866c0348ad2ea3c12002b/e4a3f/cctv.png 295w,
/static/e8ec9b4d67d866c0348ad2ea3c12002b/fcda8/cctv.png 590w,
/static/e8ec9b4d67d866c0348ad2ea3c12002b/efc66/cctv.png 885w,
/static/e8ec9b4d67d866c0348ad2ea3c12002b/c83ae/cctv.png 1180w,
/static/e8ec9b4d67d866c0348ad2ea3c12002b/b6e50/cctv.png 1862w&quot; sizes=&quot;(max-width: 590px) 100vw, 590px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;&gt;
    &lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;Moreover, I learned that most of these datasets were hand-annotated, which ensured
high accuracy, but led to a smaller dataset and longer time to create one. Since I had
the fortune of working on the project in 2018, I decided to automate Person Detection
using Machine Learning.&lt;/p&gt;
&lt;h3 id=&quot;implementation&quot;&gt;Implementation&lt;/h3&gt;
&lt;p&gt;After the literature reviews, I drafted a roadmap for the project with my mentor,
which resulted the following to be implemented.&lt;/p&gt;
&lt;h4&gt;Dataset collection strategy: NTU ReID WebApp&lt;/h4&gt;
&lt;p&gt;We were considerate about privacy from an early stage, and developed what was called
the &lt;strong&gt;Privacy-aware user-driven dataset collection strategy&lt;/strong&gt;. This involved a mobile
web-app designed using the &lt;strong&gt;Google Maps API&lt;/strong&gt;, &lt;strong&gt;Flask&lt;/strong&gt; and &lt;strong&gt;Gunicorn&lt;/strong&gt; in &lt;strong&gt;Python&lt;/strong&gt;.
It provided an easy interface for the volunteering “actors” in the dataset.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; margin: 0 0 30px;&quot;&gt;
      &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom: 37.16216216216216%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAYAAAAIy204AAAACXBIWXMAABYlAAAWJQFJUiTwAAABrElEQVR42iWRW2/TQBCF/QN54ceA4IWbhCghSPA/eImAFxASpa0aVaSNyJXSkhA7ttd78d0xdtKPDV3paM5oNGfOzDp5lmCMRsUGpSRKCMLJCPf7Gb67Jstzzgd9Oq/fctB9Q6fziqfPD+h9+Ehua9VmQ1mWKG1omgYny2K0VoSei5iOUVIRTCe4/VMWoym+jClSTVXXpHVDW+c0ecqmaamsUNtuadqWzaZmt9vi5EVGHPpcnRyzGJ7jnx7jffmEPx4RLZfWtY9WroVkOrzA+71iNrnEDQKSJKG2g/Yoy+pWME5iPM/DPbNCoyHj9z2ueu8IL3+i8phoPbONNoaC1cIjTw0mluz7Cuswywvqv43llXXb4hi7bmQM4npO5K9YjCcsvh6xvuiTymsiHZDmmiCU6MBjW2nSNCEtClSSEojI1oQdkFHatZ39h8i9qLfCP/zM0t7y15+BzWdIKRBKYRKNkRHC1mLLAxGi7e13zZab3Y4bbt/WckdreSsYCdyTbwQ/jpBqThynFjHJHon5HwNjnRlBYcWm8wH379zl0b0HPHvxkoePn9DpdvkHvxj+UA5JpzcAAAAASUVORK5CYII=&apos;); background-size: cover; display: block;&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;Mobile WebApp&quot; title=&quot;&quot; src=&quot;/static/0ab020da5b819df56e10eb434f8e3a1d/fcda8/daq_ntu.png&quot; srcset=&quot;/static/0ab020da5b819df56e10eb434f8e3a1d/12f09/daq_ntu.png 148w,
/static/0ab020da5b819df56e10eb434f8e3a1d/e4a3f/daq_ntu.png 295w,
/static/0ab020da5b819df56e10eb434f8e3a1d/fcda8/daq_ntu.png 590w,
/static/0ab020da5b819df56e10eb434f8e3a1d/efc66/daq_ntu.png 885w,
/static/0ab020da5b819df56e10eb434f8e3a1d/c83ae/daq_ntu.png 1180w,
/static/0ab020da5b819df56e10eb434f8e3a1d/3c492/daq_ntu.png 1300w&quot; sizes=&quot;(max-width: 590px) 100vw, 590px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;&gt;
    &lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Privacy-aware&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Only collected images of participants who accepted the privacy policy&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;User-driven&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The actor could indicate when they were passing through the FOV of a given surveillance camera, reducing annotation effort:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the web-app automatically recorded time-stamps which could be matched in the surveillance videos to extract 1-min clips for the particular actor&lt;/li&gt;
&lt;li&gt;the actors enterred their own accurate appearance attributes into a form&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Collection Strategy&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The web-app would display active paths which could be walked on a particular day. This reduced the number of CCTV footage archives that had to be accessed in a day.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This approach proved to be an important USP of our system, since a year later, in 2019,
due to controveries about privacy and consent, the DukeMTMC dataset was
&lt;a href=&quot;https://www.dukechronicle.com/article/2019/06/duke-university-facial-recognition-data-set-study-surveillance-video-students-china-uyghur&quot;&gt;shut down&lt;/a&gt;
and MSMT17 has to release a new version to mask up the faces of all pedestrians involved.&lt;/p&gt;
&lt;h4&gt;Dataset creation: YOLOv3 -&gt; ResNet50 -&gt; Cosine Similarity&lt;/h4&gt;
&lt;p&gt;From the previous step, we could obtain 1-min video clips of the actors in the scene.
The next step was to extract the target actor’s images from the video frames and
annotate them with the IDs and attributes of the target actor. The proposed pipeline
involved two main components - Person Detection (retrieval) and Target Extraction (ranking).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;[RETRIEVAL] Person Detection&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The first task was to isolate regions corresponding to people within a video frame.
A person detector was built using the &lt;a href=&quot;https://pjreddie.com/darknet/yolo/&quot;&gt;&lt;strong&gt;YOLOv3 architecture&lt;/strong&gt;&lt;/a&gt;
in &lt;strong&gt;PyTorch&lt;/strong&gt;, and modified to return region proposals (bounding boxes) corresponding to the “Person” class.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;The YOLOv3 system was much faster than Deformable Part Models
(DPM) used in Market-1501 and Faster-RCNN architecture used in MSMT17, and had a
mAP of 57.9% on &lt;a href=&quot;https://paperswithcode.com/sota/object-detection-on-coco&quot;&gt;COCO test-dev&lt;/a&gt;.
For more details, I recommend you to read the paper,
&lt;a href=&quot;https://pjreddie.com/media/files/papers/YOLOv3.pdf&quot;&gt;YOLOv3: An Incremental Improvement&lt;/a&gt;,
as it is a very interesting and fun read!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;The YOLOv3 Person Detection system was able to achieve 10fps processing speed on a
Nvidia GTX 1070 GPU, for detecting and annotating bounding boxes within the surveillance video.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;/3b7f63496ff1b2ac1c4a440a161d2f44/person_detect.gif&quot; alt=&quot;Person Detection Demo&quot;&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;[RANKING] Target Extraction&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The next step was to rank the retrieved images, i.e., extract target person from the YOLOv3
detections. Since this was basically a simple tracking problem in a single video, we use
a feature extraction with distance metric two-stage pipeline.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;A &lt;a href=&quot;https://pytorch.org/hub/pytorch_vision_resnet/&quot;&gt;ResNet50 CNN (pre-trained on ImageNet)&lt;/a&gt;
was used to extract robust feature representations of detected pedestrians. The user
first had to select the target person image, called the probe/query image, and tracking
was achieved by ranking the &lt;a href=&quot;https://en.wikipedia.org/wiki/Cosine_similarity#Definition&quot;&gt;cosine similarity score&lt;/a&gt;
of the 2048-D feature embeddings (after the global max-pooling layer) between query
and YOLOv3 detected images.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Since the video was continuous, a weighted average of the original query image and
the detected target image tracked in the last frame was used as the new query image.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; margin: 0 0 30px;&quot;&gt;
      &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom: 65.54054054054055%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAYAAACpUE5eAAAACXBIWXMAABYlAAAWJQFJUiTwAAAB/UlEQVR42o1T33PSQBDm73fGF5988KU2M/UHI/qiM/pSdSpYJkTGGkoEEpILEgi5BBIIIcnn3lGKtaW6M5vc3e59++2Pq4CEuS7iJBFLlGV5Q4VEUQSbfIYOAw8C3CcV8Tn98A4j17kG/FN2O7VnodHt3+lzC7D69AiGriNdrzGbzeALJSa+7yMMQ2RpirbFoJrO/wE+VxS0NQ0BgTDGMOjosChA3zDgTSYIKIDlTWH7wQ3Whxm+fIHh0JIHhYi+WgLziDbFdk/SGthoGIN/M9xsNuh2u+Ahp/vFVumC1Ku9AFhS2isqieBXSnuBvMjJnst/Tn6CemU+n2NCadVqNWha6xaD3XqTZcjzXKZ7L8OExmWxiPHo4QNUTxQsVytwGhOhcRxD2MfjMSbTKcYUWDRKnGX5GpejNnT2DT8cFS639oCchzh+8hjPlCNwaoB78R2s06FzLtW0LLBeD8PLDiyqdRRSsGQBtV/HufGZxukjrJmxBdzRV9UmdOquTDOjWlFthS2jVOWQi/qJc9E4qpeo/Z0pr8lRzN/b12/w5dPpdafLq4sC8G/ZBZK+OTWt2Ne0ElJKzGV4f1LFq2OF5s6D0/sJu9+D53kHmYQxR8usQzMbUAdnYNzcz6GQi69n0Jp1+Spix0b8a4SU1ockSWN6PU20zXMJOgptCfgbuKbf9vp7mjMAAAAASUVORK5CYII=&apos;); background-size: cover; display: block;&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;Retrieval flow&quot; title=&quot;&quot; src=&quot;/static/1167b2bcc6729a0c512f248ac434dd14/fcda8/retrieval.png&quot; srcset=&quot;/static/1167b2bcc6729a0c512f248ac434dd14/12f09/retrieval.png 148w,
/static/1167b2bcc6729a0c512f248ac434dd14/e4a3f/retrieval.png 295w,
/static/1167b2bcc6729a0c512f248ac434dd14/fcda8/retrieval.png 590w,
/static/1167b2bcc6729a0c512f248ac434dd14/efc66/retrieval.png 885w,
/static/1167b2bcc6729a0c512f248ac434dd14/ea64c/retrieval.png 1116w&quot; sizes=&quot;(max-width: 590px) 100vw, 590px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;&gt;
    &lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;The Person Tracking system was able to a maximum of 5fps processing speed on a Nvidia
GTX 1070 GPU, for the entire pipeline, which involved person detection, target matching
and annotating bounding boxes within the surveillance video. Some of the developmental
scripts are available in this &lt;a href=&quot;https://github.com/kj7kunal/Person-detect-identify&quot;&gt;Github repo&lt;/a&gt;.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;/dae77eeedf2912c3b123f7771464ebb2/person_track.gif&quot; alt=&quot;Person Tracking demo&quot;&gt;
&lt;/p&gt;
&lt;h3 id=&quot;result&quot;&gt;Result&lt;/h3&gt;
&lt;p&gt;The new Rose-IDentification-Outdoor (Re-ID-Outdoor) dataset was collected and
annotated. The dataset was collected from 50 real surveillance cameras in NTU
and came with privacy consideration from all participants (volunteers in the campus).
Overall, the Re-ID-Outdoor dataset was considered the most realistic and also
the only privacy-aware public dataset for Person Re-ID research so far.&lt;/p&gt;
&lt;p&gt;A similar dataset was also created from surveillance cameras in different areas
in Singapore, with actors consisting of ROSE Lab members. However, I am not sure
if that dataset was processed or released.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://wrap.warwick.ac.uk/143315/1/WRAP_Theses_Lin_2019.pdf&quot;&gt;Dr Lin Shan’s thesis&lt;/a&gt;
explains his work on Person Re-ID meticulously, and is worth going through to understand
how the work was carried forward. Some of the images were taken directly from
the thesis for ease of writing this blog.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; margin: 0 0 30px;&quot;&gt;
      &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom: 20.27027027027027%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAECAYAAACOXx+WAAAACXBIWXMAABYlAAAWJQFJUiTwAAABN0lEQVR42g2Qzy/CAQDF+6scXM2GRSPFkq0f3zTq61sq+iXfVqEDB1vaXGyIA3NyiIzN2LJmU+RYIdJBNl2sDh/fw9u7vM/e3lN5zSZ8MzbMGi1uixXRYmNUa2AjKePwp3ksPuMLxMgcnRIK+AmHolxd3pK7PsNjHSfqdSJLJvQTaoKhMCrjkIYp9Qi6vgFMmjF0g8P09PbjnHWg9xxyVygjusJE5DXiK3Hsgpu93WPu8jesu+YIOgRS0VlswjSCyYwquOhjQZKU9iXkyDJuUSQgOcjsbGGYNPJUemY7uUlcjrHgkhRwnoP9E/KFe1LpFInEKoJgxa6s1I5PofpqfFKrVKjVary9KlL8pVym1frmt92m2+ny02zyWqlSqVap1z/4++vQaL5TLD0o+apywTmXuRzZ7AX/Y3naXaksRZQAAAAASUVORK5CYII=&apos;); background-size: cover; display: block;&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;Result&quot; title=&quot;&quot; src=&quot;/static/dc33986bf61d0e9dded6c3da5c8a9bfe/fcda8/result.png&quot; srcset=&quot;/static/dc33986bf61d0e9dded6c3da5c8a9bfe/12f09/result.png 148w,
/static/dc33986bf61d0e9dded6c3da5c8a9bfe/e4a3f/result.png 295w,
/static/dc33986bf61d0e9dded6c3da5c8a9bfe/fcda8/result.png 590w,
/static/dc33986bf61d0e9dded6c3da5c8a9bfe/efc66/result.png 885w,
/static/dc33986bf61d0e9dded6c3da5c8a9bfe/c83ae/result.png 1180w,
/static/dc33986bf61d0e9dded6c3da5c8a9bfe/5ab15/result.png 2446w&quot; sizes=&quot;(max-width: 590px) 100vw, 590px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;&gt;
    &lt;/span&gt;
&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id=&quot;experience&quot;&gt;My Experience&lt;/h2&gt;
&lt;p&gt;As it was my first time going abroad, I was super excited to fly to Singapore as soon
as my fourth year ended. I had a list of action items sorted out for my first week
after talking to my lab’s POC, to settle in the new place.&lt;/p&gt;
&lt;p&gt;I was given a competitive stipend, and my lab had arranged for me to live in one of
the NTU hostels. NTU had a well-connected bus network, so getting around was no issue.
I had a Vietnamese roommate, who was a Bachelor’s student in the university, and also a
great tour guide around the campus.&lt;/p&gt;
&lt;p&gt;My first week at ROSE Lab was a mix of introductions and formal work. There was an
introductory session organized by Prof Kot and Dr Dennis, who introduced us to the
ROSE Lab team, talked about the various projects undertaken by the lab, and finally
showed us to our respective working areas. I had to take a day to visit the Singapore
Ministry of Manpower to receive my work permit, which was a very smooth process. I
also had to register for a temporary student ID with NTU Singapore.&lt;/p&gt;
&lt;p&gt;My “cubicle” was well-equipped and I was free to personalize it for the duration of
my stay! Even though there was a PC, I liked to use my own laptop, as most of my work
was on the on-prem servers. The research scholars around me were really friendly and
approachable, and they guided me through most of the setup process.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; margin: 0 0 30px;&quot;&gt;
      &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom: 70.94594594594594%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAYAAAAvxDzwAAAACXBIWXMAABYlAAAWJQFJUiTwAAAD90lEQVR42h2TW1DUBRTG/zM2U4oXRBC5KiAhGIiSuQpOVHiLxAUBy0SWO7s77rIsLCCgy01YXCURBFZuewHdleUiMHinCENGbBiDpjHTzOm9Mpt6+vUfH76Hcx6+853vfEeY+HaemZmHPHv2gvtTUzz4bpZXf/3NL89fcPP2NwxcG+DG+AjTk7d58v19XjyeZW76HqYuM1ZLP1f67divDrxBn9WCMPfjb1ivOnjy9FeMBiMKuZqJ8VtcH7vD5R47Lc2ttDU3Yem4yM+zd3g5P8XXE6N0mHow99ow99iw9vZjNffT29ODMOIcob29WSQZ47zBgFalxdjwFS2t3TQ0XuT0qSps5m5uDdv5afYeLxcecGtsFMfACE7nKGbLFax9DlHtFSzmPoR2o4HJgT7uDtiYGrFzf9TBeF83TrMJR9clTI1V5B05xJeHk5BnZiLPkiGXpZF/LJUTGUfRybMpVyupKS2mU1QtNDc0MHtzkJkbIiaczN0eFmsnqqw04vfvISnhU/ZJtrAtbBNhkbuJijlE1I44DkoikMVGkBe3jcyYMJQJsXSIWwnqfDnOXhP2zktc625j2HIZa+s5dm+PQliyDEF4C1eXpezYuZetkn2ER+5ke5SErAMxVKZ9Rn1uCnWygzRqlbRfEo/y+VEZlWWVVJSUUVlURGNVFW0N1XwSvRO3VSvxXO2C71p3Klru0jv1J8dUdbzr70VS9BZ06akYS9U0lShoqq6h6ZwJIfFIGpXlerTak+i0pSQeSqJWk8201UBdQQap8Xs4npxIskwc2DHN+x+lsGKJQKgkhi/kGvRqBef15dTrDZytvYCQnZnDqYoalHIVOz6QEBEeQUhwCGkJHzPUVkGDvoS4vVJiYvaRk56BNj0B2f6t+HUO4qWpJnpTMIelUvQnq6k9ZUAoUBZSXqYnNSUVz3VrCQhYT2hIMF7efqz39uCs7ihDnTXU6RR0nSnk9aM+HJazbHBMoG6pR58iIf5D0c+SKhF6hBPKInTF5SRKkwgMDMDX1xt3D1feC91E+OZwNgYFUiCT8nDYwO9z/fzzwzXqzc34tNu4N9iC7XQeRdkZ6NRllBaWIWgKdGjURUTv2kVQkEjos5ZVrsvfkOdlZRN/4IDY88HTdSnGwmT+XRCzujiJ9MJ55sfbsJ1RocmRo8rVoFVqEUqKy8jPUeDv74vbmlW4ua1k6bK3WbHSBQ/31XiucWW9nzeRIf4MtxezMFTJ4vU6Ua2d14uD/LEwJIZbTd7xfNS5KoTaqnqSpSlsjdwi+uaBy/J33hAHbPDHz2cdPuvc2RwahiI5luc3G/nv6RBPxvRMmxS8emzj0XgXspQs8tPzxWdQ8j9fOJdtBmzzbQAAAABJRU5ErkJggg==&apos;); background-size: cover; display: block;&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;My work area&quot; title=&quot;&quot; src=&quot;/static/c035752d94d62669a164a40c8aa70331/fcda8/cubicle.png&quot; srcset=&quot;/static/c035752d94d62669a164a40c8aa70331/12f09/cubicle.png 148w,
/static/c035752d94d62669a164a40c8aa70331/e4a3f/cubicle.png 295w,
/static/c035752d94d62669a164a40c8aa70331/fcda8/cubicle.png 590w,
/static/c035752d94d62669a164a40c8aa70331/ad12c/cubicle.png 856w&quot; sizes=&quot;(max-width: 590px) 100vw, 590px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;&gt;
    &lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;There were no fixed working hours within the lab, and I was trusted to work enough
hours to do my tasks well, at times that suit me and my team. Due to the extremely
hot and humid weather in Singapore, I tried to spend the day in the air-conditioned
lab. Sometimes I used to work through the nights, just to experiment. I had weekly
sync meetings with Lin Shan, and if I had anything else to discuss, we preferred
doing it over a walk around the building. I enjoyed Professor Kot’s once-in-a-while
sync-ups with the interns, where he used to coax us with the benefits of living and
working in Singapore.&lt;/p&gt;
&lt;p&gt;I lost a lot of weight during my stay in Singapore. No, it wasn’t just because I was
saving money by eating less - my job involved a lot of field-work to collect the dataset!
As I am a very passionate runner, I used to sync data collection paths with my running
routes. I was also able to do some sightseeing around Singapore with some colleagues
in the process of collecting data!&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; margin: 0 0 30px;&quot;&gt;
      &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom: 25%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAFCAYAAABFA8wzAAAACXBIWXMAABYlAAAWJQFJUiTwAAABi0lEQVR42h3N20tTAQCA8YPPPSn4GkwkQoaJyryEWEMxDPZioKCQD2rzIcFQWoj6oCESzutiyISJw0R7iHyRoIGIG4rGVHRM5yZTdzi7nOPZjRn4pX1/wO8TQhcBItEYCTVBKpUmmUySyWR4aPvPIU7PGa5Qhk0xzc71DZfpOwKSzMGpn793EAxLeM/PicVjRBMyQvY2SyQi3aMSkiSiyArqjfofXFi0UllRysfxCZr7B2jqfIOxr4PugXaGzCaWV5ZY/bnOxqabnf09vi1PIniDW/e3Xxyd/SYUOsDn2+XU6yEsBnA6rfQ1lTAz9gKLrRqbowrj+yK+2hsYHWzFUPWMsd53zI/289nUg806heDxbeA+XOXE7yJ8FeTkaBu/75iI6Gfu+w8qDJ0Mm7tYtBfhcJRjtRRjs7+k/W0LbY16DHXPKSvWUvhES7VejyCrYaLyBbIqIisSSVVGUeJk0ymGLWvkv/pEx8gg02YNU+ZSZr9o+WCqJPdpLYX1RnIe5VGgeYxOq+F1jY5/P9MmtMIdMAEAAAAASUVORK5CYII=&apos;); background-size: cover; display: block;&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;Data Walks&quot; title=&quot;&quot; src=&quot;/static/018942a896ac6b6776078154555ecb16/fcda8/data_walk.png&quot; srcset=&quot;/static/018942a896ac6b6776078154555ecb16/12f09/data_walk.png 148w,
/static/018942a896ac6b6776078154555ecb16/e4a3f/data_walk.png 295w,
/static/018942a896ac6b6776078154555ecb16/fcda8/data_walk.png 590w,
/static/018942a896ac6b6776078154555ecb16/efc66/data_walk.png 885w,
/static/018942a896ac6b6776078154555ecb16/c83ae/data_walk.png 1180w,
/static/018942a896ac6b6776078154555ecb16/30faf/data_walk.png 2802w&quot; sizes=&quot;(max-width: 590px) 100vw, 590px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;&gt;
    &lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;The lab had a diverse demographic, and I interacted with people from China, Brazil,
Belarus, Poland, etc. This was the first time I had mingled with people on a
global scale, and it was a really thrilling and emboldening experience for me. As an
introvert, I found it slightly overwhelming to socialize (free-talk) with such a group,
and I felt this experience helped me gain self-confidence and getting over my fears.&lt;/p&gt;
&lt;p&gt;I was very happy to find 3 more Indian interns, as well as an Indian research scholar
working in the lab. We bonded well both in and outside the lab, and frequently went
out to explore Singapore over the weekends.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; margin: 0 0 30px;&quot;&gt;
      &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom: 29.054054054054056%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAGCAYAAADDl76dAAAACXBIWXMAABYlAAAWJQFJUiTwAAABuElEQVR42hWQXU9SAQCGTzoFOXLwwDl8HgEROQEDxPjIopBSQzymLA2KsDlzlevDXFG2ucoByxs3W1ettn5FF1500+Z9v+jpdPFcPdu7Z69QqVQIZ+dweMMIwiDaxRw/zv5y/uc3661tmvU6Tx/t0G42mElP43EG0ANBJrQpbsSKzEfyKE4Vr28c0aohXEpGECUZu8PJUi5KdCrOfqfDs8PPtF4ccNz9wNfTU3rdI9Zqt4hZRGSrSG5MIenyk7IrBCWVjMuNxx5C+NXbpb9V4/aVDO2FAqVsnK3SBP3HVT71X/Lm+TZfekd8fP+K7usnvPOHuepQ6GgR5r0aHX+UohJnUfYwKSkI3/Y36TWqLOazjLg0s9bDvWKes+MWP7+fUKvWuWtssLf7FmOuQtA6Stnp46EnyIhZqttk/DaVJfck10yErBYmblcJqAGGxDEswzZ8Pp29NYOTwwNWjWXGQ9O0729yM51GtUjsJIqs+szPLwwzMPCfIZKSl4TkQkhZFUplA11PI5hC9YZwRwo8uLxBc3mFldkZihGd9cos5USMjN1FJ1WgoUUZNMeuy27uqBott0bMNso/6ybH+M30Jv4AAAAASUVORK5CYII=&apos;); background-size: cover; display: block;&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;ROSE Indians&quot; title=&quot;&quot; src=&quot;/static/13c4a9363d15bafbbb3bb1e12f81222a/fcda8/ROSE_indian.png&quot; srcset=&quot;/static/13c4a9363d15bafbbb3bb1e12f81222a/12f09/ROSE_indian.png 148w,
/static/13c4a9363d15bafbbb3bb1e12f81222a/e4a3f/ROSE_indian.png 295w,
/static/13c4a9363d15bafbbb3bb1e12f81222a/fcda8/ROSE_indian.png 590w,
/static/13c4a9363d15bafbbb3bb1e12f81222a/efc66/ROSE_indian.png 885w,
/static/13c4a9363d15bafbbb3bb1e12f81222a/c83ae/ROSE_indian.png 1180w,
/static/13c4a9363d15bafbbb3bb1e12f81222a/fc99f/ROSE_indian.png 2702w&quot; sizes=&quot;(max-width: 590px) 100vw, 590px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;&gt;
    &lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;The most unexpected experience during my internship was the 3-day summer school
organized by ROSE Lab on research topics in
&lt;a href=&quot;https://rose1.ntu.edu.sg/event/VISVA2018/&quot;&gt;Visual Image Search and Video Analytics (VISVA)&lt;/a&gt;.
It was an opportunity for me to get to know the state of the art technologies from
direct interaction and discussions with pioneers in academia and industry, in a
friendly and constructive environment. Research topics included Biometrics, Image
Forensics, Autonomous Vehicles, Object retrieval, as well as Person Re-ID in
Surveillance, which gave me a lot of insight while working on my own project!&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.youtube.com/watch?v=w2_HG4WV-v0&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; margin: 0 0 30px;&quot;&gt;
      &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom: 49.32432432432432%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAABYlAAAWJQFJUiTwAAAC2klEQVR42h2R3U9bBQDFb6KGhLZAWTt6P9pCL+0tvd1toaVd29HSsjKgjAJD9lVs1jGmCbBg6LIlm/JiHDGAGmd82EYixmWJJroYndHoRpxsD+qT8ePBB17UGSP+BT9vfD0n+Z2Tc4Sj0ROE9Diy5KOhwYrV0oK10Upjo4UmWxOSKKIFVHxeD25ZQhElpDaRYDhO7tk5Ti+vMLW6RWLwJNbWIILdLmG12mludmCzOQmHIujBEDZrM/sdLtrdHqIHwv9riqjgavMgOkUK5SrXt97lo81XuPDqm/SMvIBLLZjAZic2i52nnrbxTEML+xxuvG4/YptCi11Edsn0pdLoXWEUuYOmVjexsMHE1BmOza8zNnOZQHGFQL6OGjuFoDgVPA6JfWajTlGmQ+mkXVbJGAbZsMbRhMZsKclCKcbSaILheJir56bYuHKR9c0vuXTtY557/g0GJl+mWFlF8HtC+GU/L115nRs37lKrLTNVmmb+7AKjmT7eXzvP9/fW+enWVW5Nxhk52Ec+laU8Ms7ek13++fsJ/+7tcffeY9JjlxAGUjksFplaZZml+hrV04vMlSscGz5ByQReGE9xc3WejWqJy+Usw5kcDkfAPDLFg/vbPNr5gd3d37m//R3hQ7MIg7lBfE4vMV8XSXMnv8vDnNqJ1rofr/mmJivktQDn/Z2MdulkAzq61osRyfPo2x3+/OMvfv3lNz797CFGYRFh6Mg0srOdkGqQ0iOIDg9JE5gxgkRUP1nNy2y/zriuExeDRDsyyL6DaN1lhibq1Fdu887m19z58CFeYxphZvI4ye40PQd66TVi5JJpEkbEDFDxSl6Mdgnd50NVgqSjhzH6akSMQ0QTY1x77T02t7ZZe+sLzl38gK74SYREtJsj/f2MFQucHT/MUqVEvVbmxeooC6eGODNRYLpUZCCbpyeWp1gwvdoM1eMVrm+8zeeffMXjnR958M3PVBZv8h9KkXiA8KWsWwAAAABJRU5ErkJggg==&apos;); background-size: cover; display: block;&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;VISVA 2018&quot; title=&quot;&quot; src=&quot;/static/9dd689202dba44bc9f4c103484517921/fcda8/visva_all.png&quot; srcset=&quot;/static/9dd689202dba44bc9f4c103484517921/12f09/visva_all.png 148w,
/static/9dd689202dba44bc9f4c103484517921/e4a3f/visva_all.png 295w,
/static/9dd689202dba44bc9f4c103484517921/fcda8/visva_all.png 590w,
/static/9dd689202dba44bc9f4c103484517921/efc66/visva_all.png 885w,
/static/9dd689202dba44bc9f4c103484517921/c83ae/visva_all.png 1180w,
/static/9dd689202dba44bc9f4c103484517921/5cae2/visva_all.png 2332w&quot; sizes=&quot;(max-width: 590px) 100vw, 590px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;&gt;
    &lt;/span&gt;
  &lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;As I was facing my final year after the end of the internship, I was hungry for
career insight. Luckily, I was around research scholars who had and were taking such
decisions themselves. After multiple such interactions, as well as a career counseling
with Dr Dennis Sng, I found that I understood the merits to both academic and corporate life,
but also that I wanted to pursue higher studies only to get better jobs. However, one
thing was different from my past internships, and that was my reinforced enthusiasm
for the field I was working in - Machine Learning and Artificial Intelligence!&lt;/p&gt;</content:encoded></item><item><title><![CDATA[The ISRO Internship]]></title><description><![CDATA[I spent the summer of 2017 as a research intern at the ISRO Satellite Centre, Bangalore. At the start of my third-year at IIT Kharagpur, I…]]></description><link>https://kj7kunal.github.io/blog/2017-isro-intern/</link><guid isPermaLink="false">https://kj7kunal.github.io/blog/2017-isro-intern/</guid><pubDate>Sat, 01 Aug 2020 12:00:00 GMT</pubDate><content:encoded>&lt;p&gt;I spent the summer of 2017 as a research intern at the &lt;a href=&quot;https://www.ursc.gov.in/&quot;&gt;ISRO Satellite Centre, Bangalore&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;At the start of my third-year at IIT Kharagpur, I would not have guessed that I
would get to work for &lt;a href=&quot;https://www.isro.gov.in/&quot;&gt;my country’s national space agency&lt;/a&gt;
and be able to further ongoing theoretical research on ways to tackle the Kessler syndrome.&lt;/p&gt;
&lt;p&gt;In this internship, I designed mathematical models and simulations for physical objects interacting in space.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Contents&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#selected&quot;&gt;How I got selected&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#resume&quot;&gt;Resume&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#application&quot;&gt;Application Process&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#interview&quot;&gt;Interview Call&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#project&quot;&gt;The Project&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#objectives&quot;&gt;Objectives&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#motivation&quot;&gt;Motivation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#implementation&quot;&gt;Implementation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#result&quot;&gt;Result&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#experience&quot;&gt;My Experience&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id=&quot;selected&quot;&gt;How I got selected&lt;/h2&gt;
&lt;h3 id=&quot;resume&quot;&gt;Resume&lt;/h3&gt;
&lt;p&gt;Well, other than being enrolled in an Aerospace Engineering degree at one of the most
prominent institutes of the country, the strongest point on my resume was a
successful 3-month internship at &lt;a href=&quot;https://www.teamindus.in/&quot;&gt;Team Indus&lt;/a&gt; in the summer of 2016. Team Indus is a private for-profit aerospace company that was all geared up at
winning the &lt;a href=&quot;https://www.xprize.org/prizes/google-lunar/competing-teams&quot;&gt;Google Lunar X Prize competition&lt;/a&gt;. I had designed some mathematical models and simulations for thruster
misalignment estimation during various burn phases, mainly in MATLAB.&lt;/p&gt;
&lt;h3 id=&quot;application&quot;&gt;Application Process&lt;/h3&gt;
&lt;p&gt;The application process was tricky. I could not find an “open positions” page
online. And if you have ever been to the ISRO website, it’s really hard to find
what you’re looking for. No, my best friend here, was networking.&lt;/p&gt;
&lt;p&gt;I had heard about the &lt;a href=&quot;https://www.iist.ac.in/&quot;&gt;Indian Institute of Space Science and Technology&lt;/a&gt; having direct ties with ISRO, and that students usually go for internships at
various labs across the entire space agency. Through a bit of googling, I found a
list of students who had interned at the ISRO Satellite Centre the previous summer,
and through a bit of facebooking, I found one student with a close mutual friend.
I soon scheduled a call with her, and she turned out be a really interesting person.
We talked about her project which was based on robotics, as well as her university life.
She set me up for a call with her mentor, who was also a graduate from her university.&lt;/p&gt;
&lt;h3 id=&quot;interview&quot;&gt;Interview Call&lt;/h3&gt;
&lt;p&gt;My call with my mentor, &lt;a href=&quot;https://twitter.com/arallapalli&quot;&gt;Mr Aditya R.&lt;/a&gt;, basically involved
a CV runthrough, an extensive discussion about my work at Team Indus, and a free-talk
about my interests and projects at IIT Kharagpur. To my relief, he said he was looking
for interns and my resume matched his requirements. More luckily, I got to choose between
2 projects to work on!&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id=&quot;project&quot;&gt;The Project&lt;/h2&gt;
&lt;h3 id=&quot;objectives&quot;&gt;Objectives&lt;/h3&gt;
&lt;p&gt;The goal of the project was to simulate active space debris removal using tether-net
connected to satellites in formation. There were 3 main concepts involved:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1. Spacecraft formation flying&lt;/strong&gt;:
This topic is well explored and enough literature was available.
It involved relative position control combined with orbital mechanics.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2. Tether net modelling&lt;/strong&gt;:
The behaviour of the net had to be described as a spatially distributed mechanical system.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3. Contact dynamics&lt;/strong&gt;:
The collective behaviour of the net and the satellites in
formation during and after debris contact phases.&lt;/p&gt;
&lt;h3 id=&quot;motivation&quot;&gt;Motivation&lt;/h3&gt;
&lt;p&gt;Among many things, space is also the most dangerous junkyard Earth has created.
If you have seen the 2013 film Gravity, the accident that caused the astronauts
to break off from the ship was due to the debris in space.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;/b8628ebf5324809a1b1de33a9883d6af/gravity_debris_accident.gif&quot; alt=&quot;Gravity Accident&quot;&gt;
&lt;/p&gt;
&lt;p&gt;It is estimated that there are 600,000 debris fragments ranging from 1~10 cm,
and on average one satellite is destroyed by collision with space junk each year.
If satellites are not disposed of properly, the parts could become high-speed bullets
leading to a chain reaction of collisions, a theoretical scenario popularly known
as the &lt;a href=&quot;https://en.wikipedia.org/wiki/Kessler_syndrome&quot;&gt;Kessler Syndrome&lt;/a&gt;.
Apart from dramatic accidents, such a scenario could lead to serious consequences
including unusable orbits or communication blackouts.&lt;/p&gt;
&lt;p&gt;A number of cleanup efforts have been explored/proposed in recent years - robotic
grappling arms, harpoons, deploying nets, electrodynamic tethers, ground-based lasers, etc.&lt;/p&gt;
&lt;p&gt;This project combined swarm satellites and tether-nets for Active Space Debris Removal.&lt;/p&gt;
&lt;h3 id=&quot;implementation&quot;&gt;Implementation&lt;/h3&gt;
&lt;p&gt;Simulations for the various components were developed in MATLAB and working code was
replicated in C++ for faster experimentation. A C++ library implementation for MATLAB’s
matrix/vector manipulation was developed which reduced simulation time by 5 times
and allowed easy code porting.&lt;/p&gt;
&lt;h4&gt;Spacecraft formation flying&lt;/h4&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 576px; margin: 0 0 30px;&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/f28b120c63c9e77f643cab5ac1f70c54/533c1/satellite_swarm.png&quot; style=&quot;display: block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom: 70.27027027027026%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAIAAACgpqunAAAACXBIWXMAAAsTAAALEwEAmpwYAAABu0lEQVR42o1SO0/CUBiltwWkLSCUglRLAaUQdMWBwCoJoSiEgUcYDIsLzm646erm5CPRRY1GExJfiY+ExYRBjf/HQ0saSDBycnPz3Xu/853v3HstlhH4fD6Hw4GAoijMNh2Wf0HTtMEBHzHDMF6v12q1TkUGB9kcx8VisWQy6XK5DOZUZAAN5/P5arVar9dTqRR27Hb7RDKlY7iQdCBIp9ONRqNSqZRKJbfbjeahb7gw3RFCxiqhYUVRkJ1IJKBcLpebzWY0GiU6AoGAGlOBbDaraVqn02m32zzPD8noDVbD4XChUGi1WrVaLZPJgGZ6Rgxxj8cTiURyazlUgaMxsiiK68ViQdOCwaBx+aZn6IRCCqwhxy/6aUKPde50OnGAJAQT3xk7qGjMY7bREowJgmAyp/8klCyHVDUuyzIs2aw2U9muw8j5k70wL8XVpURclYJzo1mmMkoZg1CD+6coMlzTDOsRJElW5ciysrjCOf0OXpxhBcbmYjkvy84SAnG8M5mgf3LxfnrZO7/5uOr2u8+fT28/D6/fg/HydXffv7ztHZ09Hhxe7+4dt7b3q5s7xcpWKrshRVf5WfkXQglMCacwavAAAAAASUVORK5CYII=&apos;); background-size: cover; display: block;&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;Satellite swarm&quot; title=&quot;&quot; src=&quot;/static/f28b120c63c9e77f643cab5ac1f70c54/533c1/satellite_swarm.png&quot; srcset=&quot;/static/f28b120c63c9e77f643cab5ac1f70c54/12f09/satellite_swarm.png 148w,
/static/f28b120c63c9e77f643cab5ac1f70c54/e4a3f/satellite_swarm.png 295w,
/static/f28b120c63c9e77f643cab5ac1f70c54/533c1/satellite_swarm.png 576w&quot; sizes=&quot;(max-width: 576px) 100vw, 576px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;&gt;
  &lt;/a&gt;
    &lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;Relative orbit dynamics of two objects in close orbits, can be modelled using the
&lt;a href=&quot;https://en.wikipedia.org/wiki/Clohessy%E2%80%93Wiltshire_equations&quot;&gt;Hill-Clohessy-Wiltshire (HCW) equations&lt;/a&gt;. These were used to model feedback linearization
based proportional-derivative (PD) controller for generating the control commands for the spacecraft, necessary to keep the spacecraft in formation to maintain the required
tether-net shape in various phases of the operation.&lt;/p&gt;
&lt;p&gt;Continuous torque commands had to be converted to on-off commands to model actuators.
This was done using Pulse Width and Pulse Frequency modulation (PWPFM).&lt;/p&gt;
&lt;h4&gt;Tether net modelling&lt;/h4&gt;
&lt;p&gt;A &lt;a href=&quot;https://en.wikipedia.org/wiki/Lumped-element_model#Mechanical_systems&quot;&gt;lumped parameter approach&lt;/a&gt; is used to model the dynamics of the
tether-net and the tether-net to spacecraft connectivity.&lt;/p&gt;
&lt;p&gt;This simplifies the description of the behaviour of spatially distributed
physical systems such as a tether-net, into a topology consisting of discrete
entities (nodes) that represent rigid bodies with mass and interactions between
rigid bodies as kinematic pairs (joints, springs and dampers).&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;/78cec2f8f2e2f6f685a876d7d812cd0c/net_lpm.gif&quot; alt=&quot;Lumped parameter model&quot;&gt;
&lt;/p&gt;
&lt;h3 id=&quot;result&quot;&gt;Result&lt;/h3&gt;
&lt;p&gt;The results of this research were drafted into a research paper and accepted for
&lt;a href=&quot;https://iafastro.directory/iac/archive/browse/IAC-18/A6/IP/48269/&quot;&gt;Interactive Presentation in the 69th International Astronautical Congress 2018&lt;/a&gt;
in Bremen, Germany. However, due to lack of sponsorship and funds, I was not able
to attend the conference.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id=&quot;experience&quot;&gt;My Experience&lt;/h2&gt;
&lt;p&gt;As an aerospace engineering student, I was very enthusiastic about working at the
ISRO Satellite Centre, and I was very excited to fly to Bangalore as soon as my
summer vacation started.&lt;/p&gt;
&lt;p&gt;Since I was an “unofficial” intern, I was not given any stipend, and I had to cut
my costs by staying at my uncle’s place - about an hour of bus travel each day.
But also because of that, I had the benefits of working flexibly in terms of
hours and place of work.&lt;/p&gt;
&lt;p&gt;My first week at ISAC, my mentor introduced me to the team at the Control
Dynamics and Simulation Group, who were mostly graduates from IIST. They were
really friendly, and talking to them was like talking to any senior from college.
There were 3 more interns from different universities, who had their own
stories of how they got their internships.&lt;/p&gt;
&lt;p&gt;I usually started work at 10AM and ended work around 4PM. My mentor and I synced
up twice each week and discussed work updates every day. It motivated me to stay
on track. Since I could be flexible, I sometimes used to work from cafes near my
place instead of going to office. My mentor was super approachable, and I could
call him up in the middle of the night to discuss a sudden idea I had. I also
synced up with the group’s project director, &lt;a href=&quot;https://www.siliconindia.com/profiles/dr-vinod-kumar-XEac3AFM.html&quot;&gt;Dr Vinod Kumar&lt;/a&gt;, a few times, who told me about his long
journey at ISRO.&lt;/p&gt;
&lt;p&gt;I enjoyed such 1on1 sessions. They helped me connect with the team better. I also
loved the two &lt;em&gt;chai breaks&lt;/em&gt;, where employees used to line up for free tea/coffee
and free talk. In some of these, my mentor and the other team members used to
take us for tours of the Satellite Centre. It was unbelievable seeing the mission
control for the recently launched &lt;a href=&quot;https://en.wikipedia.org/wiki/South_Asia_Satellite&quot;&gt;GSAT-9&lt;/a&gt;
and &lt;a href=&quot;https://en.wikipedia.org/wiki/GSAT-11&quot;&gt;GSAT-11&lt;/a&gt; in development, right in front of our eyes!&lt;/p&gt;
&lt;p&gt;However, amongst all things shiny, there were things that stood out
that I could not help feeling irritated about. Offices were similar to old Indian
government offices desperately in need of an upgrade. Some of the PCs we used to
work on were still equipped with age-old hardware like CRT monitors. My team’s
office was a room divided into cubicles, which my mentor said made him feel
claustrophobic at times. I used to see more progress when I was working on my
laptop from a cafe than when I was working in a no-window office all day.&lt;/p&gt;
&lt;p&gt;I learned a lot talking to my colleagues. The engineers/scientists shared a lot
of career insight, which had helped me gear up into changing my career in the
coming months. I discussed about some ML projects I worked on in college, and
I learned how it was applied in projects at ISRO. Discussions that could not be
completed over &lt;em&gt;chai breaks&lt;/em&gt; were continued the next day or free pizza treats
after work!&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; margin: 0 0 30px;&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/7291d20fb873e9d83312bd236408512f/47311/free_pizzaaa.jpg&quot; style=&quot;display: block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom: 82.43243243243244%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAQABQDASIAAhEBAxEB/8QAGAAAAgMAAAAAAAAAAAAAAAAAAAQBAwX/xAAVAQEBAAAAAAAAAAAAAAAAAAAAA//aAAwDAQACEAMQAAABqYVejXPJD//EABsQAAIDAAMAAAAAAAAAAAAAAAECAAMREiIj/9oACAEBAAEFAnTRT1qPHVPowWFzP//EABURAQEAAAAAAAAAAAAAAAAAABAR/9oACAEDAQE/AYf/xAAWEQADAAAAAAAAAAAAAAAAAAABECH/2gAIAQIBAT8BMX//xAAbEAEAAgIDAAAAAAAAAAAAAAABABECMSFBkf/aAAgBAQAGPwLhVjjl3NekKblDTNs//8QAGxAAAgMAAwAAAAAAAAAAAAAAAREAITFRYZH/2gAIAQEAAT8hoEXqa4BBnZOGGAGgxwnQba8j6QqQn//aAAwDAQACAAMAAAAQJ9//xAAWEQEBAQAAAAAAAAAAAAAAAAABACH/2gAIAQMBAT8QRYX/xAAXEQEBAQEAAAAAAAAAAAAAAAABACEx/9oACAECAQE/EE9Wt//EABsQAQADAAMBAAAAAAAAAAAAAAEAESExQVGR/9oACAEBAAE/EFXaAar2PSDajSFS2F1CVGGmM5xQH1+wy2sU4L6YC5IsJRWT/9k=&apos;); background-size: cover; display: block;&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;Pizza Treats&quot; title=&quot;&quot; src=&quot;/static/7291d20fb873e9d83312bd236408512f/1c72d/free_pizzaaa.jpg&quot; srcset=&quot;/static/7291d20fb873e9d83312bd236408512f/a80bd/free_pizzaaa.jpg 148w,
/static/7291d20fb873e9d83312bd236408512f/1c91a/free_pizzaaa.jpg 295w,
/static/7291d20fb873e9d83312bd236408512f/1c72d/free_pizzaaa.jpg 590w,
/static/7291d20fb873e9d83312bd236408512f/a8a14/free_pizzaaa.jpg 885w,
/static/7291d20fb873e9d83312bd236408512f/47311/free_pizzaaa.jpg 1080w&quot; sizes=&quot;(max-width: 590px) 100vw, 590px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;&gt;
  &lt;/a&gt;
    &lt;/span&gt;
&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Meet the creator]]></title><description><![CDATA[A more personal introduction I like to document my life, and up until now, I was using social networking to do so. I wanted to create a…]]></description><link>https://kj7kunal.github.io/blog/more-about/</link><guid isPermaLink="false">https://kj7kunal.github.io/blog/more-about/</guid><pubDate>Fri, 10 May 2019 12:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;A more personal introduction&lt;/h2&gt;
&lt;p&gt;I like to document my life, and up until now, I was using social networking to do so. I wanted to create a personal virtual space, fitting my own preferences and tastes. I wonder how this will turn out in the end.&lt;/p&gt;
&lt;p&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; margin: 0 0 30px;&quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/02950b3e6be2fdb323d51e9c9fa1d463/5deef/profile.jpg&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 100%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAUABQDASIAAhEBAxEB/8QAGAABAQEBAQAAAAAAAAAAAAAAAAQCAQX/xAAWAQEBAQAAAAAAAAAAAAAAAAABAgD/2gAMAwEAAhADEAAAAbcUeWzpMzbKM8A//8QAHBAAAgICAwAAAAAAAAAAAAAAAQIAAwQSETEy/9oACAEBAAEFAg4jXqIeSV1VWbbKVgZ1LmKWDz//xAAUEQEAAAAAAAAAAAAAAAAAAAAg/9oACAEDAQE/AR//xAAVEQEBAAAAAAAAAAAAAAAAAAAQAf/aAAgBAgEBPwEh/8QAGxAAAgIDAQAAAAAAAAAAAAAAAAERIQIQIjH/2gAIAQEABj8CPCS4IjiKOnlrFpio/8QAHRABAAIDAAMBAAAAAAAAAAAAAQARITFBUWGBof/aAAgBAQABPyEp78TGCV1Ngc/kpLfUFCiwodrcBWxfI7VzETNkRHUfBqf/2gAMAwEAAgADAAAAEC/4Av/EABkRAAMAAwAAAAAAAAAAAAAAAAABERAhQf/aAAgBAwEBPxDVKh9x/8QAFxEBAQEBAAAAAAAAAAAAAAAAAQARMf/aAAgBAgEBPxA1JEcu563/xAAaEAEAAwEBAQAAAAAAAAAAAAABABEhQTFR/9oACAEBAAE/EB231bSGj2Sxiy+Wyw+PkYBOrW2XaCCA7UhMRRZETjsa/FVH3LiKCkNA5kPMa4J//9k=&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;Profile Picture&quot;
        title=&quot;&quot;
        src=&quot;/static/02950b3e6be2fdb323d51e9c9fa1d463/1c72d/profile.jpg&quot;
        srcset=&quot;/static/02950b3e6be2fdb323d51e9c9fa1d463/a80bd/profile.jpg 148w,
/static/02950b3e6be2fdb323d51e9c9fa1d463/1c91a/profile.jpg 295w,
/static/02950b3e6be2fdb323d51e9c9fa1d463/1c72d/profile.jpg 590w,
/static/02950b3e6be2fdb323d51e9c9fa1d463/a8a14/profile.jpg 885w,
/static/02950b3e6be2fdb323d51e9c9fa1d463/fbd2c/profile.jpg 1180w,
/static/02950b3e6be2fdb323d51e9c9fa1d463/5deef/profile.jpg 2517w&quot;
        sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
        decoding=&quot;async&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;
&lt;h2&gt;Meet the Pianist&lt;/h2&gt;
&lt;p&gt;lorem ipsum&lt;/p&gt;
&lt;h2&gt;Meet the Coder&lt;/h2&gt;
&lt;p&gt;foo bar&lt;/p&gt;
&lt;h2&gt;Meet the Runner&lt;/h2&gt;
&lt;p&gt;hoge hoge&lt;/p&gt;
&lt;h2&gt;Meet the Barista&lt;/h2&gt;
&lt;p&gt;yada yada&lt;/p&gt;
&lt;h2&gt;Meet the Sportsman&lt;/h2&gt;
&lt;p&gt;“It’s in the game”&lt;/p&gt;
&lt;h2&gt;Meet the Hopeless romantic&lt;/h2&gt;
&lt;p&gt;lol wha?&lt;/p&gt;</content:encoded></item><item><title><![CDATA[My First Blog Post]]></title><description><![CDATA[Lorem ipsum dolor sit amet consectetur adipisicing elit Lorem ipsum dolor sit amet consectetur adipisicing elit. Unde reprehenderit…]]></description><link>https://kj7kunal.github.io/blog/my-first-blog/</link><guid isPermaLink="false">https://kj7kunal.github.io/blog/my-first-blog/</guid><pubDate>Thu, 17 Jul 2014 12:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;Lorem ipsum dolor sit amet consectetur adipisicing elit&lt;/h2&gt;
&lt;p&gt;Lorem ipsum dolor sit amet consectetur adipisicing elit. Unde reprehenderit inventore sunt, consequatur omnis tempore ullam natus, porro odit aut, atque asperiores repudiandae corporis quidem esse eos provident velit perferendis magni fugit eum quisquam eligendi. Atque distinctio iure aliquam veniam inventore, soluta est, cum accusantium possimus illum quasi eveniet sed amet ipsa culpa vel in delectus laboriosam repellendus totam. Facere.&lt;/p&gt;
&lt;p&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; margin: 0 0 30px;&quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/04fd963e3e5a465c84a4c4d5014f320f/c08c5/mountain.jpg&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 66.89189189189189%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAANABQDASIAAhEBAxEB/8QAGAAAAwEBAAAAAAAAAAAAAAAAAAEGBAX/xAAVAQEBAAAAAAAAAAAAAAAAAAABAv/aAAwDAQACEAMQAAAB1rgK4oCfE//EABoQAAICAwAAAAAAAAAAAAAAAAABERICISL/2gAIAQEAAQUChHMaL5FmSz//xAAXEQEAAwAAAAAAAAAAAAAAAAAAAREh/9oACAEDAQE/Abhj/8QAFhEAAwAAAAAAAAAAAAAAAAAAAAER/9oACAECAQE/AYyM/8QAFxABAQEBAAAAAAAAAAAAAAAAADEBEP/aAAgBAQAGPwJcVdXn/8QAGxAAAgMAAwAAAAAAAAAAAAAAAAERITFRYcH/2gAIAQEAAT8h4IHpQJOqQkXoT6x3M//aAAwDAQACAAMAAAAQs8//xAAYEQACAwAAAAAAAAAAAAAAAAAAESExUf/aAAgBAwEBPxCGh4P/xAAVEQEBAAAAAAAAAAAAAAAAAAAAYf/aAAgBAgEBPxCyj//EABsQAQACAgMAAAAAAAAAAAAAAAEAESFxYYHx/9oACAEBAAE/EAwokLbZXheDLkk3Kh6rRwEbZ7U//9k=&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;Mountain&quot;
        title=&quot;&quot;
        src=&quot;/static/04fd963e3e5a465c84a4c4d5014f320f/1c72d/mountain.jpg&quot;
        srcset=&quot;/static/04fd963e3e5a465c84a4c4d5014f320f/a80bd/mountain.jpg 148w,
/static/04fd963e3e5a465c84a4c4d5014f320f/1c91a/mountain.jpg 295w,
/static/04fd963e3e5a465c84a4c4d5014f320f/1c72d/mountain.jpg 590w,
/static/04fd963e3e5a465c84a4c4d5014f320f/c08c5/mountain.jpg 640w&quot;
        sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
        decoding=&quot;async&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Aliquam aliquid rem facere dolorum consectetur consequatur distinctio &lt;a href=&quot;https://github.com/RyanFitzgerald/devfolio&quot;&gt;mollitia id modi repellendus&lt;/a&gt; vero quae dolorem commodi soluta voluptates iusto nobis est dolore provident, porro veritatis placeat nemo impedit! Asperiores culpa delectus hic qui saepe, ipsum quia, exercitationem repellendus magni soluta sit suscipit laborum ducimus.&lt;/p&gt;
&lt;h2&gt;Asperiores culpa delectus hic qui saepe&lt;/h2&gt;
&lt;h3&gt;Facere labore velit ad autem&lt;/h3&gt;
&lt;p&gt;Vitae veritatis quae eius quis vel soluta cumque? Facere labore velit ad autem. Nisi recusandae ducimus molestiae error ipsa quaerat, dignissimos suscipit similique itaque sunt provident quasi minus ut porro. Optio modi harum &lt;em&gt;dolore necessitatibus exercitationem&lt;/em&gt; blanditiis magni error ipsum, odit deleniti eligendi facilis, nesciunt delectus sit nostrum porro quam accusamus excepturi labore sequi maiores soluta?&lt;/p&gt;
&lt;h3&gt;Porro veritatis placeat nemo impedit&lt;/h3&gt;
&lt;p&gt;Veritatis et praesentium totam neque earum commodi nesciunt dolor quibusdam incidunt non, ex dicta molestias omnis maiores, maxime velit perferendis tenetur aut porro nostrum, suscipit soluta necessitatibus deserunt nobis. Minus rem dicta eos exercitationem illum consequatur consectetur praesentium voluptas. Dolor inventore quasi necessitatibus odio eaque doloribus.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Repudiandae iusto et iure pariatur aliquid, quisquam, non sed culpa, dignissimos recusandae facilis. Debitis hic, quaerat recusandae ad id, quis nisi perspiciatis quo aliquid natus similique.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Illum esse recusandae facere ipsam fugiat est eaque ducimus facilis provident, distinctio cum aut corporis officiis quo fugit, similique temporibus inventore quidem tempora commodi saepe dicta! Numquam fugiat quibusdam aut ut, voluptatibus accusamus &lt;strong&gt;repellendus quas minus consequuntur&lt;/strong&gt; possimus! Est eaque nesciunt, reiciendis voluptate placeat aspernatur doloremque unde cum et architecto suscipit quam facere corrupti nihil odit eum minima voluptatem nobis.&lt;/p&gt;
&lt;h2&gt;Voluptatibus accusamus repellendus quas minus&lt;/h2&gt;
&lt;p&gt;Ipsum quod, ut animi mollitia ipsam repellat, dolore voluptate quibusdam quasi reiciendis necessitatibus odio ea nostrum illo explicabo? Ducimus, in repudiandae. Ratione dolore sequi in animi obcaecati incidunt reprehenderit illo repellat atque aperiam, praesentium eligendi! Sed voluptas voluptatem sunt distinctio pariatur ullam? Laudantium laboriosam.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Numquam fugiat quibusdam aut ut&lt;/li&gt;
&lt;li&gt;Soluta necessitatibus deserunt nobis&lt;/li&gt;
&lt;li&gt;Illum esse recusandae facere ipsam&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Lorem ipsum dolor sit amet consectetur adipisicing elit. Unde reprehenderit inventore sunt, consequatur omnis tempore ullam natus.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Numquam fugiat quibusdam aut ut&lt;/li&gt;
&lt;li&gt;Soluta necessitatibus deserunt nobis&lt;/li&gt;
&lt;li&gt;Illum esse recusandae facere ipsam&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Lorem ipsum dolor sit amet consectetur adipisicing elit. Unde reprehenderit inventore sunt, consequatur omnis tempore ullam natus, porro odit aut, atque asperiores repudiandae corporis quidem esse eos provident velit perferendis magni fugit eum quisquam eligendi. Atque distinctio iure aliquam veniam inventore, soluta est, cum accusantium possimus illum quasi eveniet sed amet ipsa culpa vel in delectus laboriosam repellendus totam. Facere.&lt;/p&gt;
&lt;h2&gt;Suscipit soluta necessitatibus deserunt nobi&lt;/h2&gt;
&lt;p&gt;Minus rem dicta eos exercitationem illum consequatur consectetur praesentium voluptas. Dolor inventore quasi necessitatibus odio eaque doloribus.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;js&quot;&gt;&lt;pre class=&quot;language-js&quot;&gt;&lt;code class=&quot;language-js&quot;&gt;&lt;span class=&quot;token keyword&quot;&gt;const&lt;/span&gt; helloWorld &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;name &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&apos;World&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token operator&quot;&gt;=&gt;&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;token keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;token template-string&quot;&gt;&lt;span class=&quot;token template-punctuation string&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;Hello &lt;/span&gt;&lt;span class=&quot;token interpolation&quot;&gt;&lt;span class=&quot;token interpolation-punctuation punctuation&quot;&gt;${&lt;/span&gt;name&lt;span class=&quot;token interpolation-punctuation punctuation&quot;&gt;}&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;!&lt;/span&gt;&lt;span class=&quot;token template-punctuation string&quot;&gt;`&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt;

&lt;span class=&quot;token function&quot;&gt;helloWorld&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;token function&quot;&gt;helloWorld&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&apos;John Doe&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Numquam fugiat quibusdam aut ut, voluptatibus accusamus repellendus quas minus consequuntur possimus!&lt;/p&gt;</content:encoded></item></channel></rss>